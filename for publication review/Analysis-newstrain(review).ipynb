{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04851095 0.04255719 0.02830206 ... 0.01893853 0.01285502 0.        ]\n",
      " [0.02931929 0.03494327 0.03729978 ... 0.03034649 0.02700186 0.        ]\n",
      " [0.05283095 0.03790662 0.02872379 ... 0.05526551 0.05592461 0.        ]\n",
      " ...\n",
      " [0.26273887 0.24134372 0.2082449  ... 0.14596688 0.13957147 9.        ]\n",
      " [0.27132432 0.22532877 0.16928747 ... 0.15333333 0.13951453 9.        ]\n",
      " [0.23553421 0.26732525 0.31013608 ... 0.1854257  0.24184243 9.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " #首先，读取.CSV文件成矩阵的形式。\n",
    "my_matrix = np.loadtxt(open(\"results.csv\"),delimiter=\",\",skiprows=0)\n",
    "#print(my_matrix)\n",
    "new_data1 = np.loadtxt(open(\"data_Acinetobacter baumannii.csv\"),delimiter=\",\",skiprows=0)\n",
    "new_data2 = np.loadtxt(open(\"data_Pseudomonas nitritireducens-AOB20.csv\"),delimiter=\",\",skiprows=0)\n",
    "\n",
    "my_matrix = np.vstack((my_matrix,new_data1))\n",
    "my_matrix = np.vstack((my_matrix,new_data2))\n",
    "\n",
    "print(my_matrix)\n",
    "#for n in new_data1:\n",
    "#    my_matrix.append(n)\n",
    "#for n in new_data2:\n",
    "#    my_matrix.append(n)\n",
    "#对于矩阵而言，将矩阵倒数第一列之前的数值给了X（输入数据），将矩阵大最后一列的数值给了y（标签）\n",
    "X, y = my_matrix[:,:-1],my_matrix[:,-1]\n",
    "\n",
    "#X = np.load('X_2019clinical.npy')\n",
    "#y = np.load('y_2019clinical.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yy = np.load('y_reference.npy')\n",
    "#len(yy)\n",
    "#print(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.303831\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99]\n"
     ]
    }
   ],
   "source": [
    "#print(y_train)\n",
    "label = y \n",
    "#print(y_train)\n",
    "sx_1 = 1\n",
    "sx_2 =(sx_1*0.99*0.27*0.99)\n",
    "sx_8 = sx_1*0.99*0.27*0.25*0.99\n",
    "sx_7 = sx_1*0.99*0.27*0.25*1\n",
    "sx_6 = sx_1*0.99*0.31*0.99\n",
    "sx_3 = sx_1*0.99*0.31*0.34\n",
    "sx_4 = sx_1*0.99*0.31*0.51*0.47*0.88*0.78\n",
    "sx_5 = sx_1*0.99*0.31*0.51*0.47*0.88*0.78*0.78\n",
    "sx_9 = sx_1*1.00*0.95*0.99*0.95\n",
    "sx_10 = sx_1*0.99\n",
    "print(sx_6)\n",
    "y = [sx_2 if i == 1 else i for i in y]\n",
    "y = [sx_8 if i == 7 else i for i in y]\n",
    "y = [1 if i == 0 else i for i in y]\n",
    "y = [sx_3 if i == 2 else i for i in y]\n",
    "y = [sx_4 if i == 3 else i for i in y]\n",
    "y = [sx_5 if i == 4 else i for i in y]\n",
    "y = [sx_6 if i == 5 else i for i in y]\n",
    "y = [sx_7 if i == 6 else i for i in y]\n",
    "y = [sx_9 if i == 8 else i for i in y]\n",
    "y = [sx_10 if i == 9 else i for i in y]\n",
    "\n",
    "#y_train = np.array(y_train)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827, 0.8827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train =[]\n",
    "X_test =[]\n",
    "#X_test1 =[]\n",
    "\n",
    "y_train =[]\n",
    "y_test =[]\n",
    "\n",
    "for n in range(len(y)):\n",
    "    if y[n] == sx_1:\n",
    "        X_test.append(X[n])\n",
    "        y_test.append(y[n])\n",
    "\n",
    "for n in range(len(y)):\n",
    "    if y[n] == sx_2 or y[n] == sx_5 or y[n] == sx_7 or y[n] == sx_9 :\n",
    "        X_test.append(X[n])\n",
    "        y_test.append(y[n])\n",
    "    else:\n",
    "        X_train.append(X[n])\n",
    "        y_train.append(y[n])\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " #利用train_test_split方法，将X,y随机划分问，训练集（X_train），训练集标签（X_test），测试卷（y_train），\n",
    " #测试集标签（y_test），安训练集：测试集=7:3的\n",
    " #概率划分，到此步骤，可以直接对数据进行处理\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    " #此步骤，是为了将训练集与数据集的数据分别保存为CSV文件\n",
    " #np.column_stack将两个矩阵进行组合连接\n",
    "#train= np.column_stack((X_train,y_train))\n",
    " #numpy.savetxt 将txt文件保存为.csv结尾的文件\n",
    "#np.savetxt('train_usual.csv',train, delimiter = ',')\n",
    "#test = np.column_stack((X_test, y_test))\n",
    "#np.savetxt('test_usual.csv', test, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train)\n",
    "label = y_train\n",
    "#print(label)\n",
    "y_train = np.array(y_train)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "#new_X = np.array(new_X)\n",
    "#new_y = np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#torch_new_y = torch.from_numpy(new_y)\n",
    "#torch_new_x = torch.from_numpy(new_X)\n",
    "torch_x = torch.from_numpy(X_train)\n",
    "torch_y = torch.from_numpy(y_train)\n",
    "test_x = torch.from_numpy(X_test)\n",
    "test_y = torch.from_numpy(y_test)\n",
    "\n",
    "#x_test2 = torch.tensor(torch_new_x, dtype=torch.float32)\n",
    "#y_test2 = torch.tensor(torch_new_y, dtype=torch.float32)\n",
    "y_train = torch.tensor(torch_y, dtype=torch.float32)\n",
    "y_test = torch.tensor(test_y,dtype = torch.float32)\n",
    "x_test = torch.tensor(test_x , dtype=torch.float32)\n",
    "x_train = torch.tensor(torch_x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "EPOCH = 500\n",
    "BATCH_SIZE = 100\n",
    "TIME_STEP = 20\n",
    "INPUT_SIZE = 60\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test2 = x_test2.reshape(x_test2.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "x_test = x_test.reshape(x_test.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "x_train = x_train.reshape(x_train.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "#print(x_train.shape,x_test.shape)\n",
    "torch_dataset = Data.TensorDataset(x_train,y_train )\n",
    "train_loader = Data.DataLoader(dataset= torch_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=2)\n",
    "#print(x_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=64,         # rnn hidden unit\n",
    "            num_layers=3,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self , x, y):\n",
    "        time = 1\n",
    "        ave = torch.tensor(0)\n",
    "        for n in range(len(x)):\n",
    "            if y[n] == 1:\n",
    "                ave = torch.add(x[n], ave)\n",
    "                time += 1\n",
    "        #print(ave,time)\n",
    "        ave  = torch.div(ave,time) \n",
    "        div = torch.div(x,ave)\n",
    "        loss = torch.mean(torch.pow((div - y),2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(X_out),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(60, 64, num_layers=3, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn =  RNN()\n",
    "print(rnn)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = LR)\n",
    "loss_func = My_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:0 train:tensor(0.5111, grad_fn=<MeanBackward0>) test:tensor(0.4358, grad_fn=<MeanBackward0>)\n",
      "times:1 train:tensor(0.4954, grad_fn=<MeanBackward0>) test:tensor(0.4359, grad_fn=<MeanBackward0>)\n",
      "times:2 train:tensor(0.4960, grad_fn=<MeanBackward0>) test:tensor(0.4325, grad_fn=<MeanBackward0>)\n",
      "times:3 train:tensor(0.4920, grad_fn=<MeanBackward0>) test:tensor(0.4333, grad_fn=<MeanBackward0>)\n",
      "times:4 train:tensor(0.4919, grad_fn=<MeanBackward0>) test:tensor(0.4320, grad_fn=<MeanBackward0>)\n",
      "times:5 train:tensor(0.4906, grad_fn=<MeanBackward0>) test:tensor(0.4288, grad_fn=<MeanBackward0>)\n",
      "times:6 train:tensor(0.4882, grad_fn=<MeanBackward0>) test:tensor(0.4254, grad_fn=<MeanBackward0>)\n",
      "times:7 train:tensor(0.4859, grad_fn=<MeanBackward0>) test:tensor(0.4235, grad_fn=<MeanBackward0>)\n",
      "times:8 train:tensor(0.4852, grad_fn=<MeanBackward0>) test:tensor(0.4215, grad_fn=<MeanBackward0>)\n",
      "times:9 train:tensor(0.4837, grad_fn=<MeanBackward0>) test:tensor(0.4193, grad_fn=<MeanBackward0>)\n",
      "times:10 train:tensor(0.4806, grad_fn=<MeanBackward0>) test:tensor(0.4191, grad_fn=<MeanBackward0>)\n",
      "times:11 train:tensor(0.4786, grad_fn=<MeanBackward0>) test:tensor(0.4186, grad_fn=<MeanBackward0>)\n",
      "times:12 train:tensor(0.4767, grad_fn=<MeanBackward0>) test:tensor(0.4147, grad_fn=<MeanBackward0>)\n",
      "times:13 train:tensor(0.4729, grad_fn=<MeanBackward0>) test:tensor(0.4070, grad_fn=<MeanBackward0>)\n",
      "times:14 train:tensor(0.4678, grad_fn=<MeanBackward0>) test:tensor(0.3985, grad_fn=<MeanBackward0>)\n",
      "times:15 train:tensor(0.4628, grad_fn=<MeanBackward0>) test:tensor(0.3959, grad_fn=<MeanBackward0>)\n",
      "times:16 train:tensor(0.4528, grad_fn=<MeanBackward0>) test:tensor(0.4036, grad_fn=<MeanBackward0>)\n",
      "times:17 train:tensor(0.4414, grad_fn=<MeanBackward0>) test:tensor(0.3687, grad_fn=<MeanBackward0>)\n",
      "times:18 train:tensor(0.4149, grad_fn=<MeanBackward0>) test:tensor(0.5272, grad_fn=<MeanBackward0>)\n",
      "times:19 train:tensor(0.3665, grad_fn=<MeanBackward0>) test:tensor(1.4043, grad_fn=<MeanBackward0>)\n",
      "times:20 train:tensor(1.8252, grad_fn=<MeanBackward0>) test:tensor(0.5936, grad_fn=<MeanBackward0>)\n",
      "times:21 train:tensor(0.6669, grad_fn=<MeanBackward0>) test:tensor(0.5083, grad_fn=<MeanBackward0>)\n",
      "times:22 train:tensor(0.5685, grad_fn=<MeanBackward0>) test:tensor(0.4776, grad_fn=<MeanBackward0>)\n",
      "times:23 train:tensor(0.5372, grad_fn=<MeanBackward0>) test:tensor(0.4623, grad_fn=<MeanBackward0>)\n",
      "times:24 train:tensor(0.5223, grad_fn=<MeanBackward0>) test:tensor(0.4534, grad_fn=<MeanBackward0>)\n",
      "times:25 train:tensor(0.5138, grad_fn=<MeanBackward0>) test:tensor(0.4480, grad_fn=<MeanBackward0>)\n",
      "times:26 train:tensor(0.5084, grad_fn=<MeanBackward0>) test:tensor(0.4446, grad_fn=<MeanBackward0>)\n",
      "times:27 train:tensor(0.5049, grad_fn=<MeanBackward0>) test:tensor(0.4424, grad_fn=<MeanBackward0>)\n",
      "times:28 train:tensor(0.5025, grad_fn=<MeanBackward0>) test:tensor(0.4410, grad_fn=<MeanBackward0>)\n",
      "times:29 train:tensor(0.5008, grad_fn=<MeanBackward0>) test:tensor(0.4402, grad_fn=<MeanBackward0>)\n",
      "times:30 train:tensor(0.4997, grad_fn=<MeanBackward0>) test:tensor(0.4397, grad_fn=<MeanBackward0>)\n",
      "times:31 train:tensor(0.4988, grad_fn=<MeanBackward0>) test:tensor(0.4393, grad_fn=<MeanBackward0>)\n",
      "times:32 train:tensor(0.4981, grad_fn=<MeanBackward0>) test:tensor(0.4391, grad_fn=<MeanBackward0>)\n",
      "times:33 train:tensor(0.4975, grad_fn=<MeanBackward0>) test:tensor(0.4388, grad_fn=<MeanBackward0>)\n",
      "times:34 train:tensor(0.4970, grad_fn=<MeanBackward0>) test:tensor(0.4385, grad_fn=<MeanBackward0>)\n",
      "times:35 train:tensor(0.4965, grad_fn=<MeanBackward0>) test:tensor(0.4382, grad_fn=<MeanBackward0>)\n",
      "times:36 train:tensor(0.4959, grad_fn=<MeanBackward0>) test:tensor(0.4377, grad_fn=<MeanBackward0>)\n",
      "times:37 train:tensor(0.4954, grad_fn=<MeanBackward0>) test:tensor(0.4372, grad_fn=<MeanBackward0>)\n",
      "times:38 train:tensor(0.4947, grad_fn=<MeanBackward0>) test:tensor(0.4366, grad_fn=<MeanBackward0>)\n",
      "times:39 train:tensor(0.4941, grad_fn=<MeanBackward0>) test:tensor(0.4359, grad_fn=<MeanBackward0>)\n",
      "times:40 train:tensor(0.4934, grad_fn=<MeanBackward0>) test:tensor(0.4351, grad_fn=<MeanBackward0>)\n",
      "times:41 train:tensor(0.4926, grad_fn=<MeanBackward0>) test:tensor(0.4342, grad_fn=<MeanBackward0>)\n",
      "times:42 train:tensor(0.4919, grad_fn=<MeanBackward0>) test:tensor(0.4334, grad_fn=<MeanBackward0>)\n",
      "times:43 train:tensor(0.4911, grad_fn=<MeanBackward0>) test:tensor(0.4325, grad_fn=<MeanBackward0>)\n",
      "times:44 train:tensor(0.4903, grad_fn=<MeanBackward0>) test:tensor(0.4315, grad_fn=<MeanBackward0>)\n",
      "times:45 train:tensor(0.4896, grad_fn=<MeanBackward0>) test:tensor(0.4306, grad_fn=<MeanBackward0>)\n",
      "times:46 train:tensor(0.4889, grad_fn=<MeanBackward0>) test:tensor(0.4298, grad_fn=<MeanBackward0>)\n",
      "times:47 train:tensor(0.4883, grad_fn=<MeanBackward0>) test:tensor(0.4289, grad_fn=<MeanBackward0>)\n",
      "times:48 train:tensor(0.4877, grad_fn=<MeanBackward0>) test:tensor(0.4281, grad_fn=<MeanBackward0>)\n",
      "times:49 train:tensor(0.4872, grad_fn=<MeanBackward0>) test:tensor(0.4274, grad_fn=<MeanBackward0>)\n",
      "times:50 train:tensor(0.4867, grad_fn=<MeanBackward0>) test:tensor(0.4268, grad_fn=<MeanBackward0>)\n",
      "times:51 train:tensor(0.4863, grad_fn=<MeanBackward0>) test:tensor(0.4262, grad_fn=<MeanBackward0>)\n",
      "times:52 train:tensor(0.4860, grad_fn=<MeanBackward0>) test:tensor(0.4256, grad_fn=<MeanBackward0>)\n",
      "times:53 train:tensor(0.4857, grad_fn=<MeanBackward0>) test:tensor(0.4252, grad_fn=<MeanBackward0>)\n",
      "times:54 train:tensor(0.4855, grad_fn=<MeanBackward0>) test:tensor(0.4248, grad_fn=<MeanBackward0>)\n",
      "times:55 train:tensor(0.4853, grad_fn=<MeanBackward0>) test:tensor(0.4244, grad_fn=<MeanBackward0>)\n",
      "times:56 train:tensor(0.4851, grad_fn=<MeanBackward0>) test:tensor(0.4241, grad_fn=<MeanBackward0>)\n",
      "times:57 train:tensor(0.4850, grad_fn=<MeanBackward0>) test:tensor(0.4238, grad_fn=<MeanBackward0>)\n",
      "times:58 train:tensor(0.4848, grad_fn=<MeanBackward0>) test:tensor(0.4235, grad_fn=<MeanBackward0>)\n",
      "times:59 train:tensor(0.4846, grad_fn=<MeanBackward0>) test:tensor(0.4232, grad_fn=<MeanBackward0>)\n",
      "times:60 train:tensor(0.4845, grad_fn=<MeanBackward0>) test:tensor(0.4230, grad_fn=<MeanBackward0>)\n",
      "times:61 train:tensor(0.4843, grad_fn=<MeanBackward0>) test:tensor(0.4227, grad_fn=<MeanBackward0>)\n",
      "times:62 train:tensor(0.4840, grad_fn=<MeanBackward0>) test:tensor(0.4225, grad_fn=<MeanBackward0>)\n",
      "times:63 train:tensor(0.4838, grad_fn=<MeanBackward0>) test:tensor(0.4223, grad_fn=<MeanBackward0>)\n",
      "times:64 train:tensor(0.4835, grad_fn=<MeanBackward0>) test:tensor(0.4221, grad_fn=<MeanBackward0>)\n",
      "times:65 train:tensor(0.4833, grad_fn=<MeanBackward0>) test:tensor(0.4220, grad_fn=<MeanBackward0>)\n",
      "times:66 train:tensor(0.4830, grad_fn=<MeanBackward0>) test:tensor(0.4218, grad_fn=<MeanBackward0>)\n",
      "times:67 train:tensor(0.4827, grad_fn=<MeanBackward0>) test:tensor(0.4217, grad_fn=<MeanBackward0>)\n",
      "times:68 train:tensor(0.4824, grad_fn=<MeanBackward0>) test:tensor(0.4216, grad_fn=<MeanBackward0>)\n",
      "times:69 train:tensor(0.4821, grad_fn=<MeanBackward0>) test:tensor(0.4215, grad_fn=<MeanBackward0>)\n",
      "times:70 train:tensor(0.4818, grad_fn=<MeanBackward0>) test:tensor(0.4215, grad_fn=<MeanBackward0>)\n",
      "times:71 train:tensor(0.4815, grad_fn=<MeanBackward0>) test:tensor(0.4215, grad_fn=<MeanBackward0>)\n",
      "times:72 train:tensor(0.4813, grad_fn=<MeanBackward0>) test:tensor(0.4215, grad_fn=<MeanBackward0>)\n",
      "times:73 train:tensor(0.4810, grad_fn=<MeanBackward0>) test:tensor(0.4215, grad_fn=<MeanBackward0>)\n",
      "times:74 train:tensor(0.4808, grad_fn=<MeanBackward0>) test:tensor(0.4215, grad_fn=<MeanBackward0>)\n",
      "times:75 train:tensor(0.4805, grad_fn=<MeanBackward0>) test:tensor(0.4214, grad_fn=<MeanBackward0>)\n",
      "times:76 train:tensor(0.4803, grad_fn=<MeanBackward0>) test:tensor(0.4214, grad_fn=<MeanBackward0>)\n",
      "times:77 train:tensor(0.4800, grad_fn=<MeanBackward0>) test:tensor(0.4214, grad_fn=<MeanBackward0>)\n",
      "times:78 train:tensor(0.4798, grad_fn=<MeanBackward0>) test:tensor(0.4213, grad_fn=<MeanBackward0>)\n",
      "times:79 train:tensor(0.4796, grad_fn=<MeanBackward0>) test:tensor(0.4212, grad_fn=<MeanBackward0>)\n",
      "times:80 train:tensor(0.4793, grad_fn=<MeanBackward0>) test:tensor(0.4210, grad_fn=<MeanBackward0>)\n",
      "times:81 train:tensor(0.4790, grad_fn=<MeanBackward0>) test:tensor(0.4208, grad_fn=<MeanBackward0>)\n",
      "times:82 train:tensor(0.4788, grad_fn=<MeanBackward0>) test:tensor(0.4206, grad_fn=<MeanBackward0>)\n",
      "times:83 train:tensor(0.4785, grad_fn=<MeanBackward0>) test:tensor(0.4203, grad_fn=<MeanBackward0>)\n",
      "times:84 train:tensor(0.4782, grad_fn=<MeanBackward0>) test:tensor(0.4201, grad_fn=<MeanBackward0>)\n",
      "times:85 train:tensor(0.4779, grad_fn=<MeanBackward0>) test:tensor(0.4198, grad_fn=<MeanBackward0>)\n",
      "times:86 train:tensor(0.4776, grad_fn=<MeanBackward0>) test:tensor(0.4195, grad_fn=<MeanBackward0>)\n",
      "times:87 train:tensor(0.4773, grad_fn=<MeanBackward0>) test:tensor(0.4191, grad_fn=<MeanBackward0>)\n",
      "times:88 train:tensor(0.4770, grad_fn=<MeanBackward0>) test:tensor(0.4188, grad_fn=<MeanBackward0>)\n",
      "times:89 train:tensor(0.4767, grad_fn=<MeanBackward0>) test:tensor(0.4185, grad_fn=<MeanBackward0>)\n",
      "times:90 train:tensor(0.4764, grad_fn=<MeanBackward0>) test:tensor(0.4182, grad_fn=<MeanBackward0>)\n",
      "times:91 train:tensor(0.4761, grad_fn=<MeanBackward0>) test:tensor(0.4179, grad_fn=<MeanBackward0>)\n",
      "times:92 train:tensor(0.4758, grad_fn=<MeanBackward0>) test:tensor(0.4176, grad_fn=<MeanBackward0>)\n",
      "times:93 train:tensor(0.4754, grad_fn=<MeanBackward0>) test:tensor(0.4174, grad_fn=<MeanBackward0>)\n",
      "times:94 train:tensor(0.4751, grad_fn=<MeanBackward0>) test:tensor(0.4172, grad_fn=<MeanBackward0>)\n",
      "times:95 train:tensor(0.4748, grad_fn=<MeanBackward0>) test:tensor(0.4170, grad_fn=<MeanBackward0>)\n",
      "times:96 train:tensor(0.4744, grad_fn=<MeanBackward0>) test:tensor(0.4168, grad_fn=<MeanBackward0>)\n",
      "times:97 train:tensor(0.4741, grad_fn=<MeanBackward0>) test:tensor(0.4167, grad_fn=<MeanBackward0>)\n",
      "times:98 train:tensor(0.4737, grad_fn=<MeanBackward0>) test:tensor(0.4167, grad_fn=<MeanBackward0>)\n",
      "times:99 train:tensor(0.4734, grad_fn=<MeanBackward0>) test:tensor(0.4166, grad_fn=<MeanBackward0>)\n",
      "times:100 train:tensor(0.4730, grad_fn=<MeanBackward0>) test:tensor(0.4166, grad_fn=<MeanBackward0>)\n",
      "times:101 train:tensor(0.4726, grad_fn=<MeanBackward0>) test:tensor(0.4166, grad_fn=<MeanBackward0>)\n",
      "times:102 train:tensor(0.4722, grad_fn=<MeanBackward0>) test:tensor(0.4166, grad_fn=<MeanBackward0>)\n",
      "times:103 train:tensor(0.4718, grad_fn=<MeanBackward0>) test:tensor(0.4167, grad_fn=<MeanBackward0>)\n",
      "times:104 train:tensor(0.4714, grad_fn=<MeanBackward0>) test:tensor(0.4167, grad_fn=<MeanBackward0>)\n",
      "times:105 train:tensor(0.4710, grad_fn=<MeanBackward0>) test:tensor(0.4167, grad_fn=<MeanBackward0>)\n",
      "times:106 train:tensor(0.4705, grad_fn=<MeanBackward0>) test:tensor(0.4167, grad_fn=<MeanBackward0>)\n",
      "times:107 train:tensor(0.4701, grad_fn=<MeanBackward0>) test:tensor(0.4167, grad_fn=<MeanBackward0>)\n",
      "times:108 train:tensor(0.4696, grad_fn=<MeanBackward0>) test:tensor(0.4166, grad_fn=<MeanBackward0>)\n",
      "times:109 train:tensor(0.4692, grad_fn=<MeanBackward0>) test:tensor(0.4166, grad_fn=<MeanBackward0>)\n",
      "times:110 train:tensor(0.4687, grad_fn=<MeanBackward0>) test:tensor(0.4165, grad_fn=<MeanBackward0>)\n",
      "times:111 train:tensor(0.4681, grad_fn=<MeanBackward0>) test:tensor(0.4163, grad_fn=<MeanBackward0>)\n",
      "times:112 train:tensor(0.4676, grad_fn=<MeanBackward0>) test:tensor(0.4162, grad_fn=<MeanBackward0>)\n",
      "times:113 train:tensor(0.4671, grad_fn=<MeanBackward0>) test:tensor(0.4161, grad_fn=<MeanBackward0>)\n",
      "times:114 train:tensor(0.4665, grad_fn=<MeanBackward0>) test:tensor(0.4159, grad_fn=<MeanBackward0>)\n",
      "times:115 train:tensor(0.4659, grad_fn=<MeanBackward0>) test:tensor(0.4158, grad_fn=<MeanBackward0>)\n",
      "times:116 train:tensor(0.4653, grad_fn=<MeanBackward0>) test:tensor(0.4157, grad_fn=<MeanBackward0>)\n",
      "times:117 train:tensor(0.4646, grad_fn=<MeanBackward0>) test:tensor(0.4157, grad_fn=<MeanBackward0>)\n",
      "times:118 train:tensor(0.4639, grad_fn=<MeanBackward0>) test:tensor(0.4158, grad_fn=<MeanBackward0>)\n",
      "times:119 train:tensor(0.4632, grad_fn=<MeanBackward0>) test:tensor(0.4159, grad_fn=<MeanBackward0>)\n",
      "times:120 train:tensor(0.4625, grad_fn=<MeanBackward0>) test:tensor(0.4160, grad_fn=<MeanBackward0>)\n",
      "times:121 train:tensor(0.4617, grad_fn=<MeanBackward0>) test:tensor(0.4163, grad_fn=<MeanBackward0>)\n",
      "times:122 train:tensor(0.4609, grad_fn=<MeanBackward0>) test:tensor(0.4166, grad_fn=<MeanBackward0>)\n",
      "times:123 train:tensor(0.4601, grad_fn=<MeanBackward0>) test:tensor(0.4170, grad_fn=<MeanBackward0>)\n",
      "times:124 train:tensor(0.4592, grad_fn=<MeanBackward0>) test:tensor(0.4174, grad_fn=<MeanBackward0>)\n",
      "times:125 train:tensor(0.4582, grad_fn=<MeanBackward0>) test:tensor(0.4178, grad_fn=<MeanBackward0>)\n",
      "times:126 train:tensor(0.4572, grad_fn=<MeanBackward0>) test:tensor(0.4182, grad_fn=<MeanBackward0>)\n",
      "times:127 train:tensor(0.4562, grad_fn=<MeanBackward0>) test:tensor(0.4186, grad_fn=<MeanBackward0>)\n",
      "times:128 train:tensor(0.4551, grad_fn=<MeanBackward0>) test:tensor(0.4190, grad_fn=<MeanBackward0>)\n",
      "times:129 train:tensor(0.4539, grad_fn=<MeanBackward0>) test:tensor(0.4193, grad_fn=<MeanBackward0>)\n",
      "times:130 train:tensor(0.4526, grad_fn=<MeanBackward0>) test:tensor(0.4197, grad_fn=<MeanBackward0>)\n",
      "times:131 train:tensor(0.4513, grad_fn=<MeanBackward0>) test:tensor(0.4202, grad_fn=<MeanBackward0>)\n",
      "times:132 train:tensor(0.4499, grad_fn=<MeanBackward0>) test:tensor(0.4207, grad_fn=<MeanBackward0>)\n",
      "times:133 train:tensor(0.4484, grad_fn=<MeanBackward0>) test:tensor(0.4214, grad_fn=<MeanBackward0>)\n",
      "times:134 train:tensor(0.4468, grad_fn=<MeanBackward0>) test:tensor(0.4223, grad_fn=<MeanBackward0>)\n",
      "times:135 train:tensor(0.4451, grad_fn=<MeanBackward0>) test:tensor(0.4232, grad_fn=<MeanBackward0>)\n",
      "times:136 train:tensor(0.4433, grad_fn=<MeanBackward0>) test:tensor(0.4242, grad_fn=<MeanBackward0>)\n",
      "times:137 train:tensor(0.4414, grad_fn=<MeanBackward0>) test:tensor(0.4252, grad_fn=<MeanBackward0>)\n",
      "times:138 train:tensor(0.4394, grad_fn=<MeanBackward0>) test:tensor(0.4259, grad_fn=<MeanBackward0>)\n",
      "times:139 train:tensor(0.4373, grad_fn=<MeanBackward0>) test:tensor(0.4263, grad_fn=<MeanBackward0>)\n",
      "times:140 train:tensor(0.4351, grad_fn=<MeanBackward0>) test:tensor(0.4260, grad_fn=<MeanBackward0>)\n",
      "times:141 train:tensor(0.4328, grad_fn=<MeanBackward0>) test:tensor(0.4252, grad_fn=<MeanBackward0>)\n",
      "times:142 train:tensor(0.4304, grad_fn=<MeanBackward0>) test:tensor(0.4237, grad_fn=<MeanBackward0>)\n",
      "times:143 train:tensor(0.4279, grad_fn=<MeanBackward0>) test:tensor(0.4216, grad_fn=<MeanBackward0>)\n",
      "times:144 train:tensor(0.4253, grad_fn=<MeanBackward0>) test:tensor(0.4188, grad_fn=<MeanBackward0>)\n",
      "times:145 train:tensor(0.4226, grad_fn=<MeanBackward0>) test:tensor(0.4154, grad_fn=<MeanBackward0>)\n",
      "times:146 train:tensor(0.4198, grad_fn=<MeanBackward0>) test:tensor(0.4113, grad_fn=<MeanBackward0>)\n",
      "times:147 train:tensor(0.4168, grad_fn=<MeanBackward0>) test:tensor(0.4065, grad_fn=<MeanBackward0>)\n",
      "times:148 train:tensor(0.4136, grad_fn=<MeanBackward0>) test:tensor(0.4010, grad_fn=<MeanBackward0>)\n",
      "times:149 train:tensor(0.4101, grad_fn=<MeanBackward0>) test:tensor(0.3951, grad_fn=<MeanBackward0>)\n",
      "times:150 train:tensor(0.4064, grad_fn=<MeanBackward0>) test:tensor(0.3889, grad_fn=<MeanBackward0>)\n",
      "times:151 train:tensor(0.4023, grad_fn=<MeanBackward0>) test:tensor(0.3828, grad_fn=<MeanBackward0>)\n",
      "times:152 train:tensor(0.3980, grad_fn=<MeanBackward0>) test:tensor(0.3769, grad_fn=<MeanBackward0>)\n",
      "times:153 train:tensor(0.3933, grad_fn=<MeanBackward0>) test:tensor(0.3714, grad_fn=<MeanBackward0>)\n",
      "times:154 train:tensor(0.3884, grad_fn=<MeanBackward0>) test:tensor(0.3663, grad_fn=<MeanBackward0>)\n",
      "times:155 train:tensor(0.3832, grad_fn=<MeanBackward0>) test:tensor(0.3614, grad_fn=<MeanBackward0>)\n",
      "times:156 train:tensor(0.3778, grad_fn=<MeanBackward0>) test:tensor(0.3569, grad_fn=<MeanBackward0>)\n",
      "times:157 train:tensor(0.3721, grad_fn=<MeanBackward0>) test:tensor(0.3526, grad_fn=<MeanBackward0>)\n",
      "times:158 train:tensor(0.3661, grad_fn=<MeanBackward0>) test:tensor(0.3486, grad_fn=<MeanBackward0>)\n",
      "times:159 train:tensor(0.3598, grad_fn=<MeanBackward0>) test:tensor(0.3448, grad_fn=<MeanBackward0>)\n",
      "times:160 train:tensor(0.3531, grad_fn=<MeanBackward0>) test:tensor(0.3412, grad_fn=<MeanBackward0>)\n",
      "times:161 train:tensor(0.3461, grad_fn=<MeanBackward0>) test:tensor(0.3374, grad_fn=<MeanBackward0>)\n",
      "times:162 train:tensor(0.3389, grad_fn=<MeanBackward0>) test:tensor(0.3334, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:163 train:tensor(0.3316, grad_fn=<MeanBackward0>) test:tensor(0.3291, grad_fn=<MeanBackward0>)\n",
      "times:164 train:tensor(0.3241, grad_fn=<MeanBackward0>) test:tensor(0.3248, grad_fn=<MeanBackward0>)\n",
      "times:165 train:tensor(0.3168, grad_fn=<MeanBackward0>) test:tensor(0.3208, grad_fn=<MeanBackward0>)\n",
      "times:166 train:tensor(0.3095, grad_fn=<MeanBackward0>) test:tensor(0.3168, grad_fn=<MeanBackward0>)\n",
      "times:167 train:tensor(0.3025, grad_fn=<MeanBackward0>) test:tensor(0.3125, grad_fn=<MeanBackward0>)\n",
      "times:168 train:tensor(0.2960, grad_fn=<MeanBackward0>) test:tensor(0.3075, grad_fn=<MeanBackward0>)\n",
      "times:169 train:tensor(0.2899, grad_fn=<MeanBackward0>) test:tensor(0.3023, grad_fn=<MeanBackward0>)\n",
      "times:170 train:tensor(0.2846, grad_fn=<MeanBackward0>) test:tensor(0.2977, grad_fn=<MeanBackward0>)\n",
      "times:171 train:tensor(0.2802, grad_fn=<MeanBackward0>) test:tensor(0.2947, grad_fn=<MeanBackward0>)\n",
      "times:172 train:tensor(0.2768, grad_fn=<MeanBackward0>) test:tensor(0.2932, grad_fn=<MeanBackward0>)\n",
      "times:173 train:tensor(0.2743, grad_fn=<MeanBackward0>) test:tensor(0.2924, grad_fn=<MeanBackward0>)\n",
      "times:174 train:tensor(0.2728, grad_fn=<MeanBackward0>) test:tensor(0.2910, grad_fn=<MeanBackward0>)\n",
      "times:175 train:tensor(0.2721, grad_fn=<MeanBackward0>) test:tensor(0.2884, grad_fn=<MeanBackward0>)\n",
      "times:176 train:tensor(0.2718, grad_fn=<MeanBackward0>) test:tensor(0.2849, grad_fn=<MeanBackward0>)\n",
      "times:177 train:tensor(0.2719, grad_fn=<MeanBackward0>) test:tensor(0.2810, grad_fn=<MeanBackward0>)\n",
      "times:178 train:tensor(0.2719, grad_fn=<MeanBackward0>) test:tensor(0.2772, grad_fn=<MeanBackward0>)\n",
      "times:179 train:tensor(0.2717, grad_fn=<MeanBackward0>) test:tensor(0.2742, grad_fn=<MeanBackward0>)\n",
      "times:180 train:tensor(0.2716, grad_fn=<MeanBackward0>) test:tensor(0.2720, grad_fn=<MeanBackward0>)\n",
      "times:181 train:tensor(0.2714, grad_fn=<MeanBackward0>) test:tensor(0.2704, grad_fn=<MeanBackward0>)\n",
      "times:182 train:tensor(0.2711, grad_fn=<MeanBackward0>) test:tensor(0.2690, grad_fn=<MeanBackward0>)\n",
      "times:183 train:tensor(0.2706, grad_fn=<MeanBackward0>) test:tensor(0.2679, grad_fn=<MeanBackward0>)\n",
      "times:184 train:tensor(0.2700, grad_fn=<MeanBackward0>) test:tensor(0.2673, grad_fn=<MeanBackward0>)\n",
      "times:185 train:tensor(0.2694, grad_fn=<MeanBackward0>) test:tensor(0.2667, grad_fn=<MeanBackward0>)\n",
      "times:186 train:tensor(0.2687, grad_fn=<MeanBackward0>) test:tensor(0.2657, grad_fn=<MeanBackward0>)\n",
      "times:187 train:tensor(0.2681, grad_fn=<MeanBackward0>) test:tensor(0.2639, grad_fn=<MeanBackward0>)\n",
      "times:188 train:tensor(0.2673, grad_fn=<MeanBackward0>) test:tensor(0.2623, grad_fn=<MeanBackward0>)\n",
      "times:189 train:tensor(0.2667, grad_fn=<MeanBackward0>) test:tensor(0.2614, grad_fn=<MeanBackward0>)\n",
      "times:190 train:tensor(0.2662, grad_fn=<MeanBackward0>) test:tensor(0.2608, grad_fn=<MeanBackward0>)\n",
      "times:191 train:tensor(0.2657, grad_fn=<MeanBackward0>) test:tensor(0.2599, grad_fn=<MeanBackward0>)\n",
      "times:192 train:tensor(0.2653, grad_fn=<MeanBackward0>) test:tensor(0.2595, grad_fn=<MeanBackward0>)\n",
      "times:193 train:tensor(0.2650, grad_fn=<MeanBackward0>) test:tensor(0.2597, grad_fn=<MeanBackward0>)\n",
      "times:194 train:tensor(0.2648, grad_fn=<MeanBackward0>) test:tensor(0.2599, grad_fn=<MeanBackward0>)\n",
      "times:195 train:tensor(0.2646, grad_fn=<MeanBackward0>) test:tensor(0.2596, grad_fn=<MeanBackward0>)\n",
      "times:196 train:tensor(0.2645, grad_fn=<MeanBackward0>) test:tensor(0.2598, grad_fn=<MeanBackward0>)\n",
      "times:197 train:tensor(0.2645, grad_fn=<MeanBackward0>) test:tensor(0.2605, grad_fn=<MeanBackward0>)\n",
      "times:198 train:tensor(0.2645, grad_fn=<MeanBackward0>) test:tensor(0.2603, grad_fn=<MeanBackward0>)\n",
      "times:199 train:tensor(0.2642, grad_fn=<MeanBackward0>) test:tensor(0.2599, grad_fn=<MeanBackward0>)\n",
      "times:200 train:tensor(0.2639, grad_fn=<MeanBackward0>) test:tensor(0.2598, grad_fn=<MeanBackward0>)\n",
      "times:201 train:tensor(0.2635, grad_fn=<MeanBackward0>) test:tensor(0.2594, grad_fn=<MeanBackward0>)\n",
      "times:202 train:tensor(0.2631, grad_fn=<MeanBackward0>) test:tensor(0.2588, grad_fn=<MeanBackward0>)\n",
      "times:203 train:tensor(0.2628, grad_fn=<MeanBackward0>) test:tensor(0.2591, grad_fn=<MeanBackward0>)\n",
      "times:204 train:tensor(0.2626, grad_fn=<MeanBackward0>) test:tensor(0.2596, grad_fn=<MeanBackward0>)\n",
      "times:205 train:tensor(0.2625, grad_fn=<MeanBackward0>) test:tensor(0.2597, grad_fn=<MeanBackward0>)\n",
      "times:206 train:tensor(0.2623, grad_fn=<MeanBackward0>) test:tensor(0.2599, grad_fn=<MeanBackward0>)\n",
      "times:207 train:tensor(0.2620, grad_fn=<MeanBackward0>) test:tensor(0.2604, grad_fn=<MeanBackward0>)\n",
      "times:208 train:tensor(0.2618, grad_fn=<MeanBackward0>) test:tensor(0.2605, grad_fn=<MeanBackward0>)\n",
      "times:209 train:tensor(0.2616, grad_fn=<MeanBackward0>) test:tensor(0.2605, grad_fn=<MeanBackward0>)\n",
      "times:210 train:tensor(0.2614, grad_fn=<MeanBackward0>) test:tensor(0.2608, grad_fn=<MeanBackward0>)\n",
      "times:211 train:tensor(0.2613, grad_fn=<MeanBackward0>) test:tensor(0.2605, grad_fn=<MeanBackward0>)\n",
      "times:212 train:tensor(0.2610, grad_fn=<MeanBackward0>) test:tensor(0.2600, grad_fn=<MeanBackward0>)\n",
      "times:213 train:tensor(0.2608, grad_fn=<MeanBackward0>) test:tensor(0.2597, grad_fn=<MeanBackward0>)\n",
      "times:214 train:tensor(0.2606, grad_fn=<MeanBackward0>) test:tensor(0.2593, grad_fn=<MeanBackward0>)\n",
      "times:215 train:tensor(0.2603, grad_fn=<MeanBackward0>) test:tensor(0.2587, grad_fn=<MeanBackward0>)\n",
      "times:216 train:tensor(0.2601, grad_fn=<MeanBackward0>) test:tensor(0.2584, grad_fn=<MeanBackward0>)\n",
      "times:217 train:tensor(0.2599, grad_fn=<MeanBackward0>) test:tensor(0.2580, grad_fn=<MeanBackward0>)\n",
      "times:218 train:tensor(0.2597, grad_fn=<MeanBackward0>) test:tensor(0.2575, grad_fn=<MeanBackward0>)\n",
      "times:219 train:tensor(0.2595, grad_fn=<MeanBackward0>) test:tensor(0.2573, grad_fn=<MeanBackward0>)\n",
      "times:220 train:tensor(0.2593, grad_fn=<MeanBackward0>) test:tensor(0.2569, grad_fn=<MeanBackward0>)\n",
      "times:221 train:tensor(0.2592, grad_fn=<MeanBackward0>) test:tensor(0.2564, grad_fn=<MeanBackward0>)\n",
      "times:222 train:tensor(0.2590, grad_fn=<MeanBackward0>) test:tensor(0.2562, grad_fn=<MeanBackward0>)\n",
      "times:223 train:tensor(0.2588, grad_fn=<MeanBackward0>) test:tensor(0.2558, grad_fn=<MeanBackward0>)\n",
      "times:224 train:tensor(0.2586, grad_fn=<MeanBackward0>) test:tensor(0.2555, grad_fn=<MeanBackward0>)\n",
      "times:225 train:tensor(0.2585, grad_fn=<MeanBackward0>) test:tensor(0.2554, grad_fn=<MeanBackward0>)\n",
      "times:226 train:tensor(0.2583, grad_fn=<MeanBackward0>) test:tensor(0.2551, grad_fn=<MeanBackward0>)\n",
      "times:227 train:tensor(0.2581, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:228 train:tensor(0.2579, grad_fn=<MeanBackward0>) test:tensor(0.2550, grad_fn=<MeanBackward0>)\n",
      "times:229 train:tensor(0.2577, grad_fn=<MeanBackward0>) test:tensor(0.2551, grad_fn=<MeanBackward0>)\n",
      "times:230 train:tensor(0.2576, grad_fn=<MeanBackward0>) test:tensor(0.2551, grad_fn=<MeanBackward0>)\n",
      "times:231 train:tensor(0.2574, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:232 train:tensor(0.2572, grad_fn=<MeanBackward0>) test:tensor(0.2553, grad_fn=<MeanBackward0>)\n",
      "times:233 train:tensor(0.2570, grad_fn=<MeanBackward0>) test:tensor(0.2553, grad_fn=<MeanBackward0>)\n",
      "times:234 train:tensor(0.2569, grad_fn=<MeanBackward0>) test:tensor(0.2554, grad_fn=<MeanBackward0>)\n",
      "times:235 train:tensor(0.2567, grad_fn=<MeanBackward0>) test:tensor(0.2553, grad_fn=<MeanBackward0>)\n",
      "times:236 train:tensor(0.2565, grad_fn=<MeanBackward0>) test:tensor(0.2553, grad_fn=<MeanBackward0>)\n",
      "times:237 train:tensor(0.2563, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:238 train:tensor(0.2562, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:239 train:tensor(0.2560, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:240 train:tensor(0.2558, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:241 train:tensor(0.2556, grad_fn=<MeanBackward0>) test:tensor(0.2551, grad_fn=<MeanBackward0>)\n",
      "times:242 train:tensor(0.2554, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:243 train:tensor(0.2553, grad_fn=<MeanBackward0>) test:tensor(0.2548, grad_fn=<MeanBackward0>)\n",
      "times:244 train:tensor(0.2551, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:245 train:tensor(0.2549, grad_fn=<MeanBackward0>) test:tensor(0.2543, grad_fn=<MeanBackward0>)\n",
      "times:246 train:tensor(0.2547, grad_fn=<MeanBackward0>) test:tensor(0.2558, grad_fn=<MeanBackward0>)\n",
      "times:247 train:tensor(0.2546, grad_fn=<MeanBackward0>) test:tensor(0.2528, grad_fn=<MeanBackward0>)\n",
      "times:248 train:tensor(0.2546, grad_fn=<MeanBackward0>) test:tensor(0.2581, grad_fn=<MeanBackward0>)\n",
      "times:249 train:tensor(0.2547, grad_fn=<MeanBackward0>) test:tensor(0.2517, grad_fn=<MeanBackward0>)\n",
      "times:250 train:tensor(0.2547, grad_fn=<MeanBackward0>) test:tensor(0.2562, grad_fn=<MeanBackward0>)\n",
      "times:251 train:tensor(0.2540, grad_fn=<MeanBackward0>) test:tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "times:252 train:tensor(0.2537, grad_fn=<MeanBackward0>) test:tensor(0.2525, grad_fn=<MeanBackward0>)\n",
      "times:253 train:tensor(0.2539, grad_fn=<MeanBackward0>) test:tensor(0.2573, grad_fn=<MeanBackward0>)\n",
      "times:254 train:tensor(0.2538, grad_fn=<MeanBackward0>) test:tensor(0.2534, grad_fn=<MeanBackward0>)\n",
      "times:255 train:tensor(0.2533, grad_fn=<MeanBackward0>) test:tensor(0.2542, grad_fn=<MeanBackward0>)\n",
      "times:256 train:tensor(0.2531, grad_fn=<MeanBackward0>) test:tensor(0.2567, grad_fn=<MeanBackward0>)\n",
      "times:257 train:tensor(0.2532, grad_fn=<MeanBackward0>) test:tensor(0.2527, grad_fn=<MeanBackward0>)\n",
      "times:258 train:tensor(0.2531, grad_fn=<MeanBackward0>) test:tensor(0.2555, grad_fn=<MeanBackward0>)\n",
      "times:259 train:tensor(0.2527, grad_fn=<MeanBackward0>) test:tensor(0.2557, grad_fn=<MeanBackward0>)\n",
      "times:260 train:tensor(0.2526, grad_fn=<MeanBackward0>) test:tensor(0.2528, grad_fn=<MeanBackward0>)\n",
      "times:261 train:tensor(0.2527, grad_fn=<MeanBackward0>) test:tensor(0.2560, grad_fn=<MeanBackward0>)\n",
      "times:262 train:tensor(0.2524, grad_fn=<MeanBackward0>) test:tensor(0.2547, grad_fn=<MeanBackward0>)\n",
      "times:263 train:tensor(0.2521, grad_fn=<MeanBackward0>) test:tensor(0.2535, grad_fn=<MeanBackward0>)\n",
      "times:264 train:tensor(0.2521, grad_fn=<MeanBackward0>) test:tensor(0.2558, grad_fn=<MeanBackward0>)\n",
      "times:265 train:tensor(0.2520, grad_fn=<MeanBackward0>) test:tensor(0.2537, grad_fn=<MeanBackward0>)\n",
      "times:266 train:tensor(0.2518, grad_fn=<MeanBackward0>) test:tensor(0.2542, grad_fn=<MeanBackward0>)\n",
      "times:267 train:tensor(0.2516, grad_fn=<MeanBackward0>) test:tensor(0.2553, grad_fn=<MeanBackward0>)\n",
      "times:268 train:tensor(0.2516, grad_fn=<MeanBackward0>) test:tensor(0.2530, grad_fn=<MeanBackward0>)\n",
      "times:269 train:tensor(0.2515, grad_fn=<MeanBackward0>) test:tensor(0.2546, grad_fn=<MeanBackward0>)\n",
      "times:270 train:tensor(0.2513, grad_fn=<MeanBackward0>) test:tensor(0.2544, grad_fn=<MeanBackward0>)\n",
      "times:271 train:tensor(0.2512, grad_fn=<MeanBackward0>) test:tensor(0.2529, grad_fn=<MeanBackward0>)\n",
      "times:272 train:tensor(0.2512, grad_fn=<MeanBackward0>) test:tensor(0.2545, grad_fn=<MeanBackward0>)\n",
      "times:273 train:tensor(0.2510, grad_fn=<MeanBackward0>) test:tensor(0.2536, grad_fn=<MeanBackward0>)\n",
      "times:274 train:tensor(0.2509, grad_fn=<MeanBackward0>) test:tensor(0.2530, grad_fn=<MeanBackward0>)\n",
      "times:275 train:tensor(0.2508, grad_fn=<MeanBackward0>) test:tensor(0.2542, grad_fn=<MeanBackward0>)\n",
      "times:276 train:tensor(0.2507, grad_fn=<MeanBackward0>) test:tensor(0.2528, grad_fn=<MeanBackward0>)\n",
      "times:277 train:tensor(0.2506, grad_fn=<MeanBackward0>) test:tensor(0.2531, grad_fn=<MeanBackward0>)\n",
      "times:278 train:tensor(0.2505, grad_fn=<MeanBackward0>) test:tensor(0.2536, grad_fn=<MeanBackward0>)\n",
      "times:279 train:tensor(0.2504, grad_fn=<MeanBackward0>) test:tensor(0.2523, grad_fn=<MeanBackward0>)\n",
      "times:280 train:tensor(0.2504, grad_fn=<MeanBackward0>) test:tensor(0.2530, grad_fn=<MeanBackward0>)\n",
      "times:281 train:tensor(0.2502, grad_fn=<MeanBackward0>) test:tensor(0.2529, grad_fn=<MeanBackward0>)\n",
      "times:282 train:tensor(0.2502, grad_fn=<MeanBackward0>) test:tensor(0.2519, grad_fn=<MeanBackward0>)\n",
      "times:283 train:tensor(0.2501, grad_fn=<MeanBackward0>) test:tensor(0.2527, grad_fn=<MeanBackward0>)\n",
      "times:284 train:tensor(0.2500, grad_fn=<MeanBackward0>) test:tensor(0.2521, grad_fn=<MeanBackward0>)\n",
      "times:285 train:tensor(0.2499, grad_fn=<MeanBackward0>) test:tensor(0.2517, grad_fn=<MeanBackward0>)\n",
      "times:286 train:tensor(0.2498, grad_fn=<MeanBackward0>) test:tensor(0.2522, grad_fn=<MeanBackward0>)\n",
      "times:287 train:tensor(0.2498, grad_fn=<MeanBackward0>) test:tensor(0.2513, grad_fn=<MeanBackward0>)\n",
      "times:288 train:tensor(0.2497, grad_fn=<MeanBackward0>) test:tensor(0.2514, grad_fn=<MeanBackward0>)\n",
      "times:289 train:tensor(0.2496, grad_fn=<MeanBackward0>) test:tensor(0.2514, grad_fn=<MeanBackward0>)\n",
      "times:290 train:tensor(0.2495, grad_fn=<MeanBackward0>) test:tensor(0.2507, grad_fn=<MeanBackward0>)\n",
      "times:291 train:tensor(0.2494, grad_fn=<MeanBackward0>) test:tensor(0.2510, grad_fn=<MeanBackward0>)\n",
      "times:292 train:tensor(0.2493, grad_fn=<MeanBackward0>) test:tensor(0.2506, grad_fn=<MeanBackward0>)\n",
      "times:293 train:tensor(0.2493, grad_fn=<MeanBackward0>) test:tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "times:294 train:tensor(0.2492, grad_fn=<MeanBackward0>) test:tensor(0.2505, grad_fn=<MeanBackward0>)\n",
      "times:295 train:tensor(0.2491, grad_fn=<MeanBackward0>) test:tensor(0.2498, grad_fn=<MeanBackward0>)\n",
      "times:296 train:tensor(0.2490, grad_fn=<MeanBackward0>) test:tensor(0.2500, grad_fn=<MeanBackward0>)\n",
      "times:297 train:tensor(0.2490, grad_fn=<MeanBackward0>) test:tensor(0.2496, grad_fn=<MeanBackward0>)\n",
      "times:298 train:tensor(0.2489, grad_fn=<MeanBackward0>) test:tensor(0.2493, grad_fn=<MeanBackward0>)\n",
      "times:299 train:tensor(0.2488, grad_fn=<MeanBackward0>) test:tensor(0.2495, grad_fn=<MeanBackward0>)\n",
      "times:300 train:tensor(0.2488, grad_fn=<MeanBackward0>) test:tensor(0.2489, grad_fn=<MeanBackward0>)\n",
      "times:301 train:tensor(0.2487, grad_fn=<MeanBackward0>) test:tensor(0.2491, grad_fn=<MeanBackward0>)\n",
      "times:302 train:tensor(0.2486, grad_fn=<MeanBackward0>) test:tensor(0.2487, grad_fn=<MeanBackward0>)\n",
      "times:303 train:tensor(0.2485, grad_fn=<MeanBackward0>) test:tensor(0.2487, grad_fn=<MeanBackward0>)\n",
      "times:304 train:tensor(0.2485, grad_fn=<MeanBackward0>) test:tensor(0.2485, grad_fn=<MeanBackward0>)\n",
      "times:305 train:tensor(0.2484, grad_fn=<MeanBackward0>) test:tensor(0.2483, grad_fn=<MeanBackward0>)\n",
      "times:306 train:tensor(0.2483, grad_fn=<MeanBackward0>) test:tensor(0.2483, grad_fn=<MeanBackward0>)\n",
      "times:307 train:tensor(0.2483, grad_fn=<MeanBackward0>) test:tensor(0.2479, grad_fn=<MeanBackward0>)\n",
      "times:308 train:tensor(0.2482, grad_fn=<MeanBackward0>) test:tensor(0.2481, grad_fn=<MeanBackward0>)\n",
      "times:309 train:tensor(0.2481, grad_fn=<MeanBackward0>) test:tensor(0.2475, grad_fn=<MeanBackward0>)\n",
      "times:310 train:tensor(0.2481, grad_fn=<MeanBackward0>) test:tensor(0.2480, grad_fn=<MeanBackward0>)\n",
      "times:311 train:tensor(0.2480, grad_fn=<MeanBackward0>) test:tensor(0.2471, grad_fn=<MeanBackward0>)\n",
      "times:312 train:tensor(0.2479, grad_fn=<MeanBackward0>) test:tensor(0.2480, grad_fn=<MeanBackward0>)\n",
      "times:313 train:tensor(0.2479, grad_fn=<MeanBackward0>) test:tensor(0.2464, grad_fn=<MeanBackward0>)\n",
      "times:314 train:tensor(0.2480, grad_fn=<MeanBackward0>) test:tensor(0.2484, grad_fn=<MeanBackward0>)\n",
      "times:315 train:tensor(0.2481, grad_fn=<MeanBackward0>) test:tensor(0.2455, grad_fn=<MeanBackward0>)\n",
      "times:316 train:tensor(0.2484, grad_fn=<MeanBackward0>) test:tensor(0.2487, grad_fn=<MeanBackward0>)\n",
      "times:317 train:tensor(0.2483, grad_fn=<MeanBackward0>) test:tensor(0.2455, grad_fn=<MeanBackward0>)\n",
      "times:318 train:tensor(0.2480, grad_fn=<MeanBackward0>) test:tensor(0.2469, grad_fn=<MeanBackward0>)\n",
      "times:319 train:tensor(0.2475, grad_fn=<MeanBackward0>) test:tensor(0.2478, grad_fn=<MeanBackward0>)\n",
      "times:320 train:tensor(0.2477, grad_fn=<MeanBackward0>) test:tensor(0.2452, grad_fn=<MeanBackward0>)\n",
      "times:321 train:tensor(0.2481, grad_fn=<MeanBackward0>) test:tensor(0.2476, grad_fn=<MeanBackward0>)\n",
      "times:322 train:tensor(0.2475, grad_fn=<MeanBackward0>) test:tensor(0.2470, grad_fn=<MeanBackward0>)\n",
      "times:323 train:tensor(0.2473, grad_fn=<MeanBackward0>) test:tensor(0.2455, grad_fn=<MeanBackward0>)\n",
      "times:324 train:tensor(0.2476, grad_fn=<MeanBackward0>) test:tensor(0.2475, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:325 train:tensor(0.2474, grad_fn=<MeanBackward0>) test:tensor(0.2460, grad_fn=<MeanBackward0>)\n",
      "times:326 train:tensor(0.2471, grad_fn=<MeanBackward0>) test:tensor(0.2457, grad_fn=<MeanBackward0>)\n",
      "times:327 train:tensor(0.2470, grad_fn=<MeanBackward0>) test:tensor(0.2469, grad_fn=<MeanBackward0>)\n",
      "times:328 train:tensor(0.2471, grad_fn=<MeanBackward0>) test:tensor(0.2450, grad_fn=<MeanBackward0>)\n",
      "times:329 train:tensor(0.2471, grad_fn=<MeanBackward0>) test:tensor(0.2458, grad_fn=<MeanBackward0>)\n",
      "times:330 train:tensor(0.2468, grad_fn=<MeanBackward0>) test:tensor(0.2460, grad_fn=<MeanBackward0>)\n",
      "times:331 train:tensor(0.2468, grad_fn=<MeanBackward0>) test:tensor(0.2446, grad_fn=<MeanBackward0>)\n",
      "times:332 train:tensor(0.2469, grad_fn=<MeanBackward0>) test:tensor(0.2459, grad_fn=<MeanBackward0>)\n",
      "times:333 train:tensor(0.2467, grad_fn=<MeanBackward0>) test:tensor(0.2450, grad_fn=<MeanBackward0>)\n",
      "times:334 train:tensor(0.2466, grad_fn=<MeanBackward0>) test:tensor(0.2447, grad_fn=<MeanBackward0>)\n",
      "times:335 train:tensor(0.2465, grad_fn=<MeanBackward0>) test:tensor(0.2456, grad_fn=<MeanBackward0>)\n",
      "times:336 train:tensor(0.2466, grad_fn=<MeanBackward0>) test:tensor(0.2442, grad_fn=<MeanBackward0>)\n",
      "times:337 train:tensor(0.2466, grad_fn=<MeanBackward0>) test:tensor(0.2453, grad_fn=<MeanBackward0>)\n",
      "times:338 train:tensor(0.2464, grad_fn=<MeanBackward0>) test:tensor(0.2446, grad_fn=<MeanBackward0>)\n",
      "times:339 train:tensor(0.2463, grad_fn=<MeanBackward0>) test:tensor(0.2444, grad_fn=<MeanBackward0>)\n",
      "times:340 train:tensor(0.2462, grad_fn=<MeanBackward0>) test:tensor(0.2450, grad_fn=<MeanBackward0>)\n",
      "times:341 train:tensor(0.2462, grad_fn=<MeanBackward0>) test:tensor(0.2440, grad_fn=<MeanBackward0>)\n",
      "times:342 train:tensor(0.2462, grad_fn=<MeanBackward0>) test:tensor(0.2449, grad_fn=<MeanBackward0>)\n",
      "times:343 train:tensor(0.2461, grad_fn=<MeanBackward0>) test:tensor(0.2441, grad_fn=<MeanBackward0>)\n",
      "times:344 train:tensor(0.2460, grad_fn=<MeanBackward0>) test:tensor(0.2444, grad_fn=<MeanBackward0>)\n",
      "times:345 train:tensor(0.2459, grad_fn=<MeanBackward0>) test:tensor(0.2445, grad_fn=<MeanBackward0>)\n",
      "times:346 train:tensor(0.2459, grad_fn=<MeanBackward0>) test:tensor(0.2439, grad_fn=<MeanBackward0>)\n",
      "times:347 train:tensor(0.2458, grad_fn=<MeanBackward0>) test:tensor(0.2446, grad_fn=<MeanBackward0>)\n",
      "times:348 train:tensor(0.2458, grad_fn=<MeanBackward0>) test:tensor(0.2435, grad_fn=<MeanBackward0>)\n",
      "times:349 train:tensor(0.2458, grad_fn=<MeanBackward0>) test:tensor(0.2445, grad_fn=<MeanBackward0>)\n",
      "times:350 train:tensor(0.2457, grad_fn=<MeanBackward0>) test:tensor(0.2433, grad_fn=<MeanBackward0>)\n",
      "times:351 train:tensor(0.2457, grad_fn=<MeanBackward0>) test:tensor(0.2442, grad_fn=<MeanBackward0>)\n",
      "times:352 train:tensor(0.2456, grad_fn=<MeanBackward0>) test:tensor(0.2431, grad_fn=<MeanBackward0>)\n",
      "times:353 train:tensor(0.2455, grad_fn=<MeanBackward0>) test:tensor(0.2439, grad_fn=<MeanBackward0>)\n",
      "times:354 train:tensor(0.2455, grad_fn=<MeanBackward0>) test:tensor(0.2431, grad_fn=<MeanBackward0>)\n",
      "times:355 train:tensor(0.2454, grad_fn=<MeanBackward0>) test:tensor(0.2437, grad_fn=<MeanBackward0>)\n",
      "times:356 train:tensor(0.2453, grad_fn=<MeanBackward0>) test:tensor(0.2431, grad_fn=<MeanBackward0>)\n",
      "times:357 train:tensor(0.2453, grad_fn=<MeanBackward0>) test:tensor(0.2437, grad_fn=<MeanBackward0>)\n",
      "times:358 train:tensor(0.2452, grad_fn=<MeanBackward0>) test:tensor(0.2428, grad_fn=<MeanBackward0>)\n",
      "times:359 train:tensor(0.2452, grad_fn=<MeanBackward0>) test:tensor(0.2439, grad_fn=<MeanBackward0>)\n",
      "times:360 train:tensor(0.2453, grad_fn=<MeanBackward0>) test:tensor(0.2422, grad_fn=<MeanBackward0>)\n",
      "times:361 train:tensor(0.2456, grad_fn=<MeanBackward0>) test:tensor(0.2447, grad_fn=<MeanBackward0>)\n",
      "times:362 train:tensor(0.2460, grad_fn=<MeanBackward0>) test:tensor(0.2415, grad_fn=<MeanBackward0>)\n",
      "times:363 train:tensor(0.2470, grad_fn=<MeanBackward0>) test:tensor(0.2452, grad_fn=<MeanBackward0>)\n",
      "times:364 train:tensor(0.2466, grad_fn=<MeanBackward0>) test:tensor(0.2419, grad_fn=<MeanBackward0>)\n",
      "times:365 train:tensor(0.2457, grad_fn=<MeanBackward0>) test:tensor(0.2427, grad_fn=<MeanBackward0>)\n",
      "times:366 train:tensor(0.2449, grad_fn=<MeanBackward0>) test:tensor(0.2449, grad_fn=<MeanBackward0>)\n",
      "times:367 train:tensor(0.2460, grad_fn=<MeanBackward0>) test:tensor(0.2419, grad_fn=<MeanBackward0>)\n",
      "times:368 train:tensor(0.2466, grad_fn=<MeanBackward0>) test:tensor(0.2435, grad_fn=<MeanBackward0>)\n",
      "times:369 train:tensor(0.2447, grad_fn=<MeanBackward0>) test:tensor(0.2454, grad_fn=<MeanBackward0>)\n",
      "times:370 train:tensor(0.2459, grad_fn=<MeanBackward0>) test:tensor(0.2418, grad_fn=<MeanBackward0>)\n",
      "times:371 train:tensor(0.2470, grad_fn=<MeanBackward0>) test:tensor(0.2436, grad_fn=<MeanBackward0>)\n",
      "times:372 train:tensor(0.2446, grad_fn=<MeanBackward0>) test:tensor(0.2453, grad_fn=<MeanBackward0>)\n",
      "times:373 train:tensor(0.2460, grad_fn=<MeanBackward0>) test:tensor(0.2416, grad_fn=<MeanBackward0>)\n",
      "times:374 train:tensor(0.2469, grad_fn=<MeanBackward0>) test:tensor(0.2426, grad_fn=<MeanBackward0>)\n",
      "times:375 train:tensor(0.2444, grad_fn=<MeanBackward0>) test:tensor(0.2457, grad_fn=<MeanBackward0>)\n",
      "times:376 train:tensor(0.2471, grad_fn=<MeanBackward0>) test:tensor(0.2414, grad_fn=<MeanBackward0>)\n",
      "times:377 train:tensor(0.2467, grad_fn=<MeanBackward0>) test:tensor(0.2419, grad_fn=<MeanBackward0>)\n",
      "times:378 train:tensor(0.2449, grad_fn=<MeanBackward0>) test:tensor(0.2460, grad_fn=<MeanBackward0>)\n",
      "times:379 train:tensor(0.2475, grad_fn=<MeanBackward0>) test:tensor(0.2419, grad_fn=<MeanBackward0>)\n",
      "times:380 train:tensor(0.2449, grad_fn=<MeanBackward0>) test:tensor(0.2417, grad_fn=<MeanBackward0>)\n",
      "times:381 train:tensor(0.2457, grad_fn=<MeanBackward0>) test:tensor(0.2447, grad_fn=<MeanBackward0>)\n",
      "times:382 train:tensor(0.2456, grad_fn=<MeanBackward0>) test:tensor(0.2431, grad_fn=<MeanBackward0>)\n",
      "times:383 train:tensor(0.2442, grad_fn=<MeanBackward0>) test:tensor(0.2414, grad_fn=<MeanBackward0>)\n",
      "times:384 train:tensor(0.2456, grad_fn=<MeanBackward0>) test:tensor(0.2424, grad_fn=<MeanBackward0>)\n",
      "times:385 train:tensor(0.2440, grad_fn=<MeanBackward0>) test:tensor(0.2440, grad_fn=<MeanBackward0>)\n",
      "times:386 train:tensor(0.2453, grad_fn=<MeanBackward0>) test:tensor(0.2411, grad_fn=<MeanBackward0>)\n",
      "times:387 train:tensor(0.2447, grad_fn=<MeanBackward0>) test:tensor(0.2411, grad_fn=<MeanBackward0>)\n",
      "times:388 train:tensor(0.2444, grad_fn=<MeanBackward0>) test:tensor(0.2432, grad_fn=<MeanBackward0>)\n",
      "times:389 train:tensor(0.2450, grad_fn=<MeanBackward0>) test:tensor(0.2413, grad_fn=<MeanBackward0>)\n",
      "times:390 train:tensor(0.2438, grad_fn=<MeanBackward0>) test:tensor(0.2408, grad_fn=<MeanBackward0>)\n",
      "times:391 train:tensor(0.2445, grad_fn=<MeanBackward0>) test:tensor(0.2419, grad_fn=<MeanBackward0>)\n",
      "times:392 train:tensor(0.2439, grad_fn=<MeanBackward0>) test:tensor(0.2420, grad_fn=<MeanBackward0>)\n",
      "times:393 train:tensor(0.2439, grad_fn=<MeanBackward0>) test:tensor(0.2408, grad_fn=<MeanBackward0>)\n",
      "times:394 train:tensor(0.2441, grad_fn=<MeanBackward0>) test:tensor(0.2412, grad_fn=<MeanBackward0>)\n",
      "times:395 train:tensor(0.2436, grad_fn=<MeanBackward0>) test:tensor(0.2424, grad_fn=<MeanBackward0>)\n",
      "times:396 train:tensor(0.2441, grad_fn=<MeanBackward0>) test:tensor(0.2409, grad_fn=<MeanBackward0>)\n",
      "times:397 train:tensor(0.2437, grad_fn=<MeanBackward0>) test:tensor(0.2409, grad_fn=<MeanBackward0>)\n",
      "times:398 train:tensor(0.2436, grad_fn=<MeanBackward0>) test:tensor(0.2421, grad_fn=<MeanBackward0>)\n",
      "times:399 train:tensor(0.2439, grad_fn=<MeanBackward0>) test:tensor(0.2409, grad_fn=<MeanBackward0>)\n",
      "times:400 train:tensor(0.2434, grad_fn=<MeanBackward0>) test:tensor(0.2407, grad_fn=<MeanBackward0>)\n",
      "times:401 train:tensor(0.2435, grad_fn=<MeanBackward0>) test:tensor(0.2416, grad_fn=<MeanBackward0>)\n",
      "times:402 train:tensor(0.2435, grad_fn=<MeanBackward0>) test:tensor(0.2410, grad_fn=<MeanBackward0>)\n",
      "times:403 train:tensor(0.2432, grad_fn=<MeanBackward0>) test:tensor(0.2406, grad_fn=<MeanBackward0>)\n",
      "times:404 train:tensor(0.2434, grad_fn=<MeanBackward0>) test:tensor(0.2411, grad_fn=<MeanBackward0>)\n",
      "times:405 train:tensor(0.2432, grad_fn=<MeanBackward0>) test:tensor(0.2410, grad_fn=<MeanBackward0>)\n",
      "times:406 train:tensor(0.2431, grad_fn=<MeanBackward0>) test:tensor(0.2405, grad_fn=<MeanBackward0>)\n",
      "times:407 train:tensor(0.2432, grad_fn=<MeanBackward0>) test:tensor(0.2409, grad_fn=<MeanBackward0>)\n",
      "times:408 train:tensor(0.2430, grad_fn=<MeanBackward0>) test:tensor(0.2410, grad_fn=<MeanBackward0>)\n",
      "times:409 train:tensor(0.2430, grad_fn=<MeanBackward0>) test:tensor(0.2404, grad_fn=<MeanBackward0>)\n",
      "times:410 train:tensor(0.2430, grad_fn=<MeanBackward0>) test:tensor(0.2406, grad_fn=<MeanBackward0>)\n",
      "times:411 train:tensor(0.2428, grad_fn=<MeanBackward0>) test:tensor(0.2407, grad_fn=<MeanBackward0>)\n",
      "times:412 train:tensor(0.2429, grad_fn=<MeanBackward0>) test:tensor(0.2402, grad_fn=<MeanBackward0>)\n",
      "times:413 train:tensor(0.2429, grad_fn=<MeanBackward0>) test:tensor(0.2403, grad_fn=<MeanBackward0>)\n",
      "times:414 train:tensor(0.2427, grad_fn=<MeanBackward0>) test:tensor(0.2405, grad_fn=<MeanBackward0>)\n",
      "times:415 train:tensor(0.2427, grad_fn=<MeanBackward0>) test:tensor(0.2400, grad_fn=<MeanBackward0>)\n",
      "times:416 train:tensor(0.2427, grad_fn=<MeanBackward0>) test:tensor(0.2402, grad_fn=<MeanBackward0>)\n",
      "times:417 train:tensor(0.2426, grad_fn=<MeanBackward0>) test:tensor(0.2402, grad_fn=<MeanBackward0>)\n",
      "times:418 train:tensor(0.2426, grad_fn=<MeanBackward0>) test:tensor(0.2399, grad_fn=<MeanBackward0>)\n",
      "times:419 train:tensor(0.2426, grad_fn=<MeanBackward0>) test:tensor(0.2401, grad_fn=<MeanBackward0>)\n",
      "times:420 train:tensor(0.2424, grad_fn=<MeanBackward0>) test:tensor(0.2400, grad_fn=<MeanBackward0>)\n",
      "times:421 train:tensor(0.2424, grad_fn=<MeanBackward0>) test:tensor(0.2398, grad_fn=<MeanBackward0>)\n",
      "times:422 train:tensor(0.2424, grad_fn=<MeanBackward0>) test:tensor(0.2400, grad_fn=<MeanBackward0>)\n",
      "times:423 train:tensor(0.2423, grad_fn=<MeanBackward0>) test:tensor(0.2398, grad_fn=<MeanBackward0>)\n",
      "times:424 train:tensor(0.2422, grad_fn=<MeanBackward0>) test:tensor(0.2397, grad_fn=<MeanBackward0>)\n",
      "times:425 train:tensor(0.2422, grad_fn=<MeanBackward0>) test:tensor(0.2399, grad_fn=<MeanBackward0>)\n",
      "times:426 train:tensor(0.2422, grad_fn=<MeanBackward0>) test:tensor(0.2396, grad_fn=<MeanBackward0>)\n",
      "times:427 train:tensor(0.2421, grad_fn=<MeanBackward0>) test:tensor(0.2396, grad_fn=<MeanBackward0>)\n",
      "times:428 train:tensor(0.2420, grad_fn=<MeanBackward0>) test:tensor(0.2397, grad_fn=<MeanBackward0>)\n",
      "times:429 train:tensor(0.2420, grad_fn=<MeanBackward0>) test:tensor(0.2394, grad_fn=<MeanBackward0>)\n",
      "times:430 train:tensor(0.2420, grad_fn=<MeanBackward0>) test:tensor(0.2395, grad_fn=<MeanBackward0>)\n",
      "times:431 train:tensor(0.2419, grad_fn=<MeanBackward0>) test:tensor(0.2394, grad_fn=<MeanBackward0>)\n",
      "times:432 train:tensor(0.2418, grad_fn=<MeanBackward0>) test:tensor(0.2393, grad_fn=<MeanBackward0>)\n",
      "times:433 train:tensor(0.2418, grad_fn=<MeanBackward0>) test:tensor(0.2394, grad_fn=<MeanBackward0>)\n",
      "times:434 train:tensor(0.2418, grad_fn=<MeanBackward0>) test:tensor(0.2392, grad_fn=<MeanBackward0>)\n",
      "times:435 train:tensor(0.2417, grad_fn=<MeanBackward0>) test:tensor(0.2393, grad_fn=<MeanBackward0>)\n",
      "times:436 train:tensor(0.2416, grad_fn=<MeanBackward0>) test:tensor(0.2392, grad_fn=<MeanBackward0>)\n",
      "times:437 train:tensor(0.2416, grad_fn=<MeanBackward0>) test:tensor(0.2391, grad_fn=<MeanBackward0>)\n",
      "times:438 train:tensor(0.2415, grad_fn=<MeanBackward0>) test:tensor(0.2392, grad_fn=<MeanBackward0>)\n",
      "times:439 train:tensor(0.2415, grad_fn=<MeanBackward0>) test:tensor(0.2390, grad_fn=<MeanBackward0>)\n",
      "times:440 train:tensor(0.2414, grad_fn=<MeanBackward0>) test:tensor(0.2391, grad_fn=<MeanBackward0>)\n",
      "times:441 train:tensor(0.2414, grad_fn=<MeanBackward0>) test:tensor(0.2389, grad_fn=<MeanBackward0>)\n",
      "times:442 train:tensor(0.2413, grad_fn=<MeanBackward0>) test:tensor(0.2390, grad_fn=<MeanBackward0>)\n",
      "times:443 train:tensor(0.2413, grad_fn=<MeanBackward0>) test:tensor(0.2389, grad_fn=<MeanBackward0>)\n",
      "times:444 train:tensor(0.2412, grad_fn=<MeanBackward0>) test:tensor(0.2389, grad_fn=<MeanBackward0>)\n",
      "times:445 train:tensor(0.2412, grad_fn=<MeanBackward0>) test:tensor(0.2388, grad_fn=<MeanBackward0>)\n",
      "times:446 train:tensor(0.2411, grad_fn=<MeanBackward0>) test:tensor(0.2387, grad_fn=<MeanBackward0>)\n",
      "times:447 train:tensor(0.2411, grad_fn=<MeanBackward0>) test:tensor(0.2388, grad_fn=<MeanBackward0>)\n",
      "times:448 train:tensor(0.2410, grad_fn=<MeanBackward0>) test:tensor(0.2386, grad_fn=<MeanBackward0>)\n",
      "times:449 train:tensor(0.2410, grad_fn=<MeanBackward0>) test:tensor(0.2387, grad_fn=<MeanBackward0>)\n",
      "times:450 train:tensor(0.2410, grad_fn=<MeanBackward0>) test:tensor(0.2385, grad_fn=<MeanBackward0>)\n",
      "times:451 train:tensor(0.2410, grad_fn=<MeanBackward0>) test:tensor(0.2387, grad_fn=<MeanBackward0>)\n",
      "times:452 train:tensor(0.2410, grad_fn=<MeanBackward0>) test:tensor(0.2384, grad_fn=<MeanBackward0>)\n",
      "times:453 train:tensor(0.2412, grad_fn=<MeanBackward0>) test:tensor(0.2392, grad_fn=<MeanBackward0>)\n",
      "times:454 train:tensor(0.2416, grad_fn=<MeanBackward0>) test:tensor(0.2385, grad_fn=<MeanBackward0>)\n",
      "times:455 train:tensor(0.2432, grad_fn=<MeanBackward0>) test:tensor(0.2424, grad_fn=<MeanBackward0>)\n",
      "times:456 train:tensor(0.2462, grad_fn=<MeanBackward0>) test:tensor(0.2441, grad_fn=<MeanBackward0>)\n",
      "times:457 train:tensor(0.2627, grad_fn=<MeanBackward0>) test:tensor(0.3098, grad_fn=<MeanBackward0>)\n",
      "times:458 train:tensor(0.3256, grad_fn=<MeanBackward0>) test:tensor(0.3001, grad_fn=<MeanBackward0>)\n",
      "times:459 train:tensor(0.3605, grad_fn=<MeanBackward0>) test:tensor(0.3296, grad_fn=<MeanBackward0>)\n",
      "times:460 train:tensor(0.3947, grad_fn=<MeanBackward0>) test:tensor(0.3199, grad_fn=<MeanBackward0>)\n",
      "times:461 train:tensor(0.3813, grad_fn=<MeanBackward0>) test:tensor(0.3046, grad_fn=<MeanBackward0>)\n",
      "times:462 train:tensor(0.3595, grad_fn=<MeanBackward0>) test:tensor(0.2879, grad_fn=<MeanBackward0>)\n",
      "times:463 train:tensor(0.3336, grad_fn=<MeanBackward0>) test:tensor(0.2742, grad_fn=<MeanBackward0>)\n",
      "times:464 train:tensor(0.3059, grad_fn=<MeanBackward0>) test:tensor(0.2705, grad_fn=<MeanBackward0>)\n",
      "times:465 train:tensor(0.2864, grad_fn=<MeanBackward0>) test:tensor(0.2811, grad_fn=<MeanBackward0>)\n",
      "times:466 train:tensor(0.2842, grad_fn=<MeanBackward0>) test:tensor(0.3059, grad_fn=<MeanBackward0>)\n",
      "times:467 train:tensor(0.2944, grad_fn=<MeanBackward0>) test:tensor(0.3242, grad_fn=<MeanBackward0>)\n",
      "times:468 train:tensor(0.2957, grad_fn=<MeanBackward0>) test:tensor(0.3019, grad_fn=<MeanBackward0>)\n",
      "times:469 train:tensor(0.2804, grad_fn=<MeanBackward0>) test:tensor(0.2755, grad_fn=<MeanBackward0>)\n",
      "times:470 train:tensor(0.2704, grad_fn=<MeanBackward0>) test:tensor(0.2680, grad_fn=<MeanBackward0>)\n",
      "times:471 train:tensor(0.2709, grad_fn=<MeanBackward0>) test:tensor(0.2729, grad_fn=<MeanBackward0>)\n",
      "times:472 train:tensor(0.2754, grad_fn=<MeanBackward0>) test:tensor(0.2810, grad_fn=<MeanBackward0>)\n",
      "times:473 train:tensor(0.2779, grad_fn=<MeanBackward0>) test:tensor(0.2880, grad_fn=<MeanBackward0>)\n",
      "times:474 train:tensor(0.2769, grad_fn=<MeanBackward0>) test:tensor(0.2924, grad_fn=<MeanBackward0>)\n",
      "times:475 train:tensor(0.2733, grad_fn=<MeanBackward0>) test:tensor(0.2952, grad_fn=<MeanBackward0>)\n",
      "times:476 train:tensor(0.2688, grad_fn=<MeanBackward0>) test:tensor(0.2986, grad_fn=<MeanBackward0>)\n",
      "times:477 train:tensor(0.2654, grad_fn=<MeanBackward0>) test:tensor(0.3039, grad_fn=<MeanBackward0>)\n",
      "times:478 train:tensor(0.2641, grad_fn=<MeanBackward0>) test:tensor(0.3097, grad_fn=<MeanBackward0>)\n",
      "times:479 train:tensor(0.2646, grad_fn=<MeanBackward0>) test:tensor(0.3120, grad_fn=<MeanBackward0>)\n",
      "times:480 train:tensor(0.2655, grad_fn=<MeanBackward0>) test:tensor(0.3076, grad_fn=<MeanBackward0>)\n",
      "times:481 train:tensor(0.2655, grad_fn=<MeanBackward0>) test:tensor(0.2980, grad_fn=<MeanBackward0>)\n",
      "times:482 train:tensor(0.2642, grad_fn=<MeanBackward0>) test:tensor(0.2869, grad_fn=<MeanBackward0>)\n",
      "times:483 train:tensor(0.2621, grad_fn=<MeanBackward0>) test:tensor(0.2781, grad_fn=<MeanBackward0>)\n",
      "times:484 train:tensor(0.2602, grad_fn=<MeanBackward0>) test:tensor(0.2735, grad_fn=<MeanBackward0>)\n",
      "times:485 train:tensor(0.2601, grad_fn=<MeanBackward0>) test:tensor(0.2721, grad_fn=<MeanBackward0>)\n",
      "times:486 train:tensor(0.2611, grad_fn=<MeanBackward0>) test:tensor(0.2706, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:487 train:tensor(0.2605, grad_fn=<MeanBackward0>) test:tensor(0.2693, grad_fn=<MeanBackward0>)\n",
      "times:488 train:tensor(0.2587, grad_fn=<MeanBackward0>) test:tensor(0.2700, grad_fn=<MeanBackward0>)\n",
      "times:489 train:tensor(0.2580, grad_fn=<MeanBackward0>) test:tensor(0.2713, grad_fn=<MeanBackward0>)\n",
      "times:490 train:tensor(0.2584, grad_fn=<MeanBackward0>) test:tensor(0.2704, grad_fn=<MeanBackward0>)\n",
      "times:491 train:tensor(0.2586, grad_fn=<MeanBackward0>) test:tensor(0.2664, grad_fn=<MeanBackward0>)\n",
      "times:492 train:tensor(0.2578, grad_fn=<MeanBackward0>) test:tensor(0.2615, grad_fn=<MeanBackward0>)\n",
      "times:493 train:tensor(0.2567, grad_fn=<MeanBackward0>) test:tensor(0.2577, grad_fn=<MeanBackward0>)\n",
      "times:494 train:tensor(0.2562, grad_fn=<MeanBackward0>) test:tensor(0.2557, grad_fn=<MeanBackward0>)\n",
      "times:495 train:tensor(0.2562, grad_fn=<MeanBackward0>) test:tensor(0.2551, grad_fn=<MeanBackward0>)\n",
      "times:496 train:tensor(0.2562, grad_fn=<MeanBackward0>) test:tensor(0.2553, grad_fn=<MeanBackward0>)\n",
      "times:497 train:tensor(0.2557, grad_fn=<MeanBackward0>) test:tensor(0.2564, grad_fn=<MeanBackward0>)\n",
      "times:498 train:tensor(0.2547, grad_fn=<MeanBackward0>) test:tensor(0.2586, grad_fn=<MeanBackward0>)\n",
      "times:499 train:tensor(0.2540, grad_fn=<MeanBackward0>) test:tensor(0.2615, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_x = []\n",
    "loss_train = []\n",
    "loss_tes =[]\n",
    "#loss_tes2 =[]\n",
    "for n in range(500):\n",
    "    loss_x.append(n)\n",
    "    train_out = rnn(x_train)\n",
    "    #print(train_out.size())\n",
    "    \n",
    "    loss = loss_func(train_out, y_train)\n",
    "    #print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_train.append(loss.data.item())\n",
    "    \n",
    "    test_out = rnn(x_test)\n",
    "    loss_test = loss_func(test_out,y_test)\n",
    "    loss_tes.append(loss_test.data.item())\n",
    "    \n",
    "    #test_out2 = rnn(x_test2)\n",
    "    #loss_test2 = loss_func(test_out2,y_test2)\n",
    "    #loss_tes2.append(loss_test2.data.item())\n",
    "    print('times:'+str(n)+' train:'+str(loss)+' test:'+ str(loss_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3XmcHHWd//HXp4/pmclMzhlCSAgZjkCQI4EhAQG5MQQW5BBFs4ALZv0JirsLK6wnrLsPXF1EVwTRDex6gNxEhAXFYOQm0QDhTAiBTEKuyTmZs7s/vz+qptMJc3SOnp5MvZ+PRz+6q7q6+jNFmPd861v1/Zq7IyIiAhArdQEiItJ/KBRERCRHoSAiIjkKBRERyVEoiIhIjkJBRERyFAoiIpKjUBARkRyFgoiI5CRKXcD2qqmp8XHjxpW6DBGR3cq8efPWuHttb9vtdqEwbtw45s6dW+oyRER2K2b2XiHb6fSRiIjkKBRERCRHoSAiIjm7XZ+CiAw8HR0dNDQ00NraWupSdnvl5eWMGTOGZDK5Q59XKIhIyTU0NFBdXc24ceMws1KXs9tydxobG2loaKCurm6H9qHTRyJScq2trYwYMUKBsJPMjBEjRuxUi0uhICL9ggJh19jZ46hQ6MX8petZsGxDqcsQEekTRQsFM5tpZqvMbEE37w8xs9+a2ctm9pqZfa5YteyMT9zyDGf919OlLkNEpE8Us6VwJzC1h/evAF5398OBE4H/NLOyItYjItKl9evX85Of/GS7Pzdt2jTWr1+/3Z+79NJLue+++7b7c32haKHg7nOAtT1tAlRbcAKsKtw2Xax6RES6010oZDKZHj/36KOPMnTo0GKVVRKlvCT1x8AsYDlQDXzK3bNdbWhmM4AZAGPHju2zAkWk713/29d4ffnGXbrPg/cazLf+5iPdvn/ttdfyzjvvMHHiRJLJJFVVVYwaNYr58+fz+uuv84lPfIKlS5fS2trKVVddxYwZM4AtY7E1NTVxxhlncNxxx/Hss88yevRoHn74YSoqKnqt7cknn+Tqq68mnU5z1FFHceutt5JKpbj22muZNWsWiUSC008/ne9///vce++9XH/99cTjcYYMGcKcOXN22THqVMpQ+DgwHzgZ2A/4vZn92d0/9K/B3W8Hbgeor6/3Pq1SRAa8G2+8kQULFjB//nyeeuopzjzzTBYsWJC71n/mzJkMHz6clpYWjjrqKM4//3xGjBix1T4WLlzIXXfdxc9+9jMuvPBC7r//fqZPn97j97a2tnLppZfy5JNPMn78eC6++GJuvfVWLr74Yh588EHefPNNzCx3iuqGG27g8ccfZ/To0Tt02qoQpQyFzwE3ursDi8zsXeAg4MUS1iQiJdbTX/R9ZfLkyVvd/PWjH/2IBx98EIClS5eycOHCD4VCXV0dEydOBODII49kyZIlvX7PW2+9RV1dHePHjwfgkksu4ZZbbuHKK6+kvLycyy+/nDPPPJOzzjoLgGOPPZZLL72UCy+8kPPOO29X/KgfUspLUt8HTgEws5HAgcDiEtYjIgLAoEGDcq+feuop/vCHP/Dcc8/x8ssvM2nSpC5vDkulUrnX8XicdLr3LtLgb+IPSyQSvPjii5x//vk89NBDTJ0aXLNz22238Z3vfIelS5cyceJEGhsbt/dH61XRWgpmdhfBVUU1ZtYAfAtIArj7bcC/Anea2auAAV919zXFqkdEpDvV1dVs2rSpy/c2bNjAsGHDqKys5M033+T555/fZd970EEHsWTJEhYtWsT+++/PL37xC0444QSamppobm5m2rRpHH300ey///4AvPPOO0yZMoUpU6bw29/+lqVLl36oxbKzihYK7n5RL+8vB04v1veLiBRqxIgRHHvssRxyyCFUVFQwcuTI3HtTp07ltttu47DDDuPAAw/k6KOP3mXfW15ezh133MEnP/nJXEfzF77wBdauXcs555xDa2sr7s4PfvADAK655hoWLlyIu3PKKadw+OGH77JaOll3zZf+qr6+3vty5rVx1/4OgCU3ntln3ykSNW+88QYTJkwodRkDRlfH08zmuXt9b5/VMBciIpKjobNFRIrkiiuu4Jlnntlq3VVXXcXnPtcvR/UBFAoiIkVzyy23lLqE7abTRyIikqNQEBGRHIWCiIjkKBRERCRHoSAikbej8ykA3HzzzTQ3N/e4zbhx41izZvcYsEGhICKRV+xQ2J3oklQR6V8euxZWvLpr97nnoXDGjd2+nT+fwmmnncYee+zBPffcQ1tbG+eeey7XX389mzdv5sILL6ShoYFMJsM3vvENVq5cyfLlyznppJOoqalh9uzZvZZy0003MXPmTAAuv/xyvvKVr3S570996lNdzqlQbAoFEYm8/PkUnnjiCe677z5efPFF3J2zzz6bOXPmsHr1avbaay9+97tg6JsNGzYwZMgQbrrpJmbPnk1NTU2v3zNv3jzuuOMOXnjhBdydKVOmcMIJJ7B48eIP7Xvt2rVdzqlQbAoFEelfeviLvi888cQTPPHEE0yaNAmApqYmFi5cyPHHH8/VV1/NV7/6Vc466yyOP/747d73008/zbnnnpsbmvu8887jz3/+M1OnTv3QvtPpdJdzKhSb+hRERPK4O9dddx3z589n/vz5LFq0iMsuu4zx48czb948Dj30UK677jpuuOGGHdp3V7rad3dzKhSbQkFEIi9/PoWPf/zjzJw5k6amJgCWLVvGqlWrWL58OZWVlUyfPp2rr76av/zlLx/6bG8+9rGP8dBDD9Hc3MzmzZt58MEHOf7447vcd1NTExs2bGDatGncfPPNzJ8/vzg//DZ0+khEIi9/PoUzzjiDz3zmMxxzzDEAVFVV8ctf/pJFixZxzTXXEIvFSCaT3HrrrQDMmDGDM844g1GjRvXa0XzEEUdw6aWXMnnyZCDoaJ40aRKPP/74h/a9adOmLudUKLaizadgZjOBs4BV7n5IN9ucCNxMMCPbGnc/obf9aj4FkYFH8ynsWv11PoU7gW5PgpnZUOAnwNnu/hHgk0WsRUREClDM6TjnmNm4Hjb5DPCAu78fbr+qWLWIiPSFKVOm0NbWttW6X/ziFxx66KElqmj7lbJPYTyQNLOngGrgh+7+v11taGYzgBkAY8eO7bMCRaTvuDtmVuoydsoLL7xQ6hK6vcKpUKW8+igBHAmcCXwc+IaZje9qQ3e/3d3r3b2+tra2L2sUkT5QXl5OY2PjTv9Cizp3p7GxkfLy8h3eRylbCg0Encubgc1mNgc4HHi7hDWJSAmMGTOGhoYGVq9eXepSdnvl5eWMGTNmhz9fylB4GPixmSWAMmAK0DfXXIlIv5JMJqmrqyt1GUIRQ8HM7gJOBGrMrAH4FsGlp7j7be7+hpn9H/AKkAV+7u4LilWPiIj0rphXH11UwDbfA75XrBpERGT7aJgLERHJUSiIiEiOQkFERHIUCiIikqNQEBGRHIWCiIjkKBRERCRHoSAiIjkKBRERyVEoiIhIjkJBRERyFAoiIpKjUBARkRyFgoiI5CgUREQkR6EgIiI5RQsFM5tpZqvMrMfZ1MzsKDPLmNkFxapFREQKU8yWwp3A1J42MLM48F3g8SLWISIiBSpaKLj7HGBtL5t9CbgfWFWsOnbK0zezpPwzpGgvdSUiIn2iZH0KZjYaOBe4rYBtZ5jZXDObu3r16uIX1+m5HwNQTUvffaeISAmVsqP5ZuCr7p7pbUN3v93d6929vra2tg9K25rhff6dIiKlkCjhd9cDd5sZQA0wzczS7v5QCWsSEYm0koWCu9d1vjazO4FHFAgiIqVVtFAws7uAE4EaM2sAvgUkAdy9136E/sFKXYCISJ8qWii4+0Xbse2lxapj56gvQUSiRXc0F8hdASEiA59CoQeeO33kKBNEJAoUCgWI4TqRJCKRoFAoQAwnq6aCiESAQqEAptNHIhIRCoWeBDfWETPXCSQRiQSFQk/C5oGRVUtBRCJBoVAA3cImIlGhUOhJ5+kjsupoFpFIUCj0JMyBmDqaRSQiFAoFMHUzi0hEKBQKEFySqlgQkYFPoVAA3dEsIlGhUOhBZxDEcDxb0lJERPqEQqEAMbJqK4hIJPQaCmY23syeNLMF4fJhZvb1Aj4308xWdX6ui/c/a2avhI9nzezw7S+/yGzLk7oURCQKCmkp/Ay4DugAcPdXgE8X8Lk7gak9vP8ucIK7Hwb8K3B7AfssCSOrdoKIREIhoVDp7i9usy7d24fcfQ6wtof3n3X3deHi88CYAmopiZiuPhKRiCgkFNaY2X6E/a5mdgHwwS6u4zLgsV28z50X5oDhZJUJIhIBhczRfAXBqZ2DzGwZwWmf6buqADM7iSAUjuthmxnADICxY8fuqq8uWHBJqlJBRAa+XkPB3RcDp5rZICDm7pt21Zeb2WHAz4Ez3L2xhxpuJ+xzqK+v77vfzuHYR4ajTBCRKOg1FMzsm9ssA+DuN+zMF5vZWOAB4G/d/e2d2VexxdTRLCIRUcjpo815r8uBs4A3evuQmd0FnAjUmFkD8C0gCeDutwHfBEYAPwmDJu3u9dtTfF/RJakiEhWFnD76z/xlM/s+MKuAz13Uy/uXA5f3tp9Syr+jWUNni0gU7MgdzZXAvru6kP4sZjp9JCLRUEifwqts+aM5DtQCO9WfsDvSfQoiEgWF9Cmclfc6Dax0915vXhsQcpPsaI5mEYmGbkPBzIaHL7e9BHWwmeHu3d6tPGDkpuNUIohINPTUUphH8LdyV/PWOxHqV1BHs4hERbeh4O51fVlIf2aao1lEIqKQPgXMbBhwAMF9CkBuwLtI0BzNIhIVhVx9dDlwFcEopvOBo4HngJOLW1r/EXQ0KxZEZOAr5D6Fq4CjgPfc/SRgErC6qFX1M5qjWUSiopBQaHX3VgAzS7n7m8CBxS2rf+gMAtN8CiISEYX0KTSY2VDgIeD3ZrYOWF7csvqLLaOkKhNEJAoKGfvo3PDlt81sNjAE+L+iVtVfhEmg00ciEhU93bz2O+DXwEPuvhnA3f/UV4X1J2opiEhU9NSncDvBEBdLzOw3ZvYJMyvro7r6h/C2Pc28JiJR0W0ouPvD4fDXnZPhXAK8b2Yzzey0viqwP4jhZLOlrkJEpPh6vfrI3Vvc/Tdh38LpBJekRqNPIWRk1VIQkUjoNRTMbKSZfcnMniG4AukJ4MgCPjfTzFaZ2YJu3jcz+5GZLTKzV8zsiO2uvo9o5jURiYpuQ8HMPm9mfwT+AowH/tnd93X3r7r7/AL2fScwtYf3zyAYOuMAYAZwa8FV9xGnc5RUnTsSkWjo6ZLUjwI3An9w9+3+rejuc8xsXA+bnAP8rwd3hT1vZkPNbJS7f7C931U0nZekmq4+EpFo6GmU1M8V+btHA0vzlhvCdf0nFEKmobNFJCJ2ZI7mXaW7eRo+vKHZDDOba2ZzV6/uw2GXLO+O5r77VhGRkillKDQAe+ctj6Gb4TPc/XZ3r3f3+tra2j4pLl9MYx+JSEQUcvXRfmaWCl+faGZfDsdC2lmzgIvDq5COBjb0q/6EPBrmQkSiopCWwv1Axsz2B/4bqCMY/qJHZnYXwbwLB5pZg5ldZmZfMLMvhJs8CiwGFgE/A764Iz9AX9AwFyISFYWMkpp197SZnQvc7O7/ZWZ/7e1D4d3QPb3vwBUF1llShtNNd4eIyIBSSEuhw8wuIhjm4pFwXbJ4JfUjYQ7EyJJVJohIBBQSCp8DjgH+zd3fNbM64JfFLat/6MyBmE4fiUhEFDKfwuvAlwHMbBhQ7e43FruwfiH/klSlgohEQCFXHz1lZoPNbDjwMnCHmd1U/NL6D119JCJRUcjpoyHuvhE4D7jD3Y8ETi1uWf2Lrj4SkagoJBQSZjYKuJAtHc2RotNHIhIVhYTCDcDjwDvu/pKZ7QssLG5Z/YtOH4lIVBTS0XwvcG/e8mLg/GIW1X90Dp2t00ciEg2FdDSPMbMHwwlzVprZ/WY2pi+K6y8085qIREUhp4/uIBinaC+Coa1/G64b8DqDQDOviUhUFBIKte5+h7unw8edQN8PVVpCwR3NSgURGfgKCYU1ZjbdzOLhYzrQWOzC+hN1NItIVBQSCn9HcDnqCoJZ0S4gGPpi4AtbB2au8fBEJBJ6DQV3f9/dz3b3Wnffw90/QXAjW2QEM68pFURk4NvRmdf+cZdW0U9Z2FLQJakiEhU7Ggpdza888HgW0NDZIhIdOxoKBf2KNLOpZvaWmS0ys2u7eH+smc02s7+a2StmNm0H6ymS/EtSlQoiMvB1e0ezmW2i61/+BlT0tmMziwO3AKcBDcBLZjYrHIq709eBe9z9VjM7mGCKznGFl19kuZaCehREJBq6DQV3r97JfU8GFoXDYmBmdwPnAPmh4MDg8PUQYPlOfucu1dmnYGTVpyAikVDIHM07ajSwNG+5AZiyzTbfBp4wsy8Bg+h3Q3Jv6WjWNakiEgU72qdQiK46o7f9zXoRcKe7jwGmAb8wsw/VZGYzzGyumc1dvXp1EUrtRnj6yHB1NItIJBQzFBqAvfOWx/Dh00OXAfcAuPtzQDlQs+2O3P12d6939/ra2r4cYUOXpIpItBQzFF4CDjCzOjMrAz5NMLBevveBUwDMbAJBKPRhU6BnltdSUFeziERB0ULB3dPAlQQT9LxBcJXRa2Z2g5mdHW72T8Dnzexl4C7gUu9P137mh0L/qUpEpGiK2dGMuz9KcJlp/rpv5r1+HTi2mDXsCrokVUSiopinj3Z7lndHc39qwIiIFItCoUfqaBaRaFEo9KCzpYA6mkUkIhQKPVJLQUSiRaHQEw2dLSIRo1DogRF2NJtrjmYRiQSFQk9yA+KpR0FEokGh0ANjSygoFUQkChQK3ck7XRQjq7aCiESCQqE7uctRO2deK10pIiJ9RaHQnbxQ0BzNIhIVCoXu5DUNDCejpoKIRIBCoTtbtRSclvZ0CYsREekbCoVubd1SaGpVKIjIwKdQ6M42LYWNCgURiQCFQne2uiTVaWpTKIjIwFfUUDCzqWb2lpktMrNru9nmQjN73cxeM7NfF7Oe7ZLXUkCnj0QkIoo285qZxYFbgNOABuAlM5sVzrbWuc0BwHXAse6+zsz2KFY92y0vFMriqKUgIpFQzJbCZGCRuy9293bgbuCcbbb5PHCLu68DcPdVRaxnO205fVQWg00KBRGJgGKGwmhgad5yQ7gu33hgvJk9Y2bPm9nUItazffL6FJIx2NTaUcJiRET6RtFOHxGMDrGtbe8ASwAHACcCY4A/m9kh7r5+qx2ZzQBmAIwdO3bXV9oVz28pqE9BRKKhmC2FBmDvvOUxwPIutnnY3Tvc/V3gLYKQ2Iq73+7u9e5eX1tbW7SCt/7SLX0Kg6xVfQoiEgnFDIWXgAPMrM7MyoBPA7O22eYh4CQAM6shOJ20uIg1FS4vFKq8ieb2DBkNgCQiA1zRQsHd08CVwOPAG8A97v6amd1gZmeHmz0ONJrZ68Bs4Bp3byxWTdtnSwBUZjcD6lcQkYGvmH0KuPujwKPbrPtm3msH/jF89C9hS2G9D6Iq2wTAksZmJlaWlbIqEZGi0h3N3Qk7mtd7FYlsOynaeWvFxhIXJSJSXAqF7oQthQ0MAmCPZAtvrWgqZUUiIkWnUOhW0FLY4EEoHDoCFizbUMqCRESKTqHQnc4+BaoAOH7vJC+9t5YVG1pLWZWISFEpFLqTCa40avTBAJy0Twp3uOOZd0tZlYhIUUUmFOa8vZrTf/An1je3F/aBjhYAVvkwAPYsa+PC+jH8/Ol3mfXytvfgiYgMDJEJheGDynh7ZROPLVhR2AfSbQAs8xHB8qYP+MZZBzNp76F8+a6/ct5PnuH2Oe/wp7dXs3RtMx2ZbA87ExHZPRT1PoX+5CN7DaauZhDXPfAqC5Zt4PgDajh2/xqqy5NdfyAdthQYRqZyD+Kr3qC6PMmvP380v3j+Pe5+8X3+/dE3c5vHDEYOLmevoRWMHloRPA+rYPTQYN2oIRUMLk9g1tWQUCIi/UNkQsHM+JdpE/jvpxfzqxfe51cvvE9VKsGZh47iyHHD+JvD9qKiLL7lAx1Bh3Krl5GuPZj4qtcAKEvEuOy4Oi47ro61m9tZuHITSxo3s2x9K8vWtbBsfTPzl67nsQUf0JHZeliMyrI4o4aUs0d1OYNSCapScSpTCQaVxakoS1BZFqciGaeiLE5l+KhIBuuryhNUlyeoTiUpT8YULiJSFJEJBYDTDh7JaQePZN3mdhauauKuF9/n0Vc/4Ddzl/Ldx97k7Il78cUT96e2OpVrKbRSRrpmAqn5MyGThviWQzZ8UBlT9h3BlH1HfOi7sllndVMbDetaWL6+hRUbWvlgQysrNrawamMby9a3sLktTXN7mqa2NK0dhZ9+SsQsFxJVqWQYFuFyeYLq8iRVqQSDO5dTya1CpXO7ZDwyZw9FpECRCoVOwwaVMbluOJPrhpPNOi8tWcudzy7hV8+/zyOvfMD/fG4yB3e2FEiSHnUkzLsV3n8W6j5W0HfEYsbIweWMHFzOkfsM63X7bNZpTWdobs/Q0h48N7enaenI0NyWoaktzaa2NJtaO2hqTbOpNQiTTa3BuhUbW1m4qnNdx4daKV0pT8aoSiUZHAbGiKoUtVUpaqu3fuxRnWLUkArKEgoRkYEukqGQLxaz3F/7b67YyOfueIlP3f4cfzhhHSOBNi+jbd9ToKwKXr674FDYkToqyxJUlu38fxJ3py2dzQuOIEg2brO8KS9UNramWbmxlQXLNtC4uf1DI8LGDEYNqWCfEZWMHV7J3sMrGT+ymoP3GsxeQ8p1OktkgIh8KOQ7aM/B3Pf/PsoZN8/hjwuWchFBS8GSg+DwT8PcO+Coy2H0EaUutUdmRnkyTnkyHpwK206ZrLOuuZ3Vm9pYvamNFRtbaVjbzPvh4w9vrGJNU1tu+6GVSQ4eNZgjxg7jo/uN4Ih9hlGejPfwDSLSXykUtjF6aAWfP35flvzxQUgGfQpmwElfg7ceg1+eByd/AyacDVV9NOFPH4vHjJqqFDVVKSaM6nqbprY0b63YxOsfbOT15RtYsGwjt/7pHX48exFliRhT6oYz7dBRTP3IngwbpJFlRXYX5r57TRxTX1/vc+fO3f4Prl0Mi5+C4fsFf+mnqrvddFNrB7/87hf5f34Pda2/ZO7XT2dEVQoa34H7L4flfwk2LKuG6j2hcgRUDIPK4cFz1UgYsT/UjIdh+0C8m8teB5hNrR28tGQtzyxq5Mk3VrKksZlEzDjxwFqmH70PHzugllhMp5lESsHM5rl7fW/bRael8MHL8Mg/bFne81A48V/gwDNgm/Ph1eVJ9h8Wp60xgZN3+eeI/eDzfwz29e4c2LgMNq2AlrWwoQFWvBq87mjesrNYAobvG4TRiPAxbBwMqoXKGhhUM2BCo7o8yckHjeTkg0by9TMn8NqyDcye+zLPvDqfb7/5Mjasjr/72P5cWD+GVEKnl0T6o6KGgplNBX4IxIGfu/uN3Wx3AXAvcJS770AzoAAHfBy+sgDe+SM0rYKX74K7L4JRh8PE6TC8DsbUB3/pA6Mqoa0x+GW9VWSYwV4Tg0d3WtZD4yJY8zasWQiNC6FxMSyeDekuBtRLDYayQZCsgOQgKKvc8jpZDvEUJMJHvCx8TkGirMD3yoP9JCq2PMcSW34ebEswFtJh7A7tm4MAbFkXPJrD100rYe272NrFHLJmIYe0beBLACnY3FLJw49M4TNPns9ZJx7HZ6fsoyuaRPqZooWCmcWBW4DTgAbgJTOb5e6vb7NdNfBl4IVi1QIEv2jLKuHIS4Ll4/4BXv41PH8bPHZNsK5qJFzyW6g9kNoKaKMsrHE7v6tiaBAwY7ZpqWWzsGk5rFsCm9dA8xrY3LilddHeHD5vDl5vXhMsp9sh05b33Eb+dKHFkRcUudfhsmfBM91/bujeQevosE9C7UFQtQe0baJyydN8+tX7uaDjGf7t0YuY+ty5fOvsQzhh/MDsmxHZHRWzpTAZWOTuiwHM7G7gHOD1bbb7V+A/gKuLWMuHxRNwxMUw6W9h3bvBL+r7L4cH/x4+ex81K59muYehwC46Dx6LwZAxwWNnuEM2HYRDum1LUGTat35OtwavO1rC5ZbgTu10SxBQeDjDnG/Zb4/rCNdbEHwVYR9K56NyeLAu0XXHsk2ajp36bcpmfZnrF/4PD7Su5tKZn2H60XV87cwJumJJpB8oZiiMBpbmLTcAU/I3MLNJwN7u/oiZ9W0obCkiPOe/L5z5n3DvpfC9/YgDMQvvVO5vfaNmQT9EPAmpqlJXs32q94TP/Aae+DrnPfdjxu1dxnnPf5J5763jZ5fUM3poRakrFIm0Yp7Q7epXae6ch5nFgB8A/9TrjsxmmNlcM5u7evXqXVjiNj5yLhxzZW5xtDWG31+8r4wkMzj9O3DcP3DE6od4auJTLF27mfN/8ixvr9xU6upEIq2YodAA7J23PAbIn4igGjgEeMrMlgBHA7PM7EOXTLn77e5e7+71tbVFPv/88X+DLzy91SplQhGYwSnfgqMuZ9ybP+MPk/9C1p0Lbn2WVxrWl7o6kcgqZii8BBxgZnVmVgZ8GpjV+aa7b3D3Gncf5+7jgOeBs4t29dH2qBm/1aKGcCgSMzjje3DYpxj50nd5/KNvMbgiycUzX+TNFRtLXZ1IJBUtFNw9DVwJPA68Adzj7q+Z2Q1mdnaxvneXSGw9NER2N7vBb7cSi8E5t8CBZzLsT//Cw0cvpDwRZ/rPX+T9xubePy8iu1R07mjeXsv/yuINzq/fKedrZ05Qa6HYOlrhN9Nh0e9Zd/jfc/LLJ1E7pJIHvngsVano3GMpUiyF3tGsUJD+I5OGx/8FXvwp60Ydz+nvTWfiQfvz0+lHangMkZ1UaCjodlLpP+IJmPYf8Dc/YtiqF3mq+uusf+NP3DJ7UakrE4kMhYL0P0deAp9/kspBg7k79R1aZ3+PZxetKnVVIpGgUJD+ac9DsRlP4RPO4ZrEb/BfXcjqlctKXZXIgKdQkP6rfDCJC+9g5fH/zlHZV7Gffoz0kmdLXZXIgKZQkP7NjJGnXMGcj93F5nSM2J1nwTM/zBuLSUR2JYWC7BZOPeV0Zn7kf/i/zJHw+2/CI18JrlZEm1s4AAALaElEQVQSkV1KF4DLbuO686bwieVf54MNd3DZvDvhvWfh4HPggNNh9JEQ0yirIjtLLQXZbZQn4/z04npuS07nn+NX05wYAn/+T/jv0+DH9bDw96UuUWS3p1CQ3co+Iwbx68un8Ec7hkkN/8TtU35P48dvwS0Ov7oA7v4srH+/1GWK7LZ0R7PsllZvauO6B17hD28E9y+krIPPJ/6PL8YeIGEZFtiBvJU6lA1VdXQMriOzxyHUDK1i5OBy9hxczvCqMiqScSqScVKJmO6YlgFPw1xIJCxZs5k/L1zN6k1ttKWzVLZ8QP3Kexm7cR6jWxcSIwvAZk8xLzueZV7DWqpp8koAjCwGJGOQiMdIxiERM5LxGKlYlkHWTlksQzwWpy05hLbUcDpSw+goHwHlQ0mUpUiWlZMoqyCZKqcsfJSnUlQk45QnY6QSccoSMcriMVLJ4DkR36aR7g6N78CI/YLl9qZgzu14mSb0kF2i0FBQR7Ps1sbVDGJczaC8NROAk4OXHS3BNKur36Ti3ac55r3noWkB8dZ1xLybK5cy4aMDshjtVkaaBDHPUklLwXVl3GgnSTsJ2kkARhroCGfnSBOjnTLarYwy0ozLm2pks1UyyJvZGBuCAeXeTNrKyFiCuGfYmBpJZXoDzWUjSGWbaU8OwSxGNl6Gx5IQS5BJDiJGlnRqGGYxPFlJzDNky4cSJ0M2NQRLVuDxMiirglQ1lFURiyewRApLpoglU1iiAkuWEyurJJ5IEYsb8ZgRMyMGweuYacDIAUShIANXsgL2mAB7TCD2kXO3dKC5B4FhBhYDLPxrPPzFFv6Ci1mM8vxfduk2aF4LzWtg8xoyzevpaG8NHm2tZDpaSbe3kkm3kW1vI9vRSjbdhqfbyTpks1ky7mSyDpk0lmklnmmjJZvm/Y5qxra+xfr4cN5P7se49rdZa8NYHxvCiMwa3rT92DO7glU+jMHNm2jx0dCaBcqoshY6SDCIjWSIkaKDSlppIUmtrSdNnCqCbaqsdYcPZ9aNVspoIQgygGoLgnJJdiSDrZlWykiSYRODwGAjVWSJ0WyVZCxOkgxNVkWSDlqtAjPIWJK0JUmQpT1WDgZxnI5YiqwlMDMysSQZK8MshpkTAxKeJtW+Fks3syg5ngMTq/igo5KxrOC9TA1jYo0stHEc1PE68/xAJvMac+0QDhjUzIZELW3JavbwNSwqO5jRvpI1g/anKruRWLKCuGUhmyGdGkq1tdKeGk4qs5mO1FASZIjhWCyozWKxcHrcciweBmYsAbE4FosTj8WIxWLBLLphoJpB3IJAjZkRM4hZELiW9zpmwXwucQu2G1yRYGhl13Og7yo6fSTSn7gXfLrI3WnPZGlPh49MlkzWSWecdDYIn3Q2XBeuz6TbSHsM2jaSbW+FTBuxjmasfROxjs2Q6YBMB5ZpI5ZpwzJtWLqNWKaFWKaNeLqFeKaNLJBMN5FMN5PMbAZ3NieHUdneyMbkSKrbV9IRS5HMtJDMtmDulGWbabcUqWwwT0ZZtpU0CRLeQZm3kiFBnAzmwS/dBAPnPpSsBycqg0csfATLjpEJ1/lW2xhZj231uffGXcDJf/edHapBp49EdkfbcRrGzEgl4qQSO3J/xsgd+Ewf6fxDNdMBmfagNZftCG9WDN/LpqFtE3Q0Q2owtKyFIWOD58F7BVegDRkDK1+D4fvBileh9kBYuxiG7RN+tgWSlbDqdajaA9+4HBtUi7c1QaIM8yzplk10xFJ4y3qyZdXQso5sLIlbHM904J7F3SHdjqfbcPdgXTYD2QzuWchmw3VZ8Ezwnjt4FvdMuL5zuy3vkc2AZ4l5EBFks+y/7wFFP/xFbSmY2VTgh0Ac+Lm737jN+/8IXA6kgdXA37n7ez3tUy0FEZHtV/L5FMwsDtwCnAEcDFxkZgdvs9lfgXp3Pwy4D/iPYtUjIiK9K+bNa5OBRe6+2N3bgbuBc/I3cPfZ7t45Ee/zwJgi1iMiIr0oZiiMBpbmLTeE67pzGfBYEesREZFeFLOjuasesy47MMxsOlAPnNDN+zOAGQBjx47dVfWJiMg2itlSaAD2zlseA3l36ITM7FTga8DZ7t7W1Y7c/XZ3r3f3+tra2qIUKyIixQ2Fl4ADzKzOzMqATwOz8jcws0nATwkCQZPwioiUWNFCwd3TwJXA48AbwD3u/pqZ3WBmZ4ebfQ+oAu41s/lmNqub3YmISB8o6s1r7v4o8Og2676Z9/rUYn6/iIhsn91umAszWw30eINbD2qANbuwnN2djsfWdDy2puOxtd39eOzj7r12yu52obAzzGxuIXf0RYWOx9Z0PLam47G1qBwPzbwmIiI5CgUREcmJWijcXuoC+hkdj63peGxNx2NrkTgekepTEBGRnkWtpSAiIj2ITCiY2VQze8vMFpnZtaWupy+Y2UwzW2VmC/LWDTez35vZwvB5WLjezOxH4fF5xcyOKF3lu56Z7W1ms83sDTN7zcyuCtdH9XiUm9mLZvZyeDyuD9fXmdkL4fH4TTgaAWaWCpcXhe+PK2X9xWJmcTP7q5k9Ei5H7nhEIhQKnNthILoTmLrNumuBJ939AODJcBmCY3NA+JgB3NpHNfaVNPBP7j4BOBq4Ivw3ENXj0Qac7O6HAxOBqWZ2NPBd4Afh8VhHMHox4fM6d98f+EG43UB0FcEIDJ2idzyC6eMG9gM4Bng8b/k64LpS19VHP/s4YEHe8lvAqPD1KOCt8PVPgYu62m4gPoCHgdN0PBygEvgLMIXg5qxEuD73/w3BcDXHhK8T4XZW6tp38XEYQ/CHwcnAIwQjPUfueESipcD2z+0wkI109w8Awuc9wvWROUZhU38S8AIRPh7hqZL5wCrg98A7wHoPxi2DrX/m3PEI398AjOjbiovuZuCfgWy4PIIIHo+ohELBcztEWCSOkZlVAfcDX3H3jT1t2sW6AXU83D3j7hMJ/kKeDEzoarPweUAfDzM7C1jl7vPyV3ex6YA/HlEJhYLmdoiIlWY2CiB87hyyfMAfIzNLEgTCr9z9gXB1ZI9HJ3dfDzxF0Ncy1Mw6B8rM/5lzxyN8fwiwtm8rLapjgbPNbAnB1MEnE7QcInc8ohIKvc7tECGzgEvC15cQnFvvXH9xeNXN0cCGztMqA4GZGfDfwBvuflPeW1E9HrVmNjR8XQGcStDBOhu4INxs2+PReZwuAP7o4Qn1gcDdr3P3Me4+juD3wx/d/bNE8XiUulOjrx7ANOBtgvOmXyt1PX30M98FfAB0EPxlcxnBec8ngYXh8/BwWyO4Qusd4FWgvtT17+JjcRxB8/4VYH74mBbh43EY8NfweCwAvhmu3xd4EVgE3AukwvXl4fKi8P19S/0zFPHYnAg8EtXjoTuaRUQkJyqnj0REpAAKBRERyVEoiIhIjkJBRERyFAoiIpKT6H0TkWgxs87LVAH2BDLA6nC52d0/WpLCRPqALkkV6YGZfRtocvfvl7oWkb6g00ci28HMmsLnE83sT2Z2j5m9bWY3mtlnwzkKXjWz/cLtas3sfjN7KXwcG64/wczmh4+/mll1KX8ukU46fSSy4w4nGERuLbAY+Lm7Tw4n8PkS8BXghwTj8T9tZmMJhlyeAFwNXOHuz4SD9LWW5CcQ2YZCQWTHveTheEhm9g7wRLj+VeCk8PWpwMHB0EsADA5bBc8AN5nZr4AH3L2h78oW6Z5CQWTHteW9zuYtZ9ny/1aMYDKWlm0+e6OZ/Y5g/KXnzexUd3+zqNWKFEB9CiLF9QRwZeeCmU0Mn/dz91fd/bvAXOCgEtUnshWFgkhxfRmoN7NXzOx14Avh+q+Y2QIzexloAR4rWYUieXRJqoiI5KilICIiOQoFERHJUSiIiEiOQkFERHIUCiIikqNQEBGRHIWCiIjkKBRERCTn/wM5loAJVZnHJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Times')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.plot(loss_x[:450],loss_train[:450],label = 'train_loss')\n",
    "plt.plot(loss_x[:450],loss_tes[:450],label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.savefig('Analysis_loss_new.tif', dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05070995, 0.04952454, 0.04884437, ..., 0.02296055, 0.02446511,\n",
       "       0.02585766])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average = np.loadtxt(open(\"average.csv\"),delimiter=\",\",skiprows=0)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.051810264587402, 12.11715030670166, 11.931812286376953, 11.88532829284668, 11.660590171813965, 11.672466278076172, 11.72838020324707, 11.684107780456543, 11.554776191711426, 11.575398445129395, 11.620412826538086, 11.638528823852539, 11.961516380310059, 11.848822593688965, 11.635305404663086, 11.538260459899902, 11.546085357666016, 11.377203941345215, 11.5841703414917, 11.511459350585938, 11.721763610839844, 11.699141502380371, 11.76040267944336, 11.831377029418945, 11.964370727539062, 11.968463897705078, 12.13801097869873, 12.063545227050781, 11.769883155822754, 11.771486282348633, 11.781393051147461, 11.782004356384277, 11.78712272644043, 11.809717178344727, 11.813232421875, 11.81299114227295, 11.812744140625, 11.812564849853516, 11.811281204223633, 11.811124801635742]\n"
     ]
    }
   ],
   "source": [
    "my_matrix = np.loadtxt(open(\"results.csv\"),delimiter=\",\",skiprows=0)\n",
    "#print(my_matrix)\n",
    " #对于矩阵而言，将矩阵倒数第一列之前的数值给了X（输入数据），将矩阵大最后一列的数值给了y（标签）\n",
    "X, y = my_matrix[:,:-1],my_matrix[:,-1]\n",
    "torch_y = torch.from_numpy(y)\n",
    "y_input = torch.tensor(torch_y , dtype=torch.float32)\n",
    "X_loss = []\n",
    "axis = []\n",
    "for n in range(40):\n",
    "    #X_minloss = 0\n",
    "    #print('start'+ str(X_minloss))\n",
    "    axis.append(n)\n",
    "    for i in range(30):\n",
    "        for j in range(len(X)):\n",
    "            X[j][n*30+i] = average[n*30+i]\n",
    "    \n",
    "            \n",
    "    torch_X = torch.from_numpy(X)\n",
    "    X_input = torch.tensor(torch_X , dtype=torch.float32)\n",
    "    X_input = X_input.reshape(X_input.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "    loss_func = My_loss()\n",
    "    \n",
    "    X_out = rnn(X_input)\n",
    "    X_minloss = loss_func(X_out,y_input)\n",
    "    X_minloss = X_minloss.data.item()\n",
    "    \n",
    "    #print(X_minloss)\n",
    "    \n",
    "    '''\n",
    "    for m in range(len(X)):\n",
    "        minloss = out[m].data.item() - X_out[m].data.item()\n",
    "        minloss = minloss\n",
    "        X_minloss += minloss\n",
    "    print('time: '+str(n)+'loss: '+str(X_minloss))\n",
    "    '''\n",
    "    X_loss.append(X_minloss)\n",
    "    \n",
    "print(X_loss)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = preprocessing.scale(X_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.66448832e+00  2.05200216e+00  9.52813260e-01  6.77129459e-01\n",
      " -6.55730622e-01 -5.85296697e-01 -2.53686561e-01 -5.16254143e-01\n",
      " -1.28328428e+00 -1.16097936e+00 -8.94011424e-01 -7.86570419e-01\n",
      "  1.12898008e+00  4.60624182e-01 -8.05687633e-01 -1.38123455e+00\n",
      " -1.33482723e+00 -2.33641654e+00 -1.10895565e+00 -1.54018457e+00\n",
      " -2.92927754e-01 -4.27093267e-01 -6.37700488e-02  3.57159331e-01\n",
      "  1.14590843e+00  1.17018390e+00  2.17572108e+00  1.73408516e+00\n",
      " -7.54394677e-03  1.96375642e-03  6.07180817e-02  6.43435652e-02\n",
      "  9.46992124e-02  2.28700702e-01  2.49548646e-01  2.48117683e-01\n",
      "  2.46652784e-01  2.45589460e-01  2.37976510e-01  2.37048929e-01]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAEpJJREFUeJzt3X+w5XVdx/Hnix/+SEDU3cT44YJaRumgbZTBpBkiqUWmFqkNGEY1+StzEqVptLLI0rSx1E1IRtGyFpLwFytSpviDXX4ugkjCTqtMrFmG5RCw7/74flYO6917z92953zO3ft8zJy53/M9P76v+e7e87qf76+TqkKSpH16B5AkzQYLQZIEWAiSpMZCkCQBFoIkqbEQJEmAhSBJaiwESRJgIUiSmv16B1iMVatW1Zo1a3rHkKRlZdOmTV+rqtULPW9ZFcKaNWvYuHFj7xiStKwk2TLO89xkJEkCLARJUmMhSJIAC0GS1FgIkiTAQpAkNRaCJAmwECRJzbI6MU3S0ltz5ofmnH/r2c+cchL15ghBkgRYCJKkxkKQJAEdCyHJ4UkuS3JDkuuTvLxXFklS353KdwO/VVVXJjkQ2JRkQ1V9oWMmSVqxuhVCVd0G3Nam70hyA3Ao0KUQPNJC0ko3E/sQkqwBngB8rm8SSVq5uhdCkgOA9cArquq/53j8jCQbk2zctm3b9ANK0grRtRCS7M9QBudX1QVzPaeq1lXV2qpau3r1gt8AJ0naTT2PMgpwDnBDVb25Vw5J0qDnCOE44JeApya5ut2e0TGPJK1oPY8y+hSQXsuXJN1X953KkqTZYCFIkgALQZLUWAiSJMBCkCQ1FoIkCbAQJEmN36ksaV5eCXjlcIQgSQIsBElSs2I2GTnslaT5OUKQJAEWgiSpsRAkSYCFIElqLARJEmAhSJIaC0GSBFgIkqTGQpAkARaCJKmxECRJgIUgSWosBEkSYCFIkhoLQZIEWAiSpMZCkCQBFoIkqbEQJEmAhSBJavbrHUDSZK0580Nzzr/17GdOOYlmnYUg7QX80NdSsBA0EX5AScuP+xAkSUDnQkhybpLbk2zumUOS1H+E8G7gpM4ZJEl0LoSq+iTw9Z4ZJEmD3iMESdKMmPlCSHJGko1JNm7btq13HEnaa818IVTVuqpaW1VrV69e3TuOJO21PA9hDLs6ph48rl7S3qP3YafvBz4DfF+SrUlO75lHklayriOEqvrFnsuXJN1r5vchSJKmw0KQJAEWgiSpsRAkSYCFIElqLARJEmAhSJIaC0GSBFgIkqTGaxktAb8/WNLewBGCJAmwECRJjYUgSQIsBElSYyFIkgALQZLUWAiSJMBCkCQ1Y52YlmQ9cC7wkaraPtlIWgk8mU+aPeOOEN4OPB/4UpKzkzx2gpkkSR2MVQhV9fGqegHwROBWYEOSy5O8KMn+kwwoSZqOsfchJHkYcBrwYuAq4K0MBbFhIskkSVM17j6EC4DHAu8BfrqqbmsP/W2SjZMKJ0mannGvdvquqvrw6Iwk96+qO6tq7QRySRrhTnhNw7ibjP5gjnmfWcogkqS+5h0hJDkEOBR4YJInAGkPHQR814SzSZKmaKFNRk9n2JF8GPDmkfl3AK+dUCZJUgfzFkJVnQecl+Q5VbV+SpkkSR0stMnohVX1XmBNklfu/HhVvXmOl0nSTFtoJ/18j+/qsYUeX4r3nrSFNhk9qP08YNJBJGkpeWTW4i20yeidbfIvq2rbFPJIkjoZ97DTy5NckuT0JA+ZaCJJUhfjXsvoMcDvAD8AbEpycZIXTjSZJGmqxr6WUVV9vqpeCRwLfB04b2KpJElTN+61jA4Cng2cAjwKuJChGCSpC3caL71xRwjXAMcAv1dV31tVr66qTXu68CQnJflikpuTnLmn7ydJ2n3jXtzuqKqqpVxwkn2BvwCeBmwFrkhyUVV9YSmXI2l27c4x+Y4AJmehE9PeUlWvAC5K8h2FUFU/swfLPha4uaq+3Jb1N8DJgIUgSR0sNEJ4T/v5pxNY9qHAv43c3wr8yASWI6kT/8pfXjLOlqAkL6+qty40b1ELTp4HPL2qXtzu/xJwbFW9dKfnnQGcAXDEEUf80JYtW3Z3kd3M2mnw47x2T997T/TMPWv/VqOvl3ZXkk3jfHfNuDuVT51j3mmLSvSdtgKHj9w/DPjqzk+qqnVVtbaq1q5evXoPFylJ2pWF9iH8IvB84MgkF408dCDwH3u47CuAxyQ5EvgKwyGtz9/D95SWJUcBmgUL7UO4HLgNWAW8aWT+HcC1e7Lgqro7yUuAjwH7AudW1fV78p6SpN230MXttgBbgCdNYuHte5o/vOATJUkTt9Amo09V1fFJ7gBG9z4HqKo6aKLpJElTs9AI4fj288DpxJEk9TLWUUZJHpXk/m36KUleluTgyUaTJE3TuIedrgfuSfJo4BzgSOB9E0slSZq6cQthe1XdzXDF07dU1W8Cj5hcLEnStI1bCHe1cxJOBS5u8/afTCRJUg/jXu30RcCvAW+oqlvayWTvnVws7eAJS5KmZaxCaJekftnI/VuAsycVSlqOLG8td+N+Y9pxwOuAR7bX7DgP4ajJRZMkTdO4m4zOAX4T2ATcM7k4kqRexi2Eb1TVRyaaRJLU1biFcFmSPwEuAO7cMbOqrpxIKknS1I1bCDu+yWz0CxYKeOrSxpEk9TLuUUY/Mekg0t7Oo5A068a9ltHDk5yT5CPt/tFJTp9sNEnSNI17pvK7Gb7I5nva/ZuAV0wikCSpj3ELYVVVfQDYDsO3neHhp5K0Vxl3p/L/JHkY7Utykvwo8I2JpdLY3C4taamMWwivBC4CHpXk08Bq4LkTSyVJmrp5Nxkl+eEkh7TzDZ4MvJbhPIRLgK1TyCdJmpKFRgjvBE5o0z8GnAW8FDgGWIejhD3mJh9Js2KhQti3qr7epn8BWFdV64H1Sa6ebDRJ0jQtdJTRvkl2lMZPAp8YeWzc/Q+SpGVgoQ/19wP/nORrwLeAfwFo363sUUZjcrOQpOVg3kKoqjckuZTh+5MvqapqD+3DsC9BkrSXWHCzT1V9do55N00mjiSpl3HPVJYk7eUsBEkSYCFIkhoPHdVu8cgpae9jIWjmWDZSH24ykiQBFoIkqbEQJEmA+xA0D7flSytLlxFCkucluT7J9iRre2SQJN1Xr01Gm4GfAz7ZafmSpJ102WRUVTcAJOmxeEnSHGZ+p3KSM5JsTLJx27ZtveNI0l5rYiOEJB8HDpnjobOq6oPjvk9VrWP4uk7Wrl1bCzxdkrSbJlYIVXXCws+SJM2Kmd9kJEmajl6HnT47yVbgScCHknysRw5J0r16HWV0IXBhj2VL8/FkPK1kbjKSJAFeumJF869hSaMcIUiSAAtBktRYCJIkwEKQJDUWgiQJ8CijvZpHEUlaDEcIkiTAEYL2Qo6MpN3jCEGSBDhC0DLkCECaDEcIkiTAQpAkNRaCJAmwECRJjYUgSQIsBElS42Gn0pg83FV7O0cIkiTAQpAkNRaCJAmwECRJjYUgSQIsBElSYyFIkgALQZLUWAiSJMBCkCQ1FoIkCbAQJEmNhSBJAiwESVJjIUiSAAtBktR0KYQkf5LkxiTXJrkwycE9ckiS7tVrhLAB+MGqejxwE/CaTjkkSU2XQqiqS6rq7nb3s8BhPXJIku41C/sQfhn4yK4eTHJGko1JNm7btm2KsSRpZdlvUm+c5OPAIXM8dFZVfbA95yzgbuD8Xb1PVa0D1gGsXbu2JhBVksQEC6GqTpjv8SSnAs8CfrKq/KCXpM4mVgjzSXIS8GrgyVX1vz0ySJLuq9c+hLcBBwIbklyd5B2dckiSmi4jhKp6dI/lSpJ2bRaOMpIkzQALQZIEWAiSpMZCkCQBFoIkqbEQJEmAhSBJaiwESRJgIUiSmi5nKku93Hr2M3tHkGaWIwRJEmAhSJIaC0GSBFgIkqTGQpAkARaCJKmxECRJgIUgSWosBEkSAKmq3hnGlmQbsGUJ3moV8LUleJ+lNqu5YHazmWtxZjUXzG62vSHXI6tq9UJPWlaFsFSSbKyqtb1z7GxWc8HsZjPX4sxqLpjdbCspl5uMJEmAhSBJalZqIazrHWAXZjUXzG42cy3OrOaC2c22YnKtyH0IkqTvtFJHCJKknay4QkhyUpIvJrk5yZm98+yQ5NYk1yW5OsnGzlnOTXJ7ks0j8x6aZEOSL7WfD5mRXK9L8pW23q5O8owOuQ5PclmSG5Jcn+TlbX7XdTZPrq7rLMkDknw+yTUt1+vb/COTfK6tr79Ncr8ZyfXuJLeMrK9jpplrJN++Sa5KcnG7v/Trq6pWzA3YF/hX4CjgfsA1wNG9c7VstwKreudoWX4ceCKweWTeG4Ez2/SZwB/PSK7XAa/qvL4eATyxTR8I3AQc3XudzZOr6zoDAhzQpvcHPgf8KPAB4JQ2/x3Ar89IrncDz+35f6xleiXwPuDidn/J19dKGyEcC9xcVV+uqv8D/gY4uXOmmVNVnwS+vtPsk4Hz2vR5wM9ONRS7zNVdVd1WVVe26TuAG4BD6bzO5snVVQ2+2e7u324FPBX4+za/x/raVa7ukhwGPBN4V7sfJrC+VlohHAr828j9rczAL0hTwCVJNiU5o3eYOTy8qm6D4YMG+O7OeUa9JMm1bZPS1DdljUqyBngCw1+XM7POdsoFnddZ2/xxNXA7sIFh5P5fVXV3e0qX382dc1XVjvX1hra+/izJ/aedC3gL8NvA9nb/YUxgfa20Qsgc82biLwDguKp6IvBTwG8k+fHegZaJtwOPAo4BbgPe1CtIkgOA9cArquq/e+XY2Ry5uq+zqrqnqo4BDmMYuX//XE+bbqrvzJXkB4HXAI8Ffhh4KPDqaWZK8izg9qraNDp7jqfu8fpaaYWwFTh85P5hwFc7ZbmPqvpq+3k7cCHDL8ks+fckjwBoP2/vnAeAqvr39ku8HfgrOq23JPszfOieX1UXtNnd19lcuWZlnbUs/wX8E8O2+oOT7Nce6vq7OZLrpLbprarqTuCvmf76Og74mSS3MmzmfirDiGHJ19dKK4QrgMe0vfP3A04BLuqciSQPSnLgjmngRGDz/K+auouAU9v0qcAHO2b5th0fuM2z6bDe2vbcc4AbqurNIw91XWe7ytV7nSVZneTgNv1A4ASG/RuXAc9tT+uxvubKdeNIqYdhO/1U11dVvaaqDquqNQyfWZ+oqhcwifXVe8/5tG/AMxiOtvhX4KzeeVqmoxiOeLoGuL53LuD9DJsS7mIYVZ3OsM3yUuBL7edDZyTXe4DrgGsZPoAf0SHX8QzD9WuBq9vtGb3X2Ty5uq4z4PHAVW35m4HfbfOPAj4P3Az8HXD/Gcn1iba+NgPvpR2J1OMGPIV7jzJa8vXlmcqSJGDlbTKSJO2ChSBJAiwESVJjIUiSAAtBktRYCFoWktzTrjS5Ock/7jhevLckz2pXoLwmyReS/GrvTDtL8treGbQ8eNiploUk36yqA9r0ecBNVfWGzpn2B7YAx1bV1naNmzVV9cWeuXY2uu6k+ThC0HL0GdqFvJIckOTSJFdm+D6Jk9v8NUluTPKuNqo4P8kJST7drh9/bHvesUkub3/lX57k+9r805JckOSj7flvnCPHgcB+wH8AVNWdO8qgnfW6PskV7XbcyPwNLe87k2xJsmoReR/ULkh3Rct88nx5k5wNPLCNrs6f2L+I9g69zrjz5m0xN+Cb7ee+DGdlntTu7wcc1KZXMZy1GWANcDfwOIY/fDYB57bHTgb+ob3mIGC/Nn0CsL5NnwZ8GXgw8ACGkcDhc+R6F8M1it4PvADYp81/H3B8mz6C4fIRAG8DXtOmT2I4k3jVIvL+IfDCNn0ww1n3D5ov7451583bQrcdF0aSZt0D22WJ1zB8WG5o8wP8Ybs67HaGkcPD22O3VNV1AEmuBy6tqkpyXXsfGD5Az0vyGIYP5/1HlnlpVX2jvf4LwCO57+XTqaoXJ3kcQ5m8Cngaw4fzCcDRw+VvADioXa/qeIbrB1FVH03ynyNvN07eExkudPaqdv8BDIUzVl5pPhaClotvVdUxSR4MXAz8BvDnDH+VrwZ+qKrualeEfEB7zZ0jr98+cn879/7f/33gsqp6dobvDPinkdeMvv4edvH70j7Er0vyHuAWhkLYB3hSVX1r9LkZaYg5jJM3wHNqp/0USX5k3LzSrrgPQctK+wv4ZcCr2k7dBzNcK/6uJD/B8FfxYjwY+EqbPm0xL2z7L54yMusYhk01AJcALxl57o7v4f0U8PNt3onAYr+c5mPAS3cUS5InjPGau9q6kuZlIWjZqaqrGK4MewpwPrA2yUaG0cKNi3y7NwJ/lOTTDPsnFiPAbyf5Ytuc9XruLZWXtVzXts03v9bmvx44McmVDF+GdBtwxyKW+fsMm7WuTbK53V/IuvZ8dyprXh52Kk1ROzT1nqq6O8mTgLfX8A1dUnduY5Sm6wjgA0n2Af4P+JXOeaRvc4QgSQLchyBJaiwESRJgIUiSGgtBkgRYCJKkxkKQJAHw/wELv8xsjafIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(results)\n",
    "axis =[]\n",
    "for n in range(40):\n",
    "    axis.append(n)\n",
    "#print(axis)\n",
    "\n",
    "plt.xlabel('Raman Segment')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.bar(axis, results)\n",
    "plt.savefig('Analysis_new.tif', dpi = 400)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
