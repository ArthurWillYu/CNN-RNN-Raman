{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " #首先，读取.CSV文件成矩阵的形式。\n",
    "my_matrix = np.loadtxt(open(\"results.csv\"),delimiter=\",\",skiprows=0)\n",
    "#print(my_matrix)\n",
    " #对于矩阵而言，将矩阵倒数第一列之前的数值给了X（输入数据），将矩阵大最后一列的数值给了y（标签）\n",
    "X, y = my_matrix[:,:-1],my_matrix[:,-1]\n",
    "\n",
    "#X = np.load('X_2019clinical.npy')\n",
    "#y = np.load('y_2019clinical.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yy = np.load('y_reference.npy')\n",
    "#len(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.303831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001]\n"
     ]
    }
   ],
   "source": [
    "#print(y_train)\n",
    "label = y \n",
    "#print(y_train)\n",
    "sx_1 = 1\n",
    "sx_2 =(sx_1*0.99*0.27*0.99)\n",
    "sx_8 = sx_1*0.99*0.27*0.25*0.99\n",
    "sx_7 = sx_1*0.99*0.27*0.25*1\n",
    "sx_6 = sx_1*0.99*0.31*0.99\n",
    "sx_3 = sx_1*0.99*0.31*0.34\n",
    "sx_4 = sx_1*0.99*0.31*0.51*0.47*0.88*0.78\n",
    "sx_5 = sx_1*0.99*0.31*0.51*0.47*0.88*0.78*0.78\n",
    "print(sx_6)\n",
    "y = [sx_2 if i == 1 else i for i in y]\n",
    "y = [sx_8 if i == 7 else i for i in y]\n",
    "y = [1 if i == 0 else i for i in y]\n",
    "y = [sx_3 if i == 2 else i for i in y]\n",
    "y = [sx_4 if i == 3 else i for i in y]\n",
    "y = [sx_5 if i == 4 else i for i in y]\n",
    "y = [sx_6 if i == 5 else i for i in y]\n",
    "y = [sx_7 if i == 6 else i for i in y]\n",
    "\n",
    "#y_train = np.array(y_train)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train =[]\n",
    "X_test =[]\n",
    "y_train =[]\n",
    "y_test =[]\n",
    "\n",
    "for n in range(len(y)):\n",
    "    if y[n] == sx_1:\n",
    "        X_test.append(X[n])\n",
    "        y_test.append(y[n])\n",
    "\n",
    "for n in range(len(y)):\n",
    "    if y[n] == sx_2 or y[n] == sx_4 or y[n] == sx_7:\n",
    "        X_test.append(X[n])\n",
    "        y_test.append(y[n])\n",
    "    else:\n",
    "        X_train.append(X[n])\n",
    "        y_train.append(y[n])\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " #利用train_test_split方法，将X,y随机划分问，训练集（X_train），训练集标签（X_test），测试卷（y_train），\n",
    " #测试集标签（y_test），安训练集：测试集=7:3的\n",
    " #概率划分，到此步骤，可以直接对数据进行处理\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    " #此步骤，是为了将训练集与数据集的数据分别保存为CSV文件\n",
    " #np.column_stack将两个矩阵进行组合连接\n",
    "#train= np.column_stack((X_train,y_train))\n",
    " #numpy.savetxt 将txt文件保存为.csv结尾的文件\n",
    "#np.savetxt('train_usual.csv',train, delimiter = ',')\n",
    "#test = np.column_stack((X_test, y_test))\n",
    "#np.savetxt('test_usual.csv', test, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train)\n",
    "label = y_train\n",
    "#print(label)\n",
    "y_train = np.array(y_train)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch_x = torch.from_numpy(X_train)\n",
    "torch_y = torch.from_numpy(y_train)\n",
    "test_x = torch.from_numpy(X_test)\n",
    "test_y = torch.from_numpy(y_test)\n",
    "y_train = torch.tensor(torch_y, dtype=torch.float32)\n",
    "y_test = torch.tensor(test_y,dtype = torch.float32)\n",
    "x_test = torch.tensor(test_x , dtype=torch.float32)\n",
    "x_train = torch.tensor(torch_x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646, 0.2646,\n",
      "        0.2646, 0.2646, 0.2646, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505,\n",
      "        0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0505, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668, 0.0668,\n",
      "        0.0668, 0.0668, 0.0668])\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "EPOCH = 500\n",
    "BATCH_SIZE = 100\n",
    "TIME_STEP = 20\n",
    "INPUT_SIZE = 60\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "x_train = x_train.reshape(x_train.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "#print(x_train.shape,x_test.shape)\n",
    "torch_dataset = Data.TensorDataset(x_train,y_train )\n",
    "train_loader = Data.DataLoader(dataset= torch_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=2)\n",
    "#print(x_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=64,         # rnn hidden unit\n",
    "            num_layers=3,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self , x, y):\n",
    "        time = 1\n",
    "        ave = torch.tensor(0)\n",
    "        for n in range(len(x)):\n",
    "            if y[n] == 1:\n",
    "                ave = torch.add(x[n], ave)\n",
    "                time += 1\n",
    "        #print(ave,time)\n",
    "        ave  = torch.div(ave,time) \n",
    "        div = torch.div(x,ave)\n",
    "        loss = torch.mean(torch.pow((div - y),2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(X_out),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(60, 64, num_layers=3, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn =  RNN()\n",
    "print(rnn)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = LR)\n",
    "loss_func = My_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:0 train:tensor(0.6209, grad_fn=<MeanBackward0>) test:tensor(0.5131, grad_fn=<MeanBackward0>)\n",
      "times:1 train:tensor(0.5704, grad_fn=<MeanBackward0>) test:tensor(2.7073, grad_fn=<MeanBackward0>)\n",
      "times:2 train:tensor(2.1388, grad_fn=<MeanBackward0>) test:tensor(0.4887, grad_fn=<MeanBackward0>)\n",
      "times:3 train:tensor(0.5430, grad_fn=<MeanBackward0>) test:tensor(0.5120, grad_fn=<MeanBackward0>)\n",
      "times:4 train:tensor(0.5692, grad_fn=<MeanBackward0>) test:tensor(0.5203, grad_fn=<MeanBackward0>)\n",
      "times:5 train:tensor(0.5785, grad_fn=<MeanBackward0>) test:tensor(0.5245, grad_fn=<MeanBackward0>)\n",
      "times:6 train:tensor(0.5831, grad_fn=<MeanBackward0>) test:tensor(0.5270, grad_fn=<MeanBackward0>)\n",
      "times:7 train:tensor(0.5859, grad_fn=<MeanBackward0>) test:tensor(0.5286, grad_fn=<MeanBackward0>)\n",
      "times:8 train:tensor(0.5877, grad_fn=<MeanBackward0>) test:tensor(0.5297, grad_fn=<MeanBackward0>)\n",
      "times:9 train:tensor(0.5889, grad_fn=<MeanBackward0>) test:tensor(0.5305, grad_fn=<MeanBackward0>)\n",
      "times:10 train:tensor(0.5898, grad_fn=<MeanBackward0>) test:tensor(0.5310, grad_fn=<MeanBackward0>)\n",
      "times:11 train:tensor(0.5904, grad_fn=<MeanBackward0>) test:tensor(0.5314, grad_fn=<MeanBackward0>)\n",
      "times:12 train:tensor(0.5909, grad_fn=<MeanBackward0>) test:tensor(0.5318, grad_fn=<MeanBackward0>)\n",
      "times:13 train:tensor(0.5912, grad_fn=<MeanBackward0>) test:tensor(0.5320, grad_fn=<MeanBackward0>)\n",
      "times:14 train:tensor(0.5915, grad_fn=<MeanBackward0>) test:tensor(0.5322, grad_fn=<MeanBackward0>)\n",
      "times:15 train:tensor(0.5917, grad_fn=<MeanBackward0>) test:tensor(0.5323, grad_fn=<MeanBackward0>)\n",
      "times:16 train:tensor(0.5918, grad_fn=<MeanBackward0>) test:tensor(0.5324, grad_fn=<MeanBackward0>)\n",
      "times:17 train:tensor(0.5919, grad_fn=<MeanBackward0>) test:tensor(0.5324, grad_fn=<MeanBackward0>)\n",
      "times:18 train:tensor(0.5920, grad_fn=<MeanBackward0>) test:tensor(0.5324, grad_fn=<MeanBackward0>)\n",
      "times:19 train:tensor(0.5920, grad_fn=<MeanBackward0>) test:tensor(0.5324, grad_fn=<MeanBackward0>)\n",
      "times:20 train:tensor(0.5920, grad_fn=<MeanBackward0>) test:tensor(0.5324, grad_fn=<MeanBackward0>)\n",
      "times:21 train:tensor(0.5920, grad_fn=<MeanBackward0>) test:tensor(0.5324, grad_fn=<MeanBackward0>)\n",
      "times:22 train:tensor(0.5920, grad_fn=<MeanBackward0>) test:tensor(0.5323, grad_fn=<MeanBackward0>)\n",
      "times:23 train:tensor(0.5919, grad_fn=<MeanBackward0>) test:tensor(0.5323, grad_fn=<MeanBackward0>)\n",
      "times:24 train:tensor(0.5919, grad_fn=<MeanBackward0>) test:tensor(0.5322, grad_fn=<MeanBackward0>)\n",
      "times:25 train:tensor(0.5918, grad_fn=<MeanBackward0>) test:tensor(0.5321, grad_fn=<MeanBackward0>)\n",
      "times:26 train:tensor(0.5917, grad_fn=<MeanBackward0>) test:tensor(0.5321, grad_fn=<MeanBackward0>)\n",
      "times:27 train:tensor(0.5916, grad_fn=<MeanBackward0>) test:tensor(0.5320, grad_fn=<MeanBackward0>)\n",
      "times:28 train:tensor(0.5916, grad_fn=<MeanBackward0>) test:tensor(0.5319, grad_fn=<MeanBackward0>)\n",
      "times:29 train:tensor(0.5915, grad_fn=<MeanBackward0>) test:tensor(0.5318, grad_fn=<MeanBackward0>)\n",
      "times:30 train:tensor(0.5913, grad_fn=<MeanBackward0>) test:tensor(0.5317, grad_fn=<MeanBackward0>)\n",
      "times:31 train:tensor(0.5912, grad_fn=<MeanBackward0>) test:tensor(0.5315, grad_fn=<MeanBackward0>)\n",
      "times:32 train:tensor(0.5911, grad_fn=<MeanBackward0>) test:tensor(0.5314, grad_fn=<MeanBackward0>)\n",
      "times:33 train:tensor(0.5910, grad_fn=<MeanBackward0>) test:tensor(0.5313, grad_fn=<MeanBackward0>)\n",
      "times:34 train:tensor(0.5909, grad_fn=<MeanBackward0>) test:tensor(0.5312, grad_fn=<MeanBackward0>)\n",
      "times:35 train:tensor(0.5907, grad_fn=<MeanBackward0>) test:tensor(0.5311, grad_fn=<MeanBackward0>)\n",
      "times:36 train:tensor(0.5906, grad_fn=<MeanBackward0>) test:tensor(0.5309, grad_fn=<MeanBackward0>)\n",
      "times:37 train:tensor(0.5904, grad_fn=<MeanBackward0>) test:tensor(0.5308, grad_fn=<MeanBackward0>)\n",
      "times:38 train:tensor(0.5903, grad_fn=<MeanBackward0>) test:tensor(0.5306, grad_fn=<MeanBackward0>)\n",
      "times:39 train:tensor(0.5901, grad_fn=<MeanBackward0>) test:tensor(0.5305, grad_fn=<MeanBackward0>)\n",
      "times:40 train:tensor(0.5900, grad_fn=<MeanBackward0>) test:tensor(0.5303, grad_fn=<MeanBackward0>)\n",
      "times:41 train:tensor(0.5898, grad_fn=<MeanBackward0>) test:tensor(0.5302, grad_fn=<MeanBackward0>)\n",
      "times:42 train:tensor(0.5897, grad_fn=<MeanBackward0>) test:tensor(0.5300, grad_fn=<MeanBackward0>)\n",
      "times:43 train:tensor(0.5895, grad_fn=<MeanBackward0>) test:tensor(0.5299, grad_fn=<MeanBackward0>)\n",
      "times:44 train:tensor(0.5893, grad_fn=<MeanBackward0>) test:tensor(0.5297, grad_fn=<MeanBackward0>)\n",
      "times:45 train:tensor(0.5891, grad_fn=<MeanBackward0>) test:tensor(0.5295, grad_fn=<MeanBackward0>)\n",
      "times:46 train:tensor(0.5889, grad_fn=<MeanBackward0>) test:tensor(0.5293, grad_fn=<MeanBackward0>)\n",
      "times:47 train:tensor(0.5888, grad_fn=<MeanBackward0>) test:tensor(0.5292, grad_fn=<MeanBackward0>)\n",
      "times:48 train:tensor(0.5886, grad_fn=<MeanBackward0>) test:tensor(0.5290, grad_fn=<MeanBackward0>)\n",
      "times:49 train:tensor(0.5884, grad_fn=<MeanBackward0>) test:tensor(0.5288, grad_fn=<MeanBackward0>)\n",
      "times:50 train:tensor(0.5882, grad_fn=<MeanBackward0>) test:tensor(0.5286, grad_fn=<MeanBackward0>)\n",
      "times:51 train:tensor(0.5880, grad_fn=<MeanBackward0>) test:tensor(0.5284, grad_fn=<MeanBackward0>)\n",
      "times:52 train:tensor(0.5877, grad_fn=<MeanBackward0>) test:tensor(0.5282, grad_fn=<MeanBackward0>)\n",
      "times:53 train:tensor(0.5875, grad_fn=<MeanBackward0>) test:tensor(0.5280, grad_fn=<MeanBackward0>)\n",
      "times:54 train:tensor(0.5873, grad_fn=<MeanBackward0>) test:tensor(0.5278, grad_fn=<MeanBackward0>)\n",
      "times:55 train:tensor(0.5871, grad_fn=<MeanBackward0>) test:tensor(0.5276, grad_fn=<MeanBackward0>)\n",
      "times:56 train:tensor(0.5869, grad_fn=<MeanBackward0>) test:tensor(0.5274, grad_fn=<MeanBackward0>)\n",
      "times:57 train:tensor(0.5866, grad_fn=<MeanBackward0>) test:tensor(0.5271, grad_fn=<MeanBackward0>)\n",
      "times:58 train:tensor(0.5864, grad_fn=<MeanBackward0>) test:tensor(0.5269, grad_fn=<MeanBackward0>)\n",
      "times:59 train:tensor(0.5861, grad_fn=<MeanBackward0>) test:tensor(0.5267, grad_fn=<MeanBackward0>)\n",
      "times:60 train:tensor(0.5859, grad_fn=<MeanBackward0>) test:tensor(0.5264, grad_fn=<MeanBackward0>)\n",
      "times:61 train:tensor(0.5856, grad_fn=<MeanBackward0>) test:tensor(0.5262, grad_fn=<MeanBackward0>)\n",
      "times:62 train:tensor(0.5853, grad_fn=<MeanBackward0>) test:tensor(0.5259, grad_fn=<MeanBackward0>)\n",
      "times:63 train:tensor(0.5850, grad_fn=<MeanBackward0>) test:tensor(0.5257, grad_fn=<MeanBackward0>)\n",
      "times:64 train:tensor(0.5848, grad_fn=<MeanBackward0>) test:tensor(0.5254, grad_fn=<MeanBackward0>)\n",
      "times:65 train:tensor(0.5845, grad_fn=<MeanBackward0>) test:tensor(0.5251, grad_fn=<MeanBackward0>)\n",
      "times:66 train:tensor(0.5842, grad_fn=<MeanBackward0>) test:tensor(0.5248, grad_fn=<MeanBackward0>)\n",
      "times:67 train:tensor(0.5838, grad_fn=<MeanBackward0>) test:tensor(0.5245, grad_fn=<MeanBackward0>)\n",
      "times:68 train:tensor(0.5835, grad_fn=<MeanBackward0>) test:tensor(0.5242, grad_fn=<MeanBackward0>)\n",
      "times:69 train:tensor(0.5832, grad_fn=<MeanBackward0>) test:tensor(0.5239, grad_fn=<MeanBackward0>)\n",
      "times:70 train:tensor(0.5828, grad_fn=<MeanBackward0>) test:tensor(0.5236, grad_fn=<MeanBackward0>)\n",
      "times:71 train:tensor(0.5825, grad_fn=<MeanBackward0>) test:tensor(0.5232, grad_fn=<MeanBackward0>)\n",
      "times:72 train:tensor(0.5821, grad_fn=<MeanBackward0>) test:tensor(0.5229, grad_fn=<MeanBackward0>)\n",
      "times:73 train:tensor(0.5817, grad_fn=<MeanBackward0>) test:tensor(0.5225, grad_fn=<MeanBackward0>)\n",
      "times:74 train:tensor(0.5814, grad_fn=<MeanBackward0>) test:tensor(0.5222, grad_fn=<MeanBackward0>)\n",
      "times:75 train:tensor(0.5809, grad_fn=<MeanBackward0>) test:tensor(0.5218, grad_fn=<MeanBackward0>)\n",
      "times:76 train:tensor(0.5805, grad_fn=<MeanBackward0>) test:tensor(0.5214, grad_fn=<MeanBackward0>)\n",
      "times:77 train:tensor(0.5801, grad_fn=<MeanBackward0>) test:tensor(0.5210, grad_fn=<MeanBackward0>)\n",
      "times:78 train:tensor(0.5796, grad_fn=<MeanBackward0>) test:tensor(0.5205, grad_fn=<MeanBackward0>)\n",
      "times:79 train:tensor(0.5791, grad_fn=<MeanBackward0>) test:tensor(0.5201, grad_fn=<MeanBackward0>)\n",
      "times:80 train:tensor(0.5786, grad_fn=<MeanBackward0>) test:tensor(0.5196, grad_fn=<MeanBackward0>)\n",
      "times:81 train:tensor(0.5781, grad_fn=<MeanBackward0>) test:tensor(0.5191, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:82 train:tensor(0.5776, grad_fn=<MeanBackward0>) test:tensor(0.5186, grad_fn=<MeanBackward0>)\n",
      "times:83 train:tensor(0.5770, grad_fn=<MeanBackward0>) test:tensor(0.5181, grad_fn=<MeanBackward0>)\n",
      "times:84 train:tensor(0.5764, grad_fn=<MeanBackward0>) test:tensor(0.5175, grad_fn=<MeanBackward0>)\n",
      "times:85 train:tensor(0.5758, grad_fn=<MeanBackward0>) test:tensor(0.5170, grad_fn=<MeanBackward0>)\n",
      "times:86 train:tensor(0.5752, grad_fn=<MeanBackward0>) test:tensor(0.5163, grad_fn=<MeanBackward0>)\n",
      "times:87 train:tensor(0.5745, grad_fn=<MeanBackward0>) test:tensor(0.5157, grad_fn=<MeanBackward0>)\n",
      "times:88 train:tensor(0.5737, grad_fn=<MeanBackward0>) test:tensor(0.5150, grad_fn=<MeanBackward0>)\n",
      "times:89 train:tensor(0.5730, grad_fn=<MeanBackward0>) test:tensor(0.5143, grad_fn=<MeanBackward0>)\n",
      "times:90 train:tensor(0.5722, grad_fn=<MeanBackward0>) test:tensor(0.5136, grad_fn=<MeanBackward0>)\n",
      "times:91 train:tensor(0.5713, grad_fn=<MeanBackward0>) test:tensor(0.5128, grad_fn=<MeanBackward0>)\n",
      "times:92 train:tensor(0.5704, grad_fn=<MeanBackward0>) test:tensor(0.5119, grad_fn=<MeanBackward0>)\n",
      "times:93 train:tensor(0.5695, grad_fn=<MeanBackward0>) test:tensor(0.5110, grad_fn=<MeanBackward0>)\n",
      "times:94 train:tensor(0.5685, grad_fn=<MeanBackward0>) test:tensor(0.5101, grad_fn=<MeanBackward0>)\n",
      "times:95 train:tensor(0.5674, grad_fn=<MeanBackward0>) test:tensor(0.5091, grad_fn=<MeanBackward0>)\n",
      "times:96 train:tensor(0.5663, grad_fn=<MeanBackward0>) test:tensor(0.5080, grad_fn=<MeanBackward0>)\n",
      "times:97 train:tensor(0.5651, grad_fn=<MeanBackward0>) test:tensor(0.5069, grad_fn=<MeanBackward0>)\n",
      "times:98 train:tensor(0.5638, grad_fn=<MeanBackward0>) test:tensor(0.5057, grad_fn=<MeanBackward0>)\n",
      "times:99 train:tensor(0.5624, grad_fn=<MeanBackward0>) test:tensor(0.5045, grad_fn=<MeanBackward0>)\n",
      "times:100 train:tensor(0.5609, grad_fn=<MeanBackward0>) test:tensor(0.5031, grad_fn=<MeanBackward0>)\n",
      "times:101 train:tensor(0.5594, grad_fn=<MeanBackward0>) test:tensor(0.5017, grad_fn=<MeanBackward0>)\n",
      "times:102 train:tensor(0.5577, grad_fn=<MeanBackward0>) test:tensor(0.5001, grad_fn=<MeanBackward0>)\n",
      "times:103 train:tensor(0.5560, grad_fn=<MeanBackward0>) test:tensor(0.4985, grad_fn=<MeanBackward0>)\n",
      "times:104 train:tensor(0.5541, grad_fn=<MeanBackward0>) test:tensor(0.4968, grad_fn=<MeanBackward0>)\n",
      "times:105 train:tensor(0.5521, grad_fn=<MeanBackward0>) test:tensor(0.4950, grad_fn=<MeanBackward0>)\n",
      "times:106 train:tensor(0.5500, grad_fn=<MeanBackward0>) test:tensor(0.4931, grad_fn=<MeanBackward0>)\n",
      "times:107 train:tensor(0.5478, grad_fn=<MeanBackward0>) test:tensor(0.4910, grad_fn=<MeanBackward0>)\n",
      "times:108 train:tensor(0.5455, grad_fn=<MeanBackward0>) test:tensor(0.4889, grad_fn=<MeanBackward0>)\n",
      "times:109 train:tensor(0.5430, grad_fn=<MeanBackward0>) test:tensor(0.4867, grad_fn=<MeanBackward0>)\n",
      "times:110 train:tensor(0.5405, grad_fn=<MeanBackward0>) test:tensor(0.4844, grad_fn=<MeanBackward0>)\n",
      "times:111 train:tensor(0.5378, grad_fn=<MeanBackward0>) test:tensor(0.4820, grad_fn=<MeanBackward0>)\n",
      "times:112 train:tensor(0.5350, grad_fn=<MeanBackward0>) test:tensor(0.4795, grad_fn=<MeanBackward0>)\n",
      "times:113 train:tensor(0.5322, grad_fn=<MeanBackward0>) test:tensor(0.4770, grad_fn=<MeanBackward0>)\n",
      "times:114 train:tensor(0.5293, grad_fn=<MeanBackward0>) test:tensor(0.4744, grad_fn=<MeanBackward0>)\n",
      "times:115 train:tensor(0.5264, grad_fn=<MeanBackward0>) test:tensor(0.4718, grad_fn=<MeanBackward0>)\n",
      "times:116 train:tensor(0.5234, grad_fn=<MeanBackward0>) test:tensor(0.4692, grad_fn=<MeanBackward0>)\n",
      "times:117 train:tensor(0.5204, grad_fn=<MeanBackward0>) test:tensor(0.4666, grad_fn=<MeanBackward0>)\n",
      "times:118 train:tensor(0.5174, grad_fn=<MeanBackward0>) test:tensor(0.4639, grad_fn=<MeanBackward0>)\n",
      "times:119 train:tensor(0.5144, grad_fn=<MeanBackward0>) test:tensor(0.4612, grad_fn=<MeanBackward0>)\n",
      "times:120 train:tensor(0.5113, grad_fn=<MeanBackward0>) test:tensor(0.4585, grad_fn=<MeanBackward0>)\n",
      "times:121 train:tensor(0.5082, grad_fn=<MeanBackward0>) test:tensor(0.4558, grad_fn=<MeanBackward0>)\n",
      "times:122 train:tensor(0.5050, grad_fn=<MeanBackward0>) test:tensor(0.4531, grad_fn=<MeanBackward0>)\n",
      "times:123 train:tensor(0.5019, grad_fn=<MeanBackward0>) test:tensor(0.4503, grad_fn=<MeanBackward0>)\n",
      "times:124 train:tensor(0.4986, grad_fn=<MeanBackward0>) test:tensor(0.4475, grad_fn=<MeanBackward0>)\n",
      "times:125 train:tensor(0.4953, grad_fn=<MeanBackward0>) test:tensor(0.4447, grad_fn=<MeanBackward0>)\n",
      "times:126 train:tensor(0.4919, grad_fn=<MeanBackward0>) test:tensor(0.4419, grad_fn=<MeanBackward0>)\n",
      "times:127 train:tensor(0.4885, grad_fn=<MeanBackward0>) test:tensor(0.4391, grad_fn=<MeanBackward0>)\n",
      "times:128 train:tensor(0.4850, grad_fn=<MeanBackward0>) test:tensor(0.4362, grad_fn=<MeanBackward0>)\n",
      "times:129 train:tensor(0.4814, grad_fn=<MeanBackward0>) test:tensor(0.4333, grad_fn=<MeanBackward0>)\n",
      "times:130 train:tensor(0.4778, grad_fn=<MeanBackward0>) test:tensor(0.4305, grad_fn=<MeanBackward0>)\n",
      "times:131 train:tensor(0.4742, grad_fn=<MeanBackward0>) test:tensor(0.4276, grad_fn=<MeanBackward0>)\n",
      "times:132 train:tensor(0.4706, grad_fn=<MeanBackward0>) test:tensor(0.4248, grad_fn=<MeanBackward0>)\n",
      "times:133 train:tensor(0.4670, grad_fn=<MeanBackward0>) test:tensor(0.4220, grad_fn=<MeanBackward0>)\n",
      "times:134 train:tensor(0.4634, grad_fn=<MeanBackward0>) test:tensor(0.4192, grad_fn=<MeanBackward0>)\n",
      "times:135 train:tensor(0.4598, grad_fn=<MeanBackward0>) test:tensor(0.4164, grad_fn=<MeanBackward0>)\n",
      "times:136 train:tensor(0.4562, grad_fn=<MeanBackward0>) test:tensor(0.4136, grad_fn=<MeanBackward0>)\n",
      "times:137 train:tensor(0.4526, grad_fn=<MeanBackward0>) test:tensor(0.4108, grad_fn=<MeanBackward0>)\n",
      "times:138 train:tensor(0.4490, grad_fn=<MeanBackward0>) test:tensor(0.4080, grad_fn=<MeanBackward0>)\n",
      "times:139 train:tensor(0.4453, grad_fn=<MeanBackward0>) test:tensor(0.4051, grad_fn=<MeanBackward0>)\n",
      "times:140 train:tensor(0.4415, grad_fn=<MeanBackward0>) test:tensor(0.4022, grad_fn=<MeanBackward0>)\n",
      "times:141 train:tensor(0.4377, grad_fn=<MeanBackward0>) test:tensor(0.3992, grad_fn=<MeanBackward0>)\n",
      "times:142 train:tensor(0.4338, grad_fn=<MeanBackward0>) test:tensor(0.3962, grad_fn=<MeanBackward0>)\n",
      "times:143 train:tensor(0.4299, grad_fn=<MeanBackward0>) test:tensor(0.3932, grad_fn=<MeanBackward0>)\n",
      "times:144 train:tensor(0.4258, grad_fn=<MeanBackward0>) test:tensor(0.3902, grad_fn=<MeanBackward0>)\n",
      "times:145 train:tensor(0.4218, grad_fn=<MeanBackward0>) test:tensor(0.3871, grad_fn=<MeanBackward0>)\n",
      "times:146 train:tensor(0.4177, grad_fn=<MeanBackward0>) test:tensor(0.3841, grad_fn=<MeanBackward0>)\n",
      "times:147 train:tensor(0.4135, grad_fn=<MeanBackward0>) test:tensor(0.3810, grad_fn=<MeanBackward0>)\n",
      "times:148 train:tensor(0.4093, grad_fn=<MeanBackward0>) test:tensor(0.3780, grad_fn=<MeanBackward0>)\n",
      "times:149 train:tensor(0.4051, grad_fn=<MeanBackward0>) test:tensor(0.3750, grad_fn=<MeanBackward0>)\n",
      "times:150 train:tensor(0.4009, grad_fn=<MeanBackward0>) test:tensor(0.3720, grad_fn=<MeanBackward0>)\n",
      "times:151 train:tensor(0.3967, grad_fn=<MeanBackward0>) test:tensor(0.3691, grad_fn=<MeanBackward0>)\n",
      "times:152 train:tensor(0.3925, grad_fn=<MeanBackward0>) test:tensor(0.3662, grad_fn=<MeanBackward0>)\n",
      "times:153 train:tensor(0.3882, grad_fn=<MeanBackward0>) test:tensor(0.3634, grad_fn=<MeanBackward0>)\n",
      "times:154 train:tensor(0.3841, grad_fn=<MeanBackward0>) test:tensor(0.3606, grad_fn=<MeanBackward0>)\n",
      "times:155 train:tensor(0.3799, grad_fn=<MeanBackward0>) test:tensor(0.3579, grad_fn=<MeanBackward0>)\n",
      "times:156 train:tensor(0.3758, grad_fn=<MeanBackward0>) test:tensor(0.3553, grad_fn=<MeanBackward0>)\n",
      "times:157 train:tensor(0.3717, grad_fn=<MeanBackward0>) test:tensor(0.3528, grad_fn=<MeanBackward0>)\n",
      "times:158 train:tensor(0.3678, grad_fn=<MeanBackward0>) test:tensor(0.3503, grad_fn=<MeanBackward0>)\n",
      "times:159 train:tensor(0.3639, grad_fn=<MeanBackward0>) test:tensor(0.3479, grad_fn=<MeanBackward0>)\n",
      "times:160 train:tensor(0.3601, grad_fn=<MeanBackward0>) test:tensor(0.3457, grad_fn=<MeanBackward0>)\n",
      "times:161 train:tensor(0.3565, grad_fn=<MeanBackward0>) test:tensor(0.3435, grad_fn=<MeanBackward0>)\n",
      "times:162 train:tensor(0.3529, grad_fn=<MeanBackward0>) test:tensor(0.3414, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:163 train:tensor(0.3494, grad_fn=<MeanBackward0>) test:tensor(0.3395, grad_fn=<MeanBackward0>)\n",
      "times:164 train:tensor(0.3461, grad_fn=<MeanBackward0>) test:tensor(0.3376, grad_fn=<MeanBackward0>)\n",
      "times:165 train:tensor(0.3429, grad_fn=<MeanBackward0>) test:tensor(0.3359, grad_fn=<MeanBackward0>)\n",
      "times:166 train:tensor(0.3398, grad_fn=<MeanBackward0>) test:tensor(0.3342, grad_fn=<MeanBackward0>)\n",
      "times:167 train:tensor(0.3368, grad_fn=<MeanBackward0>) test:tensor(0.3326, grad_fn=<MeanBackward0>)\n",
      "times:168 train:tensor(0.3339, grad_fn=<MeanBackward0>) test:tensor(0.3311, grad_fn=<MeanBackward0>)\n",
      "times:169 train:tensor(0.3311, grad_fn=<MeanBackward0>) test:tensor(0.3298, grad_fn=<MeanBackward0>)\n",
      "times:170 train:tensor(0.3285, grad_fn=<MeanBackward0>) test:tensor(0.3285, grad_fn=<MeanBackward0>)\n",
      "times:171 train:tensor(0.3259, grad_fn=<MeanBackward0>) test:tensor(0.3272, grad_fn=<MeanBackward0>)\n",
      "times:172 train:tensor(0.3235, grad_fn=<MeanBackward0>) test:tensor(0.3261, grad_fn=<MeanBackward0>)\n",
      "times:173 train:tensor(0.3212, grad_fn=<MeanBackward0>) test:tensor(0.3251, grad_fn=<MeanBackward0>)\n",
      "times:174 train:tensor(0.3190, grad_fn=<MeanBackward0>) test:tensor(0.3241, grad_fn=<MeanBackward0>)\n",
      "times:175 train:tensor(0.3169, grad_fn=<MeanBackward0>) test:tensor(0.3232, grad_fn=<MeanBackward0>)\n",
      "times:176 train:tensor(0.3149, grad_fn=<MeanBackward0>) test:tensor(0.3224, grad_fn=<MeanBackward0>)\n",
      "times:177 train:tensor(0.3130, grad_fn=<MeanBackward0>) test:tensor(0.3217, grad_fn=<MeanBackward0>)\n",
      "times:178 train:tensor(0.3112, grad_fn=<MeanBackward0>) test:tensor(0.3210, grad_fn=<MeanBackward0>)\n",
      "times:179 train:tensor(0.3095, grad_fn=<MeanBackward0>) test:tensor(0.3205, grad_fn=<MeanBackward0>)\n",
      "times:180 train:tensor(0.3079, grad_fn=<MeanBackward0>) test:tensor(0.3200, grad_fn=<MeanBackward0>)\n",
      "times:181 train:tensor(0.3064, grad_fn=<MeanBackward0>) test:tensor(0.3196, grad_fn=<MeanBackward0>)\n",
      "times:182 train:tensor(0.3051, grad_fn=<MeanBackward0>) test:tensor(0.3192, grad_fn=<MeanBackward0>)\n",
      "times:183 train:tensor(0.3038, grad_fn=<MeanBackward0>) test:tensor(0.3190, grad_fn=<MeanBackward0>)\n",
      "times:184 train:tensor(0.3027, grad_fn=<MeanBackward0>) test:tensor(0.3188, grad_fn=<MeanBackward0>)\n",
      "times:185 train:tensor(0.3016, grad_fn=<MeanBackward0>) test:tensor(0.3186, grad_fn=<MeanBackward0>)\n",
      "times:186 train:tensor(0.3007, grad_fn=<MeanBackward0>) test:tensor(0.3185, grad_fn=<MeanBackward0>)\n",
      "times:187 train:tensor(0.2998, grad_fn=<MeanBackward0>) test:tensor(0.3185, grad_fn=<MeanBackward0>)\n",
      "times:188 train:tensor(0.2991, grad_fn=<MeanBackward0>) test:tensor(0.3185, grad_fn=<MeanBackward0>)\n",
      "times:189 train:tensor(0.2984, grad_fn=<MeanBackward0>) test:tensor(0.3185, grad_fn=<MeanBackward0>)\n",
      "times:190 train:tensor(0.2978, grad_fn=<MeanBackward0>) test:tensor(0.3186, grad_fn=<MeanBackward0>)\n",
      "times:191 train:tensor(0.2972, grad_fn=<MeanBackward0>) test:tensor(0.3188, grad_fn=<MeanBackward0>)\n",
      "times:192 train:tensor(0.2967, grad_fn=<MeanBackward0>) test:tensor(0.3189, grad_fn=<MeanBackward0>)\n",
      "times:193 train:tensor(0.2963, grad_fn=<MeanBackward0>) test:tensor(0.3191, grad_fn=<MeanBackward0>)\n",
      "times:194 train:tensor(0.2959, grad_fn=<MeanBackward0>) test:tensor(0.3192, grad_fn=<MeanBackward0>)\n",
      "times:195 train:tensor(0.2956, grad_fn=<MeanBackward0>) test:tensor(0.3194, grad_fn=<MeanBackward0>)\n",
      "times:196 train:tensor(0.2953, grad_fn=<MeanBackward0>) test:tensor(0.3196, grad_fn=<MeanBackward0>)\n",
      "times:197 train:tensor(0.2950, grad_fn=<MeanBackward0>) test:tensor(0.3198, grad_fn=<MeanBackward0>)\n",
      "times:198 train:tensor(0.2947, grad_fn=<MeanBackward0>) test:tensor(0.3200, grad_fn=<MeanBackward0>)\n",
      "times:199 train:tensor(0.2944, grad_fn=<MeanBackward0>) test:tensor(0.3202, grad_fn=<MeanBackward0>)\n",
      "times:200 train:tensor(0.2942, grad_fn=<MeanBackward0>) test:tensor(0.3204, grad_fn=<MeanBackward0>)\n",
      "times:201 train:tensor(0.2939, grad_fn=<MeanBackward0>) test:tensor(0.3205, grad_fn=<MeanBackward0>)\n",
      "times:202 train:tensor(0.2937, grad_fn=<MeanBackward0>) test:tensor(0.3207, grad_fn=<MeanBackward0>)\n",
      "times:203 train:tensor(0.2935, grad_fn=<MeanBackward0>) test:tensor(0.3208, grad_fn=<MeanBackward0>)\n",
      "times:204 train:tensor(0.2932, grad_fn=<MeanBackward0>) test:tensor(0.3209, grad_fn=<MeanBackward0>)\n",
      "times:205 train:tensor(0.2930, grad_fn=<MeanBackward0>) test:tensor(0.3210, grad_fn=<MeanBackward0>)\n",
      "times:206 train:tensor(0.2927, grad_fn=<MeanBackward0>) test:tensor(0.3210, grad_fn=<MeanBackward0>)\n",
      "times:207 train:tensor(0.2925, grad_fn=<MeanBackward0>) test:tensor(0.3211, grad_fn=<MeanBackward0>)\n",
      "times:208 train:tensor(0.2923, grad_fn=<MeanBackward0>) test:tensor(0.3211, grad_fn=<MeanBackward0>)\n",
      "times:209 train:tensor(0.2920, grad_fn=<MeanBackward0>) test:tensor(0.3211, grad_fn=<MeanBackward0>)\n",
      "times:210 train:tensor(0.2918, grad_fn=<MeanBackward0>) test:tensor(0.3211, grad_fn=<MeanBackward0>)\n",
      "times:211 train:tensor(0.2915, grad_fn=<MeanBackward0>) test:tensor(0.3210, grad_fn=<MeanBackward0>)\n",
      "times:212 train:tensor(0.2913, grad_fn=<MeanBackward0>) test:tensor(0.3210, grad_fn=<MeanBackward0>)\n",
      "times:213 train:tensor(0.2910, grad_fn=<MeanBackward0>) test:tensor(0.3209, grad_fn=<MeanBackward0>)\n",
      "times:214 train:tensor(0.2908, grad_fn=<MeanBackward0>) test:tensor(0.3209, grad_fn=<MeanBackward0>)\n",
      "times:215 train:tensor(0.2905, grad_fn=<MeanBackward0>) test:tensor(0.3208, grad_fn=<MeanBackward0>)\n",
      "times:216 train:tensor(0.2903, grad_fn=<MeanBackward0>) test:tensor(0.3208, grad_fn=<MeanBackward0>)\n",
      "times:217 train:tensor(0.2901, grad_fn=<MeanBackward0>) test:tensor(0.3207, grad_fn=<MeanBackward0>)\n",
      "times:218 train:tensor(0.2898, grad_fn=<MeanBackward0>) test:tensor(0.3206, grad_fn=<MeanBackward0>)\n",
      "times:219 train:tensor(0.2896, grad_fn=<MeanBackward0>) test:tensor(0.3206, grad_fn=<MeanBackward0>)\n",
      "times:220 train:tensor(0.2894, grad_fn=<MeanBackward0>) test:tensor(0.3205, grad_fn=<MeanBackward0>)\n",
      "times:221 train:tensor(0.2892, grad_fn=<MeanBackward0>) test:tensor(0.3204, grad_fn=<MeanBackward0>)\n",
      "times:222 train:tensor(0.2890, grad_fn=<MeanBackward0>) test:tensor(0.3204, grad_fn=<MeanBackward0>)\n",
      "times:223 train:tensor(0.2888, grad_fn=<MeanBackward0>) test:tensor(0.3203, grad_fn=<MeanBackward0>)\n",
      "times:224 train:tensor(0.2886, grad_fn=<MeanBackward0>) test:tensor(0.3202, grad_fn=<MeanBackward0>)\n",
      "times:225 train:tensor(0.2884, grad_fn=<MeanBackward0>) test:tensor(0.3202, grad_fn=<MeanBackward0>)\n",
      "times:226 train:tensor(0.2882, grad_fn=<MeanBackward0>) test:tensor(0.3201, grad_fn=<MeanBackward0>)\n",
      "times:227 train:tensor(0.2881, grad_fn=<MeanBackward0>) test:tensor(0.3200, grad_fn=<MeanBackward0>)\n",
      "times:228 train:tensor(0.2879, grad_fn=<MeanBackward0>) test:tensor(0.3200, grad_fn=<MeanBackward0>)\n",
      "times:229 train:tensor(0.2877, grad_fn=<MeanBackward0>) test:tensor(0.3199, grad_fn=<MeanBackward0>)\n",
      "times:230 train:tensor(0.2876, grad_fn=<MeanBackward0>) test:tensor(0.3199, grad_fn=<MeanBackward0>)\n",
      "times:231 train:tensor(0.2874, grad_fn=<MeanBackward0>) test:tensor(0.3198, grad_fn=<MeanBackward0>)\n",
      "times:232 train:tensor(0.2873, grad_fn=<MeanBackward0>) test:tensor(0.3198, grad_fn=<MeanBackward0>)\n",
      "times:233 train:tensor(0.2871, grad_fn=<MeanBackward0>) test:tensor(0.3197, grad_fn=<MeanBackward0>)\n",
      "times:234 train:tensor(0.2870, grad_fn=<MeanBackward0>) test:tensor(0.3197, grad_fn=<MeanBackward0>)\n",
      "times:235 train:tensor(0.2868, grad_fn=<MeanBackward0>) test:tensor(0.3196, grad_fn=<MeanBackward0>)\n",
      "times:236 train:tensor(0.2867, grad_fn=<MeanBackward0>) test:tensor(0.3196, grad_fn=<MeanBackward0>)\n",
      "times:237 train:tensor(0.2866, grad_fn=<MeanBackward0>) test:tensor(0.3195, grad_fn=<MeanBackward0>)\n",
      "times:238 train:tensor(0.2864, grad_fn=<MeanBackward0>) test:tensor(0.3195, grad_fn=<MeanBackward0>)\n",
      "times:239 train:tensor(0.2863, grad_fn=<MeanBackward0>) test:tensor(0.3194, grad_fn=<MeanBackward0>)\n",
      "times:240 train:tensor(0.2861, grad_fn=<MeanBackward0>) test:tensor(0.3194, grad_fn=<MeanBackward0>)\n",
      "times:241 train:tensor(0.2860, grad_fn=<MeanBackward0>) test:tensor(0.3194, grad_fn=<MeanBackward0>)\n",
      "times:242 train:tensor(0.2859, grad_fn=<MeanBackward0>) test:tensor(0.3193, grad_fn=<MeanBackward0>)\n",
      "times:243 train:tensor(0.2857, grad_fn=<MeanBackward0>) test:tensor(0.3193, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:244 train:tensor(0.2856, grad_fn=<MeanBackward0>) test:tensor(0.3192, grad_fn=<MeanBackward0>)\n",
      "times:245 train:tensor(0.2855, grad_fn=<MeanBackward0>) test:tensor(0.3192, grad_fn=<MeanBackward0>)\n",
      "times:246 train:tensor(0.2854, grad_fn=<MeanBackward0>) test:tensor(0.3191, grad_fn=<MeanBackward0>)\n",
      "times:247 train:tensor(0.2852, grad_fn=<MeanBackward0>) test:tensor(0.3191, grad_fn=<MeanBackward0>)\n",
      "times:248 train:tensor(0.2851, grad_fn=<MeanBackward0>) test:tensor(0.3190, grad_fn=<MeanBackward0>)\n",
      "times:249 train:tensor(0.2850, grad_fn=<MeanBackward0>) test:tensor(0.3190, grad_fn=<MeanBackward0>)\n",
      "times:250 train:tensor(0.2848, grad_fn=<MeanBackward0>) test:tensor(0.3189, grad_fn=<MeanBackward0>)\n",
      "times:251 train:tensor(0.2847, grad_fn=<MeanBackward0>) test:tensor(0.3189, grad_fn=<MeanBackward0>)\n",
      "times:252 train:tensor(0.2846, grad_fn=<MeanBackward0>) test:tensor(0.3188, grad_fn=<MeanBackward0>)\n",
      "times:253 train:tensor(0.2844, grad_fn=<MeanBackward0>) test:tensor(0.3188, grad_fn=<MeanBackward0>)\n",
      "times:254 train:tensor(0.2843, grad_fn=<MeanBackward0>) test:tensor(0.3187, grad_fn=<MeanBackward0>)\n",
      "times:255 train:tensor(0.2842, grad_fn=<MeanBackward0>) test:tensor(0.3186, grad_fn=<MeanBackward0>)\n",
      "times:256 train:tensor(0.2840, grad_fn=<MeanBackward0>) test:tensor(0.3186, grad_fn=<MeanBackward0>)\n",
      "times:257 train:tensor(0.2839, grad_fn=<MeanBackward0>) test:tensor(0.3185, grad_fn=<MeanBackward0>)\n",
      "times:258 train:tensor(0.2838, grad_fn=<MeanBackward0>) test:tensor(0.3184, grad_fn=<MeanBackward0>)\n",
      "times:259 train:tensor(0.2836, grad_fn=<MeanBackward0>) test:tensor(0.3184, grad_fn=<MeanBackward0>)\n",
      "times:260 train:tensor(0.2835, grad_fn=<MeanBackward0>) test:tensor(0.3183, grad_fn=<MeanBackward0>)\n",
      "times:261 train:tensor(0.2834, grad_fn=<MeanBackward0>) test:tensor(0.3182, grad_fn=<MeanBackward0>)\n",
      "times:262 train:tensor(0.2832, grad_fn=<MeanBackward0>) test:tensor(0.3181, grad_fn=<MeanBackward0>)\n",
      "times:263 train:tensor(0.2831, grad_fn=<MeanBackward0>) test:tensor(0.3181, grad_fn=<MeanBackward0>)\n",
      "times:264 train:tensor(0.2830, grad_fn=<MeanBackward0>) test:tensor(0.3180, grad_fn=<MeanBackward0>)\n",
      "times:265 train:tensor(0.2828, grad_fn=<MeanBackward0>) test:tensor(0.3179, grad_fn=<MeanBackward0>)\n",
      "times:266 train:tensor(0.2827, grad_fn=<MeanBackward0>) test:tensor(0.3178, grad_fn=<MeanBackward0>)\n",
      "times:267 train:tensor(0.2826, grad_fn=<MeanBackward0>) test:tensor(0.3177, grad_fn=<MeanBackward0>)\n",
      "times:268 train:tensor(0.2824, grad_fn=<MeanBackward0>) test:tensor(0.3176, grad_fn=<MeanBackward0>)\n",
      "times:269 train:tensor(0.2823, grad_fn=<MeanBackward0>) test:tensor(0.3176, grad_fn=<MeanBackward0>)\n",
      "times:270 train:tensor(0.2822, grad_fn=<MeanBackward0>) test:tensor(0.3175, grad_fn=<MeanBackward0>)\n",
      "times:271 train:tensor(0.2820, grad_fn=<MeanBackward0>) test:tensor(0.3174, grad_fn=<MeanBackward0>)\n",
      "times:272 train:tensor(0.2819, grad_fn=<MeanBackward0>) test:tensor(0.3173, grad_fn=<MeanBackward0>)\n",
      "times:273 train:tensor(0.2818, grad_fn=<MeanBackward0>) test:tensor(0.3172, grad_fn=<MeanBackward0>)\n",
      "times:274 train:tensor(0.2816, grad_fn=<MeanBackward0>) test:tensor(0.3171, grad_fn=<MeanBackward0>)\n",
      "times:275 train:tensor(0.2815, grad_fn=<MeanBackward0>) test:tensor(0.3170, grad_fn=<MeanBackward0>)\n",
      "times:276 train:tensor(0.2814, grad_fn=<MeanBackward0>) test:tensor(0.3169, grad_fn=<MeanBackward0>)\n",
      "times:277 train:tensor(0.2812, grad_fn=<MeanBackward0>) test:tensor(0.3168, grad_fn=<MeanBackward0>)\n",
      "times:278 train:tensor(0.2811, grad_fn=<MeanBackward0>) test:tensor(0.3167, grad_fn=<MeanBackward0>)\n",
      "times:279 train:tensor(0.2810, grad_fn=<MeanBackward0>) test:tensor(0.3166, grad_fn=<MeanBackward0>)\n",
      "times:280 train:tensor(0.2808, grad_fn=<MeanBackward0>) test:tensor(0.3166, grad_fn=<MeanBackward0>)\n",
      "times:281 train:tensor(0.2807, grad_fn=<MeanBackward0>) test:tensor(0.3165, grad_fn=<MeanBackward0>)\n",
      "times:282 train:tensor(0.2806, grad_fn=<MeanBackward0>) test:tensor(0.3164, grad_fn=<MeanBackward0>)\n",
      "times:283 train:tensor(0.2804, grad_fn=<MeanBackward0>) test:tensor(0.3163, grad_fn=<MeanBackward0>)\n",
      "times:284 train:tensor(0.2803, grad_fn=<MeanBackward0>) test:tensor(0.3162, grad_fn=<MeanBackward0>)\n",
      "times:285 train:tensor(0.2802, grad_fn=<MeanBackward0>) test:tensor(0.3161, grad_fn=<MeanBackward0>)\n",
      "times:286 train:tensor(0.2800, grad_fn=<MeanBackward0>) test:tensor(0.3160, grad_fn=<MeanBackward0>)\n",
      "times:287 train:tensor(0.2799, grad_fn=<MeanBackward0>) test:tensor(0.3160, grad_fn=<MeanBackward0>)\n",
      "times:288 train:tensor(0.2798, grad_fn=<MeanBackward0>) test:tensor(0.3159, grad_fn=<MeanBackward0>)\n",
      "times:289 train:tensor(0.2796, grad_fn=<MeanBackward0>) test:tensor(0.3158, grad_fn=<MeanBackward0>)\n",
      "times:290 train:tensor(0.2795, grad_fn=<MeanBackward0>) test:tensor(0.3157, grad_fn=<MeanBackward0>)\n",
      "times:291 train:tensor(0.2794, grad_fn=<MeanBackward0>) test:tensor(0.3156, grad_fn=<MeanBackward0>)\n",
      "times:292 train:tensor(0.2792, grad_fn=<MeanBackward0>) test:tensor(0.3155, grad_fn=<MeanBackward0>)\n",
      "times:293 train:tensor(0.2791, grad_fn=<MeanBackward0>) test:tensor(0.3155, grad_fn=<MeanBackward0>)\n",
      "times:294 train:tensor(0.2790, grad_fn=<MeanBackward0>) test:tensor(0.3154, grad_fn=<MeanBackward0>)\n",
      "times:295 train:tensor(0.2788, grad_fn=<MeanBackward0>) test:tensor(0.3153, grad_fn=<MeanBackward0>)\n",
      "times:296 train:tensor(0.2787, grad_fn=<MeanBackward0>) test:tensor(0.3152, grad_fn=<MeanBackward0>)\n",
      "times:297 train:tensor(0.2786, grad_fn=<MeanBackward0>) test:tensor(0.3152, grad_fn=<MeanBackward0>)\n",
      "times:298 train:tensor(0.2784, grad_fn=<MeanBackward0>) test:tensor(0.3151, grad_fn=<MeanBackward0>)\n",
      "times:299 train:tensor(0.2783, grad_fn=<MeanBackward0>) test:tensor(0.3150, grad_fn=<MeanBackward0>)\n",
      "times:300 train:tensor(0.2782, grad_fn=<MeanBackward0>) test:tensor(0.3149, grad_fn=<MeanBackward0>)\n",
      "times:301 train:tensor(0.2780, grad_fn=<MeanBackward0>) test:tensor(0.3148, grad_fn=<MeanBackward0>)\n",
      "times:302 train:tensor(0.2779, grad_fn=<MeanBackward0>) test:tensor(0.3148, grad_fn=<MeanBackward0>)\n",
      "times:303 train:tensor(0.2778, grad_fn=<MeanBackward0>) test:tensor(0.3147, grad_fn=<MeanBackward0>)\n",
      "times:304 train:tensor(0.2776, grad_fn=<MeanBackward0>) test:tensor(0.3146, grad_fn=<MeanBackward0>)\n",
      "times:305 train:tensor(0.2775, grad_fn=<MeanBackward0>) test:tensor(0.3145, grad_fn=<MeanBackward0>)\n",
      "times:306 train:tensor(0.2774, grad_fn=<MeanBackward0>) test:tensor(0.3145, grad_fn=<MeanBackward0>)\n",
      "times:307 train:tensor(0.2772, grad_fn=<MeanBackward0>) test:tensor(0.3144, grad_fn=<MeanBackward0>)\n",
      "times:308 train:tensor(0.2771, grad_fn=<MeanBackward0>) test:tensor(0.3143, grad_fn=<MeanBackward0>)\n",
      "times:309 train:tensor(0.2770, grad_fn=<MeanBackward0>) test:tensor(0.3142, grad_fn=<MeanBackward0>)\n",
      "times:310 train:tensor(0.2768, grad_fn=<MeanBackward0>) test:tensor(0.3142, grad_fn=<MeanBackward0>)\n",
      "times:311 train:tensor(0.2767, grad_fn=<MeanBackward0>) test:tensor(0.3141, grad_fn=<MeanBackward0>)\n",
      "times:312 train:tensor(0.2765, grad_fn=<MeanBackward0>) test:tensor(0.3140, grad_fn=<MeanBackward0>)\n",
      "times:313 train:tensor(0.2764, grad_fn=<MeanBackward0>) test:tensor(0.3139, grad_fn=<MeanBackward0>)\n",
      "times:314 train:tensor(0.2763, grad_fn=<MeanBackward0>) test:tensor(0.3139, grad_fn=<MeanBackward0>)\n",
      "times:315 train:tensor(0.2761, grad_fn=<MeanBackward0>) test:tensor(0.3138, grad_fn=<MeanBackward0>)\n",
      "times:316 train:tensor(0.2760, grad_fn=<MeanBackward0>) test:tensor(0.3137, grad_fn=<MeanBackward0>)\n",
      "times:317 train:tensor(0.2759, grad_fn=<MeanBackward0>) test:tensor(0.3136, grad_fn=<MeanBackward0>)\n",
      "times:318 train:tensor(0.2757, grad_fn=<MeanBackward0>) test:tensor(0.3136, grad_fn=<MeanBackward0>)\n",
      "times:319 train:tensor(0.2756, grad_fn=<MeanBackward0>) test:tensor(0.3135, grad_fn=<MeanBackward0>)\n",
      "times:320 train:tensor(0.2754, grad_fn=<MeanBackward0>) test:tensor(0.3134, grad_fn=<MeanBackward0>)\n",
      "times:321 train:tensor(0.2753, grad_fn=<MeanBackward0>) test:tensor(0.3133, grad_fn=<MeanBackward0>)\n",
      "times:322 train:tensor(0.2752, grad_fn=<MeanBackward0>) test:tensor(0.3133, grad_fn=<MeanBackward0>)\n",
      "times:323 train:tensor(0.2750, grad_fn=<MeanBackward0>) test:tensor(0.3132, grad_fn=<MeanBackward0>)\n",
      "times:324 train:tensor(0.2749, grad_fn=<MeanBackward0>) test:tensor(0.3131, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:325 train:tensor(0.2747, grad_fn=<MeanBackward0>) test:tensor(0.3130, grad_fn=<MeanBackward0>)\n",
      "times:326 train:tensor(0.2746, grad_fn=<MeanBackward0>) test:tensor(0.3129, grad_fn=<MeanBackward0>)\n",
      "times:327 train:tensor(0.2745, grad_fn=<MeanBackward0>) test:tensor(0.3129, grad_fn=<MeanBackward0>)\n",
      "times:328 train:tensor(0.2743, grad_fn=<MeanBackward0>) test:tensor(0.3128, grad_fn=<MeanBackward0>)\n",
      "times:329 train:tensor(0.2742, grad_fn=<MeanBackward0>) test:tensor(0.3127, grad_fn=<MeanBackward0>)\n",
      "times:330 train:tensor(0.2740, grad_fn=<MeanBackward0>) test:tensor(0.3126, grad_fn=<MeanBackward0>)\n",
      "times:331 train:tensor(0.2739, grad_fn=<MeanBackward0>) test:tensor(0.3126, grad_fn=<MeanBackward0>)\n",
      "times:332 train:tensor(0.2737, grad_fn=<MeanBackward0>) test:tensor(0.3125, grad_fn=<MeanBackward0>)\n",
      "times:333 train:tensor(0.2736, grad_fn=<MeanBackward0>) test:tensor(0.3124, grad_fn=<MeanBackward0>)\n",
      "times:334 train:tensor(0.2735, grad_fn=<MeanBackward0>) test:tensor(0.3123, grad_fn=<MeanBackward0>)\n",
      "times:335 train:tensor(0.2733, grad_fn=<MeanBackward0>) test:tensor(0.3122, grad_fn=<MeanBackward0>)\n",
      "times:336 train:tensor(0.2732, grad_fn=<MeanBackward0>) test:tensor(0.3122, grad_fn=<MeanBackward0>)\n",
      "times:337 train:tensor(0.2730, grad_fn=<MeanBackward0>) test:tensor(0.3121, grad_fn=<MeanBackward0>)\n",
      "times:338 train:tensor(0.2729, grad_fn=<MeanBackward0>) test:tensor(0.3120, grad_fn=<MeanBackward0>)\n",
      "times:339 train:tensor(0.2727, grad_fn=<MeanBackward0>) test:tensor(0.3119, grad_fn=<MeanBackward0>)\n",
      "times:340 train:tensor(0.2726, grad_fn=<MeanBackward0>) test:tensor(0.3118, grad_fn=<MeanBackward0>)\n",
      "times:341 train:tensor(0.2724, grad_fn=<MeanBackward0>) test:tensor(0.3118, grad_fn=<MeanBackward0>)\n",
      "times:342 train:tensor(0.2723, grad_fn=<MeanBackward0>) test:tensor(0.3117, grad_fn=<MeanBackward0>)\n",
      "times:343 train:tensor(0.2721, grad_fn=<MeanBackward0>) test:tensor(0.3116, grad_fn=<MeanBackward0>)\n",
      "times:344 train:tensor(0.2720, grad_fn=<MeanBackward0>) test:tensor(0.3115, grad_fn=<MeanBackward0>)\n",
      "times:345 train:tensor(0.2718, grad_fn=<MeanBackward0>) test:tensor(0.3115, grad_fn=<MeanBackward0>)\n",
      "times:346 train:tensor(0.2717, grad_fn=<MeanBackward0>) test:tensor(0.3114, grad_fn=<MeanBackward0>)\n",
      "times:347 train:tensor(0.2715, grad_fn=<MeanBackward0>) test:tensor(0.3113, grad_fn=<MeanBackward0>)\n",
      "times:348 train:tensor(0.2714, grad_fn=<MeanBackward0>) test:tensor(0.3112, grad_fn=<MeanBackward0>)\n",
      "times:349 train:tensor(0.2712, grad_fn=<MeanBackward0>) test:tensor(0.3111, grad_fn=<MeanBackward0>)\n",
      "times:350 train:tensor(0.2711, grad_fn=<MeanBackward0>) test:tensor(0.3110, grad_fn=<MeanBackward0>)\n",
      "times:351 train:tensor(0.2709, grad_fn=<MeanBackward0>) test:tensor(0.3110, grad_fn=<MeanBackward0>)\n",
      "times:352 train:tensor(0.2708, grad_fn=<MeanBackward0>) test:tensor(0.3109, grad_fn=<MeanBackward0>)\n",
      "times:353 train:tensor(0.2706, grad_fn=<MeanBackward0>) test:tensor(0.3108, grad_fn=<MeanBackward0>)\n",
      "times:354 train:tensor(0.2705, grad_fn=<MeanBackward0>) test:tensor(0.3107, grad_fn=<MeanBackward0>)\n",
      "times:355 train:tensor(0.2703, grad_fn=<MeanBackward0>) test:tensor(0.3106, grad_fn=<MeanBackward0>)\n",
      "times:356 train:tensor(0.2702, grad_fn=<MeanBackward0>) test:tensor(0.3105, grad_fn=<MeanBackward0>)\n",
      "times:357 train:tensor(0.2700, grad_fn=<MeanBackward0>) test:tensor(0.3104, grad_fn=<MeanBackward0>)\n",
      "times:358 train:tensor(0.2699, grad_fn=<MeanBackward0>) test:tensor(0.3104, grad_fn=<MeanBackward0>)\n",
      "times:359 train:tensor(0.2697, grad_fn=<MeanBackward0>) test:tensor(0.3103, grad_fn=<MeanBackward0>)\n",
      "times:360 train:tensor(0.2695, grad_fn=<MeanBackward0>) test:tensor(0.3102, grad_fn=<MeanBackward0>)\n",
      "times:361 train:tensor(0.2694, grad_fn=<MeanBackward0>) test:tensor(0.3101, grad_fn=<MeanBackward0>)\n",
      "times:362 train:tensor(0.2692, grad_fn=<MeanBackward0>) test:tensor(0.3100, grad_fn=<MeanBackward0>)\n",
      "times:363 train:tensor(0.2691, grad_fn=<MeanBackward0>) test:tensor(0.3099, grad_fn=<MeanBackward0>)\n",
      "times:364 train:tensor(0.2689, grad_fn=<MeanBackward0>) test:tensor(0.3098, grad_fn=<MeanBackward0>)\n",
      "times:365 train:tensor(0.2687, grad_fn=<MeanBackward0>) test:tensor(0.3097, grad_fn=<MeanBackward0>)\n",
      "times:366 train:tensor(0.2686, grad_fn=<MeanBackward0>) test:tensor(0.3096, grad_fn=<MeanBackward0>)\n",
      "times:367 train:tensor(0.2684, grad_fn=<MeanBackward0>) test:tensor(0.3095, grad_fn=<MeanBackward0>)\n",
      "times:368 train:tensor(0.2683, grad_fn=<MeanBackward0>) test:tensor(0.3094, grad_fn=<MeanBackward0>)\n",
      "times:369 train:tensor(0.2681, grad_fn=<MeanBackward0>) test:tensor(0.3093, grad_fn=<MeanBackward0>)\n",
      "times:370 train:tensor(0.2679, grad_fn=<MeanBackward0>) test:tensor(0.3092, grad_fn=<MeanBackward0>)\n",
      "times:371 train:tensor(0.2678, grad_fn=<MeanBackward0>) test:tensor(0.3091, grad_fn=<MeanBackward0>)\n",
      "times:372 train:tensor(0.2676, grad_fn=<MeanBackward0>) test:tensor(0.3090, grad_fn=<MeanBackward0>)\n",
      "times:373 train:tensor(0.2674, grad_fn=<MeanBackward0>) test:tensor(0.3089, grad_fn=<MeanBackward0>)\n",
      "times:374 train:tensor(0.2673, grad_fn=<MeanBackward0>) test:tensor(0.3088, grad_fn=<MeanBackward0>)\n",
      "times:375 train:tensor(0.2671, grad_fn=<MeanBackward0>) test:tensor(0.3087, grad_fn=<MeanBackward0>)\n",
      "times:376 train:tensor(0.2669, grad_fn=<MeanBackward0>) test:tensor(0.3086, grad_fn=<MeanBackward0>)\n",
      "times:377 train:tensor(0.2667, grad_fn=<MeanBackward0>) test:tensor(0.3085, grad_fn=<MeanBackward0>)\n",
      "times:378 train:tensor(0.2666, grad_fn=<MeanBackward0>) test:tensor(0.3083, grad_fn=<MeanBackward0>)\n",
      "times:379 train:tensor(0.2664, grad_fn=<MeanBackward0>) test:tensor(0.3082, grad_fn=<MeanBackward0>)\n",
      "times:380 train:tensor(0.2662, grad_fn=<MeanBackward0>) test:tensor(0.3081, grad_fn=<MeanBackward0>)\n",
      "times:381 train:tensor(0.2660, grad_fn=<MeanBackward0>) test:tensor(0.3080, grad_fn=<MeanBackward0>)\n",
      "times:382 train:tensor(0.2659, grad_fn=<MeanBackward0>) test:tensor(0.3079, grad_fn=<MeanBackward0>)\n",
      "times:383 train:tensor(0.2657, grad_fn=<MeanBackward0>) test:tensor(0.3077, grad_fn=<MeanBackward0>)\n",
      "times:384 train:tensor(0.2655, grad_fn=<MeanBackward0>) test:tensor(0.3076, grad_fn=<MeanBackward0>)\n",
      "times:385 train:tensor(0.2653, grad_fn=<MeanBackward0>) test:tensor(0.3075, grad_fn=<MeanBackward0>)\n",
      "times:386 train:tensor(0.2651, grad_fn=<MeanBackward0>) test:tensor(0.3073, grad_fn=<MeanBackward0>)\n",
      "times:387 train:tensor(0.2649, grad_fn=<MeanBackward0>) test:tensor(0.3072, grad_fn=<MeanBackward0>)\n",
      "times:388 train:tensor(0.2647, grad_fn=<MeanBackward0>) test:tensor(0.3070, grad_fn=<MeanBackward0>)\n",
      "times:389 train:tensor(0.2645, grad_fn=<MeanBackward0>) test:tensor(0.3069, grad_fn=<MeanBackward0>)\n",
      "times:390 train:tensor(0.2644, grad_fn=<MeanBackward0>) test:tensor(0.3067, grad_fn=<MeanBackward0>)\n",
      "times:391 train:tensor(0.2642, grad_fn=<MeanBackward0>) test:tensor(0.3066, grad_fn=<MeanBackward0>)\n",
      "times:392 train:tensor(0.2640, grad_fn=<MeanBackward0>) test:tensor(0.3064, grad_fn=<MeanBackward0>)\n",
      "times:393 train:tensor(0.2638, grad_fn=<MeanBackward0>) test:tensor(0.3063, grad_fn=<MeanBackward0>)\n",
      "times:394 train:tensor(0.2636, grad_fn=<MeanBackward0>) test:tensor(0.3061, grad_fn=<MeanBackward0>)\n",
      "times:395 train:tensor(0.2634, grad_fn=<MeanBackward0>) test:tensor(0.3059, grad_fn=<MeanBackward0>)\n",
      "times:396 train:tensor(0.2631, grad_fn=<MeanBackward0>) test:tensor(0.3057, grad_fn=<MeanBackward0>)\n",
      "times:397 train:tensor(0.2629, grad_fn=<MeanBackward0>) test:tensor(0.3055, grad_fn=<MeanBackward0>)\n",
      "times:398 train:tensor(0.2627, grad_fn=<MeanBackward0>) test:tensor(0.3053, grad_fn=<MeanBackward0>)\n",
      "times:399 train:tensor(0.2625, grad_fn=<MeanBackward0>) test:tensor(0.3051, grad_fn=<MeanBackward0>)\n",
      "times:400 train:tensor(0.2623, grad_fn=<MeanBackward0>) test:tensor(0.3049, grad_fn=<MeanBackward0>)\n",
      "times:401 train:tensor(0.2621, grad_fn=<MeanBackward0>) test:tensor(0.3047, grad_fn=<MeanBackward0>)\n",
      "times:402 train:tensor(0.2618, grad_fn=<MeanBackward0>) test:tensor(0.3045, grad_fn=<MeanBackward0>)\n",
      "times:403 train:tensor(0.2616, grad_fn=<MeanBackward0>) test:tensor(0.3043, grad_fn=<MeanBackward0>)\n",
      "times:404 train:tensor(0.2614, grad_fn=<MeanBackward0>) test:tensor(0.3040, grad_fn=<MeanBackward0>)\n",
      "times:405 train:tensor(0.2612, grad_fn=<MeanBackward0>) test:tensor(0.3038, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:406 train:tensor(0.2609, grad_fn=<MeanBackward0>) test:tensor(0.3035, grad_fn=<MeanBackward0>)\n",
      "times:407 train:tensor(0.2607, grad_fn=<MeanBackward0>) test:tensor(0.3033, grad_fn=<MeanBackward0>)\n",
      "times:408 train:tensor(0.2604, grad_fn=<MeanBackward0>) test:tensor(0.3030, grad_fn=<MeanBackward0>)\n",
      "times:409 train:tensor(0.2602, grad_fn=<MeanBackward0>) test:tensor(0.3027, grad_fn=<MeanBackward0>)\n",
      "times:410 train:tensor(0.2599, grad_fn=<MeanBackward0>) test:tensor(0.3025, grad_fn=<MeanBackward0>)\n",
      "times:411 train:tensor(0.2597, grad_fn=<MeanBackward0>) test:tensor(0.3022, grad_fn=<MeanBackward0>)\n",
      "times:412 train:tensor(0.2594, grad_fn=<MeanBackward0>) test:tensor(0.3018, grad_fn=<MeanBackward0>)\n",
      "times:413 train:tensor(0.2592, grad_fn=<MeanBackward0>) test:tensor(0.3015, grad_fn=<MeanBackward0>)\n",
      "times:414 train:tensor(0.2589, grad_fn=<MeanBackward0>) test:tensor(0.3012, grad_fn=<MeanBackward0>)\n",
      "times:415 train:tensor(0.2586, grad_fn=<MeanBackward0>) test:tensor(0.3009, grad_fn=<MeanBackward0>)\n",
      "times:416 train:tensor(0.2584, grad_fn=<MeanBackward0>) test:tensor(0.3005, grad_fn=<MeanBackward0>)\n",
      "times:417 train:tensor(0.2581, grad_fn=<MeanBackward0>) test:tensor(0.3002, grad_fn=<MeanBackward0>)\n",
      "times:418 train:tensor(0.2578, grad_fn=<MeanBackward0>) test:tensor(0.2998, grad_fn=<MeanBackward0>)\n",
      "times:419 train:tensor(0.2575, grad_fn=<MeanBackward0>) test:tensor(0.2994, grad_fn=<MeanBackward0>)\n",
      "times:420 train:tensor(0.2573, grad_fn=<MeanBackward0>) test:tensor(0.2991, grad_fn=<MeanBackward0>)\n",
      "times:421 train:tensor(0.2570, grad_fn=<MeanBackward0>) test:tensor(0.2987, grad_fn=<MeanBackward0>)\n",
      "times:422 train:tensor(0.2567, grad_fn=<MeanBackward0>) test:tensor(0.2983, grad_fn=<MeanBackward0>)\n",
      "times:423 train:tensor(0.2565, grad_fn=<MeanBackward0>) test:tensor(0.2979, grad_fn=<MeanBackward0>)\n",
      "times:424 train:tensor(0.2562, grad_fn=<MeanBackward0>) test:tensor(0.2975, grad_fn=<MeanBackward0>)\n",
      "times:425 train:tensor(0.2560, grad_fn=<MeanBackward0>) test:tensor(0.2972, grad_fn=<MeanBackward0>)\n",
      "times:426 train:tensor(0.2557, grad_fn=<MeanBackward0>) test:tensor(0.2968, grad_fn=<MeanBackward0>)\n",
      "times:427 train:tensor(0.2555, grad_fn=<MeanBackward0>) test:tensor(0.2964, grad_fn=<MeanBackward0>)\n",
      "times:428 train:tensor(0.2553, grad_fn=<MeanBackward0>) test:tensor(0.2961, grad_fn=<MeanBackward0>)\n",
      "times:429 train:tensor(0.2551, grad_fn=<MeanBackward0>) test:tensor(0.2957, grad_fn=<MeanBackward0>)\n",
      "times:430 train:tensor(0.2549, grad_fn=<MeanBackward0>) test:tensor(0.2954, grad_fn=<MeanBackward0>)\n",
      "times:431 train:tensor(0.2547, grad_fn=<MeanBackward0>) test:tensor(0.2951, grad_fn=<MeanBackward0>)\n",
      "times:432 train:tensor(0.2545, grad_fn=<MeanBackward0>) test:tensor(0.2948, grad_fn=<MeanBackward0>)\n",
      "times:433 train:tensor(0.2543, grad_fn=<MeanBackward0>) test:tensor(0.2945, grad_fn=<MeanBackward0>)\n",
      "times:434 train:tensor(0.2542, grad_fn=<MeanBackward0>) test:tensor(0.2943, grad_fn=<MeanBackward0>)\n",
      "times:435 train:tensor(0.2540, grad_fn=<MeanBackward0>) test:tensor(0.2940, grad_fn=<MeanBackward0>)\n",
      "times:436 train:tensor(0.2539, grad_fn=<MeanBackward0>) test:tensor(0.2937, grad_fn=<MeanBackward0>)\n",
      "times:437 train:tensor(0.2538, grad_fn=<MeanBackward0>) test:tensor(0.2934, grad_fn=<MeanBackward0>)\n",
      "times:438 train:tensor(0.2536, grad_fn=<MeanBackward0>) test:tensor(0.2932, grad_fn=<MeanBackward0>)\n",
      "times:439 train:tensor(0.2535, grad_fn=<MeanBackward0>) test:tensor(0.2929, grad_fn=<MeanBackward0>)\n",
      "times:440 train:tensor(0.2534, grad_fn=<MeanBackward0>) test:tensor(0.2926, grad_fn=<MeanBackward0>)\n",
      "times:441 train:tensor(0.2533, grad_fn=<MeanBackward0>) test:tensor(0.2924, grad_fn=<MeanBackward0>)\n",
      "times:442 train:tensor(0.2531, grad_fn=<MeanBackward0>) test:tensor(0.2921, grad_fn=<MeanBackward0>)\n",
      "times:443 train:tensor(0.2530, grad_fn=<MeanBackward0>) test:tensor(0.2918, grad_fn=<MeanBackward0>)\n",
      "times:444 train:tensor(0.2529, grad_fn=<MeanBackward0>) test:tensor(0.2915, grad_fn=<MeanBackward0>)\n",
      "times:445 train:tensor(0.2528, grad_fn=<MeanBackward0>) test:tensor(0.2913, grad_fn=<MeanBackward0>)\n",
      "times:446 train:tensor(0.2526, grad_fn=<MeanBackward0>) test:tensor(0.2911, grad_fn=<MeanBackward0>)\n",
      "times:447 train:tensor(0.2525, grad_fn=<MeanBackward0>) test:tensor(0.2908, grad_fn=<MeanBackward0>)\n",
      "times:448 train:tensor(0.2524, grad_fn=<MeanBackward0>) test:tensor(0.2906, grad_fn=<MeanBackward0>)\n",
      "times:449 train:tensor(0.2523, grad_fn=<MeanBackward0>) test:tensor(0.2904, grad_fn=<MeanBackward0>)\n",
      "times:450 train:tensor(0.2522, grad_fn=<MeanBackward0>) test:tensor(0.2902, grad_fn=<MeanBackward0>)\n",
      "times:451 train:tensor(0.2521, grad_fn=<MeanBackward0>) test:tensor(0.2900, grad_fn=<MeanBackward0>)\n",
      "times:452 train:tensor(0.2520, grad_fn=<MeanBackward0>) test:tensor(0.2899, grad_fn=<MeanBackward0>)\n",
      "times:453 train:tensor(0.2518, grad_fn=<MeanBackward0>) test:tensor(0.2897, grad_fn=<MeanBackward0>)\n",
      "times:454 train:tensor(0.2517, grad_fn=<MeanBackward0>) test:tensor(0.2896, grad_fn=<MeanBackward0>)\n",
      "times:455 train:tensor(0.2516, grad_fn=<MeanBackward0>) test:tensor(0.2894, grad_fn=<MeanBackward0>)\n",
      "times:456 train:tensor(0.2515, grad_fn=<MeanBackward0>) test:tensor(0.2893, grad_fn=<MeanBackward0>)\n",
      "times:457 train:tensor(0.2514, grad_fn=<MeanBackward0>) test:tensor(0.2892, grad_fn=<MeanBackward0>)\n",
      "times:458 train:tensor(0.2513, grad_fn=<MeanBackward0>) test:tensor(0.2891, grad_fn=<MeanBackward0>)\n",
      "times:459 train:tensor(0.2512, grad_fn=<MeanBackward0>) test:tensor(0.2890, grad_fn=<MeanBackward0>)\n",
      "times:460 train:tensor(0.2511, grad_fn=<MeanBackward0>) test:tensor(0.2889, grad_fn=<MeanBackward0>)\n",
      "times:461 train:tensor(0.2510, grad_fn=<MeanBackward0>) test:tensor(0.2888, grad_fn=<MeanBackward0>)\n",
      "times:462 train:tensor(0.2509, grad_fn=<MeanBackward0>) test:tensor(0.2887, grad_fn=<MeanBackward0>)\n",
      "times:463 train:tensor(0.2508, grad_fn=<MeanBackward0>) test:tensor(0.2886, grad_fn=<MeanBackward0>)\n",
      "times:464 train:tensor(0.2507, grad_fn=<MeanBackward0>) test:tensor(0.2886, grad_fn=<MeanBackward0>)\n",
      "times:465 train:tensor(0.2506, grad_fn=<MeanBackward0>) test:tensor(0.2885, grad_fn=<MeanBackward0>)\n",
      "times:466 train:tensor(0.2505, grad_fn=<MeanBackward0>) test:tensor(0.2884, grad_fn=<MeanBackward0>)\n",
      "times:467 train:tensor(0.2504, grad_fn=<MeanBackward0>) test:tensor(0.2883, grad_fn=<MeanBackward0>)\n",
      "times:468 train:tensor(0.2503, grad_fn=<MeanBackward0>) test:tensor(0.2882, grad_fn=<MeanBackward0>)\n",
      "times:469 train:tensor(0.2502, grad_fn=<MeanBackward0>) test:tensor(0.2881, grad_fn=<MeanBackward0>)\n",
      "times:470 train:tensor(0.2501, grad_fn=<MeanBackward0>) test:tensor(0.2880, grad_fn=<MeanBackward0>)\n",
      "times:471 train:tensor(0.2500, grad_fn=<MeanBackward0>) test:tensor(0.2879, grad_fn=<MeanBackward0>)\n",
      "times:472 train:tensor(0.2499, grad_fn=<MeanBackward0>) test:tensor(0.2877, grad_fn=<MeanBackward0>)\n",
      "times:473 train:tensor(0.2498, grad_fn=<MeanBackward0>) test:tensor(0.2878, grad_fn=<MeanBackward0>)\n",
      "times:474 train:tensor(0.2497, grad_fn=<MeanBackward0>) test:tensor(0.2873, grad_fn=<MeanBackward0>)\n",
      "times:475 train:tensor(0.2496, grad_fn=<MeanBackward0>) test:tensor(0.2880, grad_fn=<MeanBackward0>)\n",
      "times:476 train:tensor(0.2497, grad_fn=<MeanBackward0>) test:tensor(0.2867, grad_fn=<MeanBackward0>)\n",
      "times:477 train:tensor(0.2499, grad_fn=<MeanBackward0>) test:tensor(0.2886, grad_fn=<MeanBackward0>)\n",
      "times:478 train:tensor(0.2509, grad_fn=<MeanBackward0>) test:tensor(0.2859, grad_fn=<MeanBackward0>)\n",
      "times:479 train:tensor(0.2514, grad_fn=<MeanBackward0>) test:tensor(0.2895, grad_fn=<MeanBackward0>)\n",
      "times:480 train:tensor(0.2551, grad_fn=<MeanBackward0>) test:tensor(0.2860, grad_fn=<MeanBackward0>)\n",
      "times:481 train:tensor(0.2541, grad_fn=<MeanBackward0>) test:tensor(0.2904, grad_fn=<MeanBackward0>)\n",
      "times:482 train:tensor(0.2538, grad_fn=<MeanBackward0>) test:tensor(0.2900, grad_fn=<MeanBackward0>)\n",
      "times:483 train:tensor(0.2503, grad_fn=<MeanBackward0>) test:tensor(0.2870, grad_fn=<MeanBackward0>)\n",
      "times:484 train:tensor(0.2540, grad_fn=<MeanBackward0>) test:tensor(0.2908, grad_fn=<MeanBackward0>)\n",
      "times:485 train:tensor(0.2512, grad_fn=<MeanBackward0>) test:tensor(0.2912, grad_fn=<MeanBackward0>)\n",
      "times:486 train:tensor(0.2530, grad_fn=<MeanBackward0>) test:tensor(0.2877, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:487 train:tensor(0.2504, grad_fn=<MeanBackward0>) test:tensor(0.2866, grad_fn=<MeanBackward0>)\n",
      "times:488 train:tensor(0.2524, grad_fn=<MeanBackward0>) test:tensor(0.2895, grad_fn=<MeanBackward0>)\n",
      "times:489 train:tensor(0.2522, grad_fn=<MeanBackward0>) test:tensor(0.2888, grad_fn=<MeanBackward0>)\n",
      "times:490 train:tensor(0.2513, grad_fn=<MeanBackward0>) test:tensor(0.2855, grad_fn=<MeanBackward0>)\n",
      "times:491 train:tensor(0.2516, grad_fn=<MeanBackward0>) test:tensor(0.2864, grad_fn=<MeanBackward0>)\n",
      "times:492 train:tensor(0.2488, grad_fn=<MeanBackward0>) test:tensor(0.2873, grad_fn=<MeanBackward0>)\n",
      "times:493 train:tensor(0.2506, grad_fn=<MeanBackward0>) test:tensor(0.2862, grad_fn=<MeanBackward0>)\n",
      "times:494 train:tensor(0.2487, grad_fn=<MeanBackward0>) test:tensor(0.2846, grad_fn=<MeanBackward0>)\n",
      "times:495 train:tensor(0.2506, grad_fn=<MeanBackward0>) test:tensor(0.2864, grad_fn=<MeanBackward0>)\n",
      "times:496 train:tensor(0.2493, grad_fn=<MeanBackward0>) test:tensor(0.2865, grad_fn=<MeanBackward0>)\n",
      "times:497 train:tensor(0.2497, grad_fn=<MeanBackward0>) test:tensor(0.2849, grad_fn=<MeanBackward0>)\n",
      "times:498 train:tensor(0.2493, grad_fn=<MeanBackward0>) test:tensor(0.2853, grad_fn=<MeanBackward0>)\n",
      "times:499 train:tensor(0.2487, grad_fn=<MeanBackward0>) test:tensor(0.2867, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_x = []\n",
    "loss_train = []\n",
    "loss_tes =[]\n",
    "for n in range(500):\n",
    "    loss_x.append(n)\n",
    "    train_out = rnn(x_train)\n",
    "    #print(train_out.size())\n",
    "    \n",
    "    loss = loss_func(train_out, y_train)\n",
    "    #print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_train.append(loss.data.item())\n",
    "    \n",
    "    test_out = rnn(x_test)\n",
    "    loss_test = loss_func(test_out,y_test)\n",
    "    loss_tes.append(loss_test.data.item())\n",
    "    print('times:'+str(n)+' train:'+str(loss)+' test:'+ str(loss_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Times')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.plot(loss_x,loss_train,label = 'train_loss')\n",
    "plt.plot(loss_x,loss_tes,label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.savefig('Analysis_loss.tif', dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05070995, 0.04952454, 0.04884437, ..., 0.02296055, 0.02446511,\n",
       "       0.02585766])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average = np.loadtxt(open(\"average.csv\"),delimiter=\",\",skiprows=0)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "D:\\softwares\\Program Files\\Python37\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.636916160583496, 11.686973571777344, 11.611454010009766, 11.701814651489258, 11.640459060668945, 11.481893539428711, 11.50269603729248, 11.611889839172363, 11.554426193237305, 11.598244667053223, 11.62209701538086, 11.79577922821045, 11.919951438903809, 11.759248733520508, 11.888781547546387, 11.762721061706543, 11.651314735412598, 11.550673484802246, 11.750004768371582, 11.630305290222168, 11.835027694702148, 11.782811164855957, 12.053817749023438, 12.20974063873291, 12.573625564575195, 12.524901390075684, 13.143271446228027, 13.216571807861328, 11.565595626831055, 11.563440322875977, 11.634492874145508, 11.641812324523926, 11.709940910339355, 11.790889739990234, 11.782201766967773, 11.782801628112793, 11.790120124816895, 11.796016693115234, 11.809345245361328, 11.811124801635742]\n"
     ]
    }
   ],
   "source": [
    "my_matrix = np.loadtxt(open(\"results.csv\"),delimiter=\",\",skiprows=0)\n",
    "#print(my_matrix)\n",
    " #对于矩阵而言，将矩阵倒数第一列之前的数值给了X（输入数据），将矩阵大最后一列的数值给了y（标签）\n",
    "X, y = my_matrix[:,:-1],my_matrix[:,-1]\n",
    "torch_y = torch.from_numpy(y)\n",
    "y_input = torch.tensor(torch_y , dtype=torch.float32)\n",
    "X_loss = []\n",
    "axis = []\n",
    "for n in range(40):\n",
    "    #X_minloss = 0\n",
    "    #print('start'+ str(X_minloss))\n",
    "    axis.append(n)\n",
    "    for i in range(30):\n",
    "        for j in range(len(X)):\n",
    "            X[j][n*30+i] = average[n*30+i]\n",
    "    \n",
    "            \n",
    "    torch_X = torch.from_numpy(X)\n",
    "    X_input = torch.tensor(torch_X , dtype=torch.float32)\n",
    "    X_input = X_input.reshape(X_input.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "    loss_func = My_loss()\n",
    "    \n",
    "    X_out = rnn(X_input)\n",
    "    X_minloss = loss_func(X_out,y_input)\n",
    "    X_minloss = X_minloss.data.item()\n",
    "    \n",
    "    #print(X_minloss)\n",
    "    \n",
    "    '''\n",
    "    for m in range(len(X)):\n",
    "        minloss = out[m].data.item() - X_out[m].data.item()\n",
    "        minloss = minloss\n",
    "        X_minloss += minloss\n",
    "    print('time: '+str(n)+'loss: '+str(X_minloss))\n",
    "    '''\n",
    "    X_loss.append(X_minloss)\n",
    "    \n",
    "print(X_loss)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = preprocessing.scale(X_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.14038990e-01 -3.83729162e-01 -5.80322251e-01 -3.45094753e-01\n",
      " -5.04816086e-01 -9.17595037e-01 -8.63441819e-01 -5.79187697e-01\n",
      " -7.28777490e-01 -6.14708911e-01 -5.52616300e-01 -1.00485463e-01\n",
      "  2.22760565e-01 -1.95581920e-01  1.41618870e-01 -1.86542730e-01\n",
      " -4.76556512e-01 -7.38546569e-01 -2.19645880e-01 -5.31248457e-01\n",
      "  1.68643841e-03 -1.34244023e-01  5.71242345e-01  9.77141979e-01\n",
      "  1.92440994e+00  1.79757081e+00  3.40731636e+00  3.59813241e+00\n",
      " -6.99701137e-01 -7.05311840e-01 -5.20347307e-01 -5.01293259e-01\n",
      " -3.23940415e-01 -1.13213816e-01 -1.35830412e-01 -1.34268849e-01\n",
      " -1.15217284e-01 -9.98672929e-02 -6.51703059e-02 -6.05377517e-02]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(results)\n",
    "axis =[]\n",
    "for n in range(40):\n",
    "    axis.append(n)\n",
    "#print(axis)\n",
    "\n",
    "plt.xlabel('Raman Segment')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.bar(axis, results)\n",
    "plt.savefig('Analysis.tif', dpi = 400)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
