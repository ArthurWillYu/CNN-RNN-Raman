{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " #首先，读取.CSV文件成矩阵的形式。\n",
    "my_matrix = np.loadtxt(open(\"results.csv\"),delimiter=\",\",skiprows=0)\n",
    "#print(my_matrix)\n",
    " #对于矩阵而言，将矩阵倒数第一列之前的数值给了X（输入数据），将矩阵大最后一列的数值给了y（标签）\n",
    "X, y = my_matrix[:,:-1],my_matrix[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.303831\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.26462700000000006, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.10434600000000001, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.050494281551999996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.039385539610559996, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.303831, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06682500000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001, 0.06615675000000001]\n"
     ]
    }
   ],
   "source": [
    "#print(y_train)\n",
    "label = y \n",
    "#print(y_train)\n",
    "sx_1 = 1\n",
    "sx_2 =(sx_1*0.99*0.27*0.99)\n",
    "sx_8 = sx_1*0.99*0.27*0.25*0.99\n",
    "sx_7 = sx_1*0.99*0.27*0.25*1\n",
    "sx_6 = sx_1*0.99*0.31*0.99\n",
    "sx_3 = sx_1*0.99*0.31*0.34\n",
    "sx_4 = sx_1*0.99*0.31*0.51*0.47*0.88*0.78\n",
    "sx_5 = sx_1*0.99*0.31*0.51*0.47*0.88*0.78*0.78\n",
    "print(sx_6)\n",
    "y = [sx_2 if i == 1 else i for i in y]\n",
    "y = [sx_8 if i == 7 else i for i in y]\n",
    "y = [1 if i == 0 else i for i in y]\n",
    "y = [sx_3 if i == 2 else i for i in y]\n",
    "y = [sx_4 if i == 3 else i for i in y]\n",
    "y = [sx_5 if i == 4 else i for i in y]\n",
    "y = [sx_6 if i == 5 else i for i in y]\n",
    "y = [sx_7 if i == 6 else i for i in y]\n",
    "\n",
    "#y_train = np.array(y_train)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " #利用train_test_split方法，将X,y随机划分问，训练集（X_train），训练集标签（X_test），测试卷（y_train），\n",
    " #测试集标签（y_test），安训练集：测试集=7:3的\n",
    " #概率划分，到此步骤，可以直接对数据进行处理\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    " #此步骤，是为了将训练集与数据集的数据分别保存为CSV文件\n",
    " #np.column_stack将两个矩阵进行组合连接\n",
    "train= np.column_stack((X_train,y_train))\n",
    " #numpy.savetxt 将txt文件保存为.csv结尾的文件\n",
    "np.savetxt('train_usual.csv',train, delimiter = ',')\n",
    "test = np.column_stack((X_test, y_test))\n",
    "np.savetxt('test_usual.csv', test, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train)\n",
    "label = y_train\n",
    "#print(label)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysx/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/ysx/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/ysx/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/ysx/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch_x = torch.from_numpy(X_train)\n",
    "torch_y = torch.from_numpy(y_train)\n",
    "test_x = torch.from_numpy(X_test)\n",
    "test_y = torch.from_numpy(y_test)\n",
    "y_train = torch.tensor(torch_y, dtype=torch.float32)\n",
    "y_test = torch.tensor(test_y,dtype = torch.float32)\n",
    "x_test = torch.tensor(test_x , dtype=torch.float32)\n",
    "x_train = torch.tensor(torch_x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "EPOCH = 500\n",
    "BATCH_SIZE = 100\n",
    "TIME_STEP = 20\n",
    "INPUT_SIZE = 60\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "x_train = x_train.reshape(x_train.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "#print(x_train.shape,x_test.shape)\n",
    "torch_dataset = Data.TensorDataset(x_train,y_train )\n",
    "train_loader = Data.DataLoader(dataset= torch_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=2)\n",
    "#print(x_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=64,         # rnn hidden unit\n",
    "            num_layers=3,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self , x, y):\n",
    "        time = 1\n",
    "        ave = torch.tensor(0)\n",
    "        for n in range(len(x)):\n",
    "            if y[n] == 1:\n",
    "                ave = torch.add(x[n], ave)\n",
    "                time += 1\n",
    "        #print(ave,time)\n",
    "        ave  = torch.true_divide(ave,time) \n",
    "        div = torch.div(x,ave)\n",
    "        loss = torch.mean(torch.pow((div - y),2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(X_out),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(60, 64, num_layers=3, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn =  RNN()\n",
    "print(rnn)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = LR)\n",
    "loss_func = My_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:0 train:tensor(0.6767, grad_fn=<MeanBackward0>) test:tensor(0.6881, grad_fn=<MeanBackward0>)\n",
      "times:1 train:tensor(0.6617, grad_fn=<MeanBackward0>) test:tensor(0.6714, grad_fn=<MeanBackward0>)\n",
      "times:2 train:tensor(0.6453, grad_fn=<MeanBackward0>) test:tensor(0.6433, grad_fn=<MeanBackward0>)\n",
      "times:3 train:tensor(0.6177, grad_fn=<MeanBackward0>) test:tensor(0.5845, grad_fn=<MeanBackward0>)\n",
      "times:4 train:tensor(0.5599, grad_fn=<MeanBackward0>) test:tensor(0.4242, grad_fn=<MeanBackward0>)\n",
      "times:5 train:tensor(0.4003, grad_fn=<MeanBackward0>) test:tensor(2.1731, grad_fn=<MeanBackward0>)\n",
      "times:6 train:tensor(2.0567, grad_fn=<MeanBackward0>) test:tensor(0.9399, grad_fn=<MeanBackward0>)\n",
      "times:7 train:tensor(0.9064, grad_fn=<MeanBackward0>) test:tensor(0.8223, grad_fn=<MeanBackward0>)\n",
      "times:8 train:tensor(0.7931, grad_fn=<MeanBackward0>) test:tensor(0.7795, grad_fn=<MeanBackward0>)\n",
      "times:9 train:tensor(0.7517, grad_fn=<MeanBackward0>) test:tensor(0.7575, grad_fn=<MeanBackward0>)\n",
      "times:10 train:tensor(0.7302, grad_fn=<MeanBackward0>) test:tensor(0.7439, grad_fn=<MeanBackward0>)\n",
      "times:11 train:tensor(0.7170, grad_fn=<MeanBackward0>) test:tensor(0.7348, grad_fn=<MeanBackward0>)\n",
      "times:12 train:tensor(0.7081, grad_fn=<MeanBackward0>) test:tensor(0.7281, grad_fn=<MeanBackward0>)\n",
      "times:13 train:tensor(0.7016, grad_fn=<MeanBackward0>) test:tensor(0.7231, grad_fn=<MeanBackward0>)\n",
      "times:14 train:tensor(0.6967, grad_fn=<MeanBackward0>) test:tensor(0.7190, grad_fn=<MeanBackward0>)\n",
      "times:15 train:tensor(0.6928, grad_fn=<MeanBackward0>) test:tensor(0.7157, grad_fn=<MeanBackward0>)\n",
      "times:16 train:tensor(0.6896, grad_fn=<MeanBackward0>) test:tensor(0.7130, grad_fn=<MeanBackward0>)\n",
      "times:17 train:tensor(0.6869, grad_fn=<MeanBackward0>) test:tensor(0.7106, grad_fn=<MeanBackward0>)\n",
      "times:18 train:tensor(0.6846, grad_fn=<MeanBackward0>) test:tensor(0.7085, grad_fn=<MeanBackward0>)\n",
      "times:19 train:tensor(0.6825, grad_fn=<MeanBackward0>) test:tensor(0.7067, grad_fn=<MeanBackward0>)\n",
      "times:20 train:tensor(0.6807, grad_fn=<MeanBackward0>) test:tensor(0.7050, grad_fn=<MeanBackward0>)\n",
      "times:21 train:tensor(0.6791, grad_fn=<MeanBackward0>) test:tensor(0.7034, grad_fn=<MeanBackward0>)\n",
      "times:22 train:tensor(0.6776, grad_fn=<MeanBackward0>) test:tensor(0.7020, grad_fn=<MeanBackward0>)\n",
      "times:23 train:tensor(0.6761, grad_fn=<MeanBackward0>) test:tensor(0.7006, grad_fn=<MeanBackward0>)\n",
      "times:24 train:tensor(0.6748, grad_fn=<MeanBackward0>) test:tensor(0.6992, grad_fn=<MeanBackward0>)\n",
      "times:25 train:tensor(0.6735, grad_fn=<MeanBackward0>) test:tensor(0.6979, grad_fn=<MeanBackward0>)\n",
      "times:26 train:tensor(0.6722, grad_fn=<MeanBackward0>) test:tensor(0.6967, grad_fn=<MeanBackward0>)\n",
      "times:27 train:tensor(0.6710, grad_fn=<MeanBackward0>) test:tensor(0.6954, grad_fn=<MeanBackward0>)\n",
      "times:28 train:tensor(0.6697, grad_fn=<MeanBackward0>) test:tensor(0.6941, grad_fn=<MeanBackward0>)\n",
      "times:29 train:tensor(0.6685, grad_fn=<MeanBackward0>) test:tensor(0.6927, grad_fn=<MeanBackward0>)\n",
      "times:30 train:tensor(0.6672, grad_fn=<MeanBackward0>) test:tensor(0.6914, grad_fn=<MeanBackward0>)\n",
      "times:31 train:tensor(0.6659, grad_fn=<MeanBackward0>) test:tensor(0.6900, grad_fn=<MeanBackward0>)\n",
      "times:32 train:tensor(0.6645, grad_fn=<MeanBackward0>) test:tensor(0.6885, grad_fn=<MeanBackward0>)\n",
      "times:33 train:tensor(0.6630, grad_fn=<MeanBackward0>) test:tensor(0.6869, grad_fn=<MeanBackward0>)\n",
      "times:34 train:tensor(0.6615, grad_fn=<MeanBackward0>) test:tensor(0.6852, grad_fn=<MeanBackward0>)\n",
      "times:35 train:tensor(0.6599, grad_fn=<MeanBackward0>) test:tensor(0.6834, grad_fn=<MeanBackward0>)\n",
      "times:36 train:tensor(0.6582, grad_fn=<MeanBackward0>) test:tensor(0.6815, grad_fn=<MeanBackward0>)\n",
      "times:37 train:tensor(0.6563, grad_fn=<MeanBackward0>) test:tensor(0.6793, grad_fn=<MeanBackward0>)\n",
      "times:38 train:tensor(0.6542, grad_fn=<MeanBackward0>) test:tensor(0.6770, grad_fn=<MeanBackward0>)\n",
      "times:39 train:tensor(0.6520, grad_fn=<MeanBackward0>) test:tensor(0.6744, grad_fn=<MeanBackward0>)\n",
      "times:40 train:tensor(0.6495, grad_fn=<MeanBackward0>) test:tensor(0.6716, grad_fn=<MeanBackward0>)\n",
      "times:41 train:tensor(0.6468, grad_fn=<MeanBackward0>) test:tensor(0.6684, grad_fn=<MeanBackward0>)\n",
      "times:42 train:tensor(0.6437, grad_fn=<MeanBackward0>) test:tensor(0.6649, grad_fn=<MeanBackward0>)\n",
      "times:43 train:tensor(0.6403, grad_fn=<MeanBackward0>) test:tensor(0.6609, grad_fn=<MeanBackward0>)\n",
      "times:44 train:tensor(0.6365, grad_fn=<MeanBackward0>) test:tensor(0.6564, grad_fn=<MeanBackward0>)\n",
      "times:45 train:tensor(0.6322, grad_fn=<MeanBackward0>) test:tensor(0.6514, grad_fn=<MeanBackward0>)\n",
      "times:46 train:tensor(0.6274, grad_fn=<MeanBackward0>) test:tensor(0.6457, grad_fn=<MeanBackward0>)\n",
      "times:47 train:tensor(0.6219, grad_fn=<MeanBackward0>) test:tensor(0.6393, grad_fn=<MeanBackward0>)\n",
      "times:48 train:tensor(0.6157, grad_fn=<MeanBackward0>) test:tensor(0.6321, grad_fn=<MeanBackward0>)\n",
      "times:49 train:tensor(0.6088, grad_fn=<MeanBackward0>) test:tensor(0.6240, grad_fn=<MeanBackward0>)\n",
      "times:50 train:tensor(0.6010, grad_fn=<MeanBackward0>) test:tensor(0.6151, grad_fn=<MeanBackward0>)\n",
      "times:51 train:tensor(0.5925, grad_fn=<MeanBackward0>) test:tensor(0.6055, grad_fn=<MeanBackward0>)\n",
      "times:52 train:tensor(0.5832, grad_fn=<MeanBackward0>) test:tensor(0.5953, grad_fn=<MeanBackward0>)\n",
      "times:53 train:tensor(0.5733, grad_fn=<MeanBackward0>) test:tensor(0.5849, grad_fn=<MeanBackward0>)\n",
      "times:54 train:tensor(0.5632, grad_fn=<MeanBackward0>) test:tensor(0.5746, grad_fn=<MeanBackward0>)\n",
      "times:55 train:tensor(0.5532, grad_fn=<MeanBackward0>) test:tensor(0.5648, grad_fn=<MeanBackward0>)\n",
      "times:56 train:tensor(0.5436, grad_fn=<MeanBackward0>) test:tensor(0.5556, grad_fn=<MeanBackward0>)\n",
      "times:57 train:tensor(0.5346, grad_fn=<MeanBackward0>) test:tensor(0.5468, grad_fn=<MeanBackward0>)\n",
      "times:58 train:tensor(0.5259, grad_fn=<MeanBackward0>) test:tensor(0.5375, grad_fn=<MeanBackward0>)\n",
      "times:59 train:tensor(0.5168, grad_fn=<MeanBackward0>) test:tensor(0.5267, grad_fn=<MeanBackward0>)\n",
      "times:60 train:tensor(0.5062, grad_fn=<MeanBackward0>) test:tensor(0.5135, grad_fn=<MeanBackward0>)\n",
      "times:61 train:tensor(0.4934, grad_fn=<MeanBackward0>) test:tensor(0.4977, grad_fn=<MeanBackward0>)\n",
      "times:62 train:tensor(0.4780, grad_fn=<MeanBackward0>) test:tensor(0.4795, grad_fn=<MeanBackward0>)\n",
      "times:63 train:tensor(0.4604, grad_fn=<MeanBackward0>) test:tensor(0.4604, grad_fn=<MeanBackward0>)\n",
      "times:64 train:tensor(0.4418, grad_fn=<MeanBackward0>) test:tensor(0.4416, grad_fn=<MeanBackward0>)\n",
      "times:65 train:tensor(0.4238, grad_fn=<MeanBackward0>) test:tensor(0.4241, grad_fn=<MeanBackward0>)\n",
      "times:66 train:tensor(0.4069, grad_fn=<MeanBackward0>) test:tensor(0.4074, grad_fn=<MeanBackward0>)\n",
      "times:67 train:tensor(0.3908, grad_fn=<MeanBackward0>) test:tensor(0.3907, grad_fn=<MeanBackward0>)\n",
      "times:68 train:tensor(0.3746, grad_fn=<MeanBackward0>) test:tensor(0.3731, grad_fn=<MeanBackward0>)\n",
      "times:69 train:tensor(0.3574, grad_fn=<MeanBackward0>) test:tensor(0.3550, grad_fn=<MeanBackward0>)\n",
      "times:70 train:tensor(0.3393, grad_fn=<MeanBackward0>) test:tensor(0.3373, grad_fn=<MeanBackward0>)\n",
      "times:71 train:tensor(0.3213, grad_fn=<MeanBackward0>) test:tensor(0.3215, grad_fn=<MeanBackward0>)\n",
      "times:72 train:tensor(0.3051, grad_fn=<MeanBackward0>) test:tensor(0.3084, grad_fn=<MeanBackward0>)\n",
      "times:73 train:tensor(0.2913, grad_fn=<MeanBackward0>) test:tensor(0.2967, grad_fn=<MeanBackward0>)\n",
      "times:74 train:tensor(0.2791, grad_fn=<MeanBackward0>) test:tensor(0.2851, grad_fn=<MeanBackward0>)\n",
      "times:75 train:tensor(0.2672, grad_fn=<MeanBackward0>) test:tensor(0.2738, grad_fn=<MeanBackward0>)\n",
      "times:76 train:tensor(0.2558, grad_fn=<MeanBackward0>) test:tensor(0.2641, grad_fn=<MeanBackward0>)\n",
      "times:77 train:tensor(0.2461, grad_fn=<MeanBackward0>) test:tensor(0.2569, grad_fn=<MeanBackward0>)\n",
      "times:78 train:tensor(0.2388, grad_fn=<MeanBackward0>) test:tensor(0.2517, grad_fn=<MeanBackward0>)\n",
      "times:79 train:tensor(0.2334, grad_fn=<MeanBackward0>) test:tensor(0.2478, grad_fn=<MeanBackward0>)\n",
      "times:80 train:tensor(0.2291, grad_fn=<MeanBackward0>) test:tensor(0.2449, grad_fn=<MeanBackward0>)\n",
      "times:81 train:tensor(0.2255, grad_fn=<MeanBackward0>) test:tensor(0.2430, grad_fn=<MeanBackward0>)\n",
      "times:82 train:tensor(0.2226, grad_fn=<MeanBackward0>) test:tensor(0.2421, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:83 train:tensor(0.2208, grad_fn=<MeanBackward0>) test:tensor(0.2421, grad_fn=<MeanBackward0>)\n",
      "times:84 train:tensor(0.2200, grad_fn=<MeanBackward0>) test:tensor(0.2426, grad_fn=<MeanBackward0>)\n",
      "times:85 train:tensor(0.2198, grad_fn=<MeanBackward0>) test:tensor(0.2433, grad_fn=<MeanBackward0>)\n",
      "times:86 train:tensor(0.2199, grad_fn=<MeanBackward0>) test:tensor(0.2439, grad_fn=<MeanBackward0>)\n",
      "times:87 train:tensor(0.2202, grad_fn=<MeanBackward0>) test:tensor(0.2444, grad_fn=<MeanBackward0>)\n",
      "times:88 train:tensor(0.2203, grad_fn=<MeanBackward0>) test:tensor(0.2446, grad_fn=<MeanBackward0>)\n",
      "times:89 train:tensor(0.2203, grad_fn=<MeanBackward0>) test:tensor(0.2444, grad_fn=<MeanBackward0>)\n",
      "times:90 train:tensor(0.2200, grad_fn=<MeanBackward0>) test:tensor(0.2438, grad_fn=<MeanBackward0>)\n",
      "times:91 train:tensor(0.2193, grad_fn=<MeanBackward0>) test:tensor(0.2429, grad_fn=<MeanBackward0>)\n",
      "times:92 train:tensor(0.2185, grad_fn=<MeanBackward0>) test:tensor(0.2417, grad_fn=<MeanBackward0>)\n",
      "times:93 train:tensor(0.2174, grad_fn=<MeanBackward0>) test:tensor(0.2403, grad_fn=<MeanBackward0>)\n",
      "times:94 train:tensor(0.2162, grad_fn=<MeanBackward0>) test:tensor(0.2390, grad_fn=<MeanBackward0>)\n",
      "times:95 train:tensor(0.2150, grad_fn=<MeanBackward0>) test:tensor(0.2378, grad_fn=<MeanBackward0>)\n",
      "times:96 train:tensor(0.2138, grad_fn=<MeanBackward0>) test:tensor(0.2366, grad_fn=<MeanBackward0>)\n",
      "times:97 train:tensor(0.2125, grad_fn=<MeanBackward0>) test:tensor(0.2354, grad_fn=<MeanBackward0>)\n",
      "times:98 train:tensor(0.2111, grad_fn=<MeanBackward0>) test:tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "times:99 train:tensor(0.2099, grad_fn=<MeanBackward0>) test:tensor(0.2335, grad_fn=<MeanBackward0>)\n",
      "times:100 train:tensor(0.2089, grad_fn=<MeanBackward0>) test:tensor(0.2326, grad_fn=<MeanBackward0>)\n",
      "times:101 train:tensor(0.2080, grad_fn=<MeanBackward0>) test:tensor(0.2319, grad_fn=<MeanBackward0>)\n",
      "times:102 train:tensor(0.2072, grad_fn=<MeanBackward0>) test:tensor(0.2313, grad_fn=<MeanBackward0>)\n",
      "times:103 train:tensor(0.2067, grad_fn=<MeanBackward0>) test:tensor(0.2310, grad_fn=<MeanBackward0>)\n",
      "times:104 train:tensor(0.2064, grad_fn=<MeanBackward0>) test:tensor(0.2307, grad_fn=<MeanBackward0>)\n",
      "times:105 train:tensor(0.2062, grad_fn=<MeanBackward0>) test:tensor(0.2306, grad_fn=<MeanBackward0>)\n",
      "times:106 train:tensor(0.2061, grad_fn=<MeanBackward0>) test:tensor(0.2305, grad_fn=<MeanBackward0>)\n",
      "times:107 train:tensor(0.2062, grad_fn=<MeanBackward0>) test:tensor(0.2304, grad_fn=<MeanBackward0>)\n",
      "times:108 train:tensor(0.2063, grad_fn=<MeanBackward0>) test:tensor(0.2303, grad_fn=<MeanBackward0>)\n",
      "times:109 train:tensor(0.2063, grad_fn=<MeanBackward0>) test:tensor(0.2301, grad_fn=<MeanBackward0>)\n",
      "times:110 train:tensor(0.2062, grad_fn=<MeanBackward0>) test:tensor(0.2298, grad_fn=<MeanBackward0>)\n",
      "times:111 train:tensor(0.2060, grad_fn=<MeanBackward0>) test:tensor(0.2295, grad_fn=<MeanBackward0>)\n",
      "times:112 train:tensor(0.2056, grad_fn=<MeanBackward0>) test:tensor(0.2291, grad_fn=<MeanBackward0>)\n",
      "times:113 train:tensor(0.2053, grad_fn=<MeanBackward0>) test:tensor(0.2287, grad_fn=<MeanBackward0>)\n",
      "times:114 train:tensor(0.2048, grad_fn=<MeanBackward0>) test:tensor(0.2284, grad_fn=<MeanBackward0>)\n",
      "times:115 train:tensor(0.2045, grad_fn=<MeanBackward0>) test:tensor(0.2281, grad_fn=<MeanBackward0>)\n",
      "times:116 train:tensor(0.2041, grad_fn=<MeanBackward0>) test:tensor(0.2278, grad_fn=<MeanBackward0>)\n",
      "times:117 train:tensor(0.2039, grad_fn=<MeanBackward0>) test:tensor(0.2276, grad_fn=<MeanBackward0>)\n",
      "times:118 train:tensor(0.2036, grad_fn=<MeanBackward0>) test:tensor(0.2274, grad_fn=<MeanBackward0>)\n",
      "times:119 train:tensor(0.2034, grad_fn=<MeanBackward0>) test:tensor(0.2273, grad_fn=<MeanBackward0>)\n",
      "times:120 train:tensor(0.2032, grad_fn=<MeanBackward0>) test:tensor(0.2271, grad_fn=<MeanBackward0>)\n",
      "times:121 train:tensor(0.2030, grad_fn=<MeanBackward0>) test:tensor(0.2269, grad_fn=<MeanBackward0>)\n",
      "times:122 train:tensor(0.2028, grad_fn=<MeanBackward0>) test:tensor(0.2268, grad_fn=<MeanBackward0>)\n",
      "times:123 train:tensor(0.2027, grad_fn=<MeanBackward0>) test:tensor(0.2266, grad_fn=<MeanBackward0>)\n",
      "times:124 train:tensor(0.2025, grad_fn=<MeanBackward0>) test:tensor(0.2264, grad_fn=<MeanBackward0>)\n",
      "times:125 train:tensor(0.2023, grad_fn=<MeanBackward0>) test:tensor(0.2262, grad_fn=<MeanBackward0>)\n",
      "times:126 train:tensor(0.2021, grad_fn=<MeanBackward0>) test:tensor(0.2260, grad_fn=<MeanBackward0>)\n",
      "times:127 train:tensor(0.2019, grad_fn=<MeanBackward0>) test:tensor(0.2258, grad_fn=<MeanBackward0>)\n",
      "times:128 train:tensor(0.2016, grad_fn=<MeanBackward0>) test:tensor(0.2255, grad_fn=<MeanBackward0>)\n",
      "times:129 train:tensor(0.2014, grad_fn=<MeanBackward0>) test:tensor(0.2253, grad_fn=<MeanBackward0>)\n",
      "times:130 train:tensor(0.2012, grad_fn=<MeanBackward0>) test:tensor(0.2250, grad_fn=<MeanBackward0>)\n",
      "times:131 train:tensor(0.2010, grad_fn=<MeanBackward0>) test:tensor(0.2248, grad_fn=<MeanBackward0>)\n",
      "times:132 train:tensor(0.2007, grad_fn=<MeanBackward0>) test:tensor(0.2245, grad_fn=<MeanBackward0>)\n",
      "times:133 train:tensor(0.2005, grad_fn=<MeanBackward0>) test:tensor(0.2243, grad_fn=<MeanBackward0>)\n",
      "times:134 train:tensor(0.2003, grad_fn=<MeanBackward0>) test:tensor(0.2240, grad_fn=<MeanBackward0>)\n",
      "times:135 train:tensor(0.2001, grad_fn=<MeanBackward0>) test:tensor(0.2238, grad_fn=<MeanBackward0>)\n",
      "times:136 train:tensor(0.1999, grad_fn=<MeanBackward0>) test:tensor(0.2235, grad_fn=<MeanBackward0>)\n",
      "times:137 train:tensor(0.1997, grad_fn=<MeanBackward0>) test:tensor(0.2233, grad_fn=<MeanBackward0>)\n",
      "times:138 train:tensor(0.1995, grad_fn=<MeanBackward0>) test:tensor(0.2230, grad_fn=<MeanBackward0>)\n",
      "times:139 train:tensor(0.1993, grad_fn=<MeanBackward0>) test:tensor(0.2228, grad_fn=<MeanBackward0>)\n",
      "times:140 train:tensor(0.1991, grad_fn=<MeanBackward0>) test:tensor(0.2225, grad_fn=<MeanBackward0>)\n",
      "times:141 train:tensor(0.1989, grad_fn=<MeanBackward0>) test:tensor(0.2223, grad_fn=<MeanBackward0>)\n",
      "times:142 train:tensor(0.1987, grad_fn=<MeanBackward0>) test:tensor(0.2221, grad_fn=<MeanBackward0>)\n",
      "times:143 train:tensor(0.1985, grad_fn=<MeanBackward0>) test:tensor(0.2218, grad_fn=<MeanBackward0>)\n",
      "times:144 train:tensor(0.1983, grad_fn=<MeanBackward0>) test:tensor(0.2216, grad_fn=<MeanBackward0>)\n",
      "times:145 train:tensor(0.1981, grad_fn=<MeanBackward0>) test:tensor(0.2214, grad_fn=<MeanBackward0>)\n",
      "times:146 train:tensor(0.1979, grad_fn=<MeanBackward0>) test:tensor(0.2212, grad_fn=<MeanBackward0>)\n",
      "times:147 train:tensor(0.1977, grad_fn=<MeanBackward0>) test:tensor(0.2210, grad_fn=<MeanBackward0>)\n",
      "times:148 train:tensor(0.1975, grad_fn=<MeanBackward0>) test:tensor(0.2207, grad_fn=<MeanBackward0>)\n",
      "times:149 train:tensor(0.1973, grad_fn=<MeanBackward0>) test:tensor(0.2205, grad_fn=<MeanBackward0>)\n",
      "times:150 train:tensor(0.1971, grad_fn=<MeanBackward0>) test:tensor(0.2203, grad_fn=<MeanBackward0>)\n",
      "times:151 train:tensor(0.1969, grad_fn=<MeanBackward0>) test:tensor(0.2201, grad_fn=<MeanBackward0>)\n",
      "times:152 train:tensor(0.1966, grad_fn=<MeanBackward0>) test:tensor(0.2198, grad_fn=<MeanBackward0>)\n",
      "times:153 train:tensor(0.1964, grad_fn=<MeanBackward0>) test:tensor(0.2196, grad_fn=<MeanBackward0>)\n",
      "times:154 train:tensor(0.1962, grad_fn=<MeanBackward0>) test:tensor(0.2193, grad_fn=<MeanBackward0>)\n",
      "times:155 train:tensor(0.1960, grad_fn=<MeanBackward0>) test:tensor(0.2191, grad_fn=<MeanBackward0>)\n",
      "times:156 train:tensor(0.1958, grad_fn=<MeanBackward0>) test:tensor(0.2188, grad_fn=<MeanBackward0>)\n",
      "times:157 train:tensor(0.1956, grad_fn=<MeanBackward0>) test:tensor(0.2186, grad_fn=<MeanBackward0>)\n",
      "times:158 train:tensor(0.1953, grad_fn=<MeanBackward0>) test:tensor(0.2183, grad_fn=<MeanBackward0>)\n",
      "times:159 train:tensor(0.1951, grad_fn=<MeanBackward0>) test:tensor(0.2180, grad_fn=<MeanBackward0>)\n",
      "times:160 train:tensor(0.1949, grad_fn=<MeanBackward0>) test:tensor(0.2178, grad_fn=<MeanBackward0>)\n",
      "times:161 train:tensor(0.1947, grad_fn=<MeanBackward0>) test:tensor(0.2175, grad_fn=<MeanBackward0>)\n",
      "times:162 train:tensor(0.1944, grad_fn=<MeanBackward0>) test:tensor(0.2173, grad_fn=<MeanBackward0>)\n",
      "times:163 train:tensor(0.1942, grad_fn=<MeanBackward0>) test:tensor(0.2170, grad_fn=<MeanBackward0>)\n",
      "times:164 train:tensor(0.1940, grad_fn=<MeanBackward0>) test:tensor(0.2167, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:165 train:tensor(0.1938, grad_fn=<MeanBackward0>) test:tensor(0.2165, grad_fn=<MeanBackward0>)\n",
      "times:166 train:tensor(0.1935, grad_fn=<MeanBackward0>) test:tensor(0.2162, grad_fn=<MeanBackward0>)\n",
      "times:167 train:tensor(0.1933, grad_fn=<MeanBackward0>) test:tensor(0.2160, grad_fn=<MeanBackward0>)\n",
      "times:168 train:tensor(0.1931, grad_fn=<MeanBackward0>) test:tensor(0.2157, grad_fn=<MeanBackward0>)\n",
      "times:169 train:tensor(0.1928, grad_fn=<MeanBackward0>) test:tensor(0.2154, grad_fn=<MeanBackward0>)\n",
      "times:170 train:tensor(0.1926, grad_fn=<MeanBackward0>) test:tensor(0.2152, grad_fn=<MeanBackward0>)\n",
      "times:171 train:tensor(0.1923, grad_fn=<MeanBackward0>) test:tensor(0.2149, grad_fn=<MeanBackward0>)\n",
      "times:172 train:tensor(0.1921, grad_fn=<MeanBackward0>) test:tensor(0.2146, grad_fn=<MeanBackward0>)\n",
      "times:173 train:tensor(0.1919, grad_fn=<MeanBackward0>) test:tensor(0.2144, grad_fn=<MeanBackward0>)\n",
      "times:174 train:tensor(0.1916, grad_fn=<MeanBackward0>) test:tensor(0.2141, grad_fn=<MeanBackward0>)\n",
      "times:175 train:tensor(0.1914, grad_fn=<MeanBackward0>) test:tensor(0.2138, grad_fn=<MeanBackward0>)\n",
      "times:176 train:tensor(0.1911, grad_fn=<MeanBackward0>) test:tensor(0.2135, grad_fn=<MeanBackward0>)\n",
      "times:177 train:tensor(0.1909, grad_fn=<MeanBackward0>) test:tensor(0.2133, grad_fn=<MeanBackward0>)\n",
      "times:178 train:tensor(0.1906, grad_fn=<MeanBackward0>) test:tensor(0.2130, grad_fn=<MeanBackward0>)\n",
      "times:179 train:tensor(0.1904, grad_fn=<MeanBackward0>) test:tensor(0.2127, grad_fn=<MeanBackward0>)\n",
      "times:180 train:tensor(0.1901, grad_fn=<MeanBackward0>) test:tensor(0.2124, grad_fn=<MeanBackward0>)\n",
      "times:181 train:tensor(0.1898, grad_fn=<MeanBackward0>) test:tensor(0.2121, grad_fn=<MeanBackward0>)\n",
      "times:182 train:tensor(0.1896, grad_fn=<MeanBackward0>) test:tensor(0.2119, grad_fn=<MeanBackward0>)\n",
      "times:183 train:tensor(0.1893, grad_fn=<MeanBackward0>) test:tensor(0.2116, grad_fn=<MeanBackward0>)\n",
      "times:184 train:tensor(0.1891, grad_fn=<MeanBackward0>) test:tensor(0.2113, grad_fn=<MeanBackward0>)\n",
      "times:185 train:tensor(0.1888, grad_fn=<MeanBackward0>) test:tensor(0.2110, grad_fn=<MeanBackward0>)\n",
      "times:186 train:tensor(0.1885, grad_fn=<MeanBackward0>) test:tensor(0.2108, grad_fn=<MeanBackward0>)\n",
      "times:187 train:tensor(0.1883, grad_fn=<MeanBackward0>) test:tensor(0.2105, grad_fn=<MeanBackward0>)\n",
      "times:188 train:tensor(0.1880, grad_fn=<MeanBackward0>) test:tensor(0.2102, grad_fn=<MeanBackward0>)\n",
      "times:189 train:tensor(0.1877, grad_fn=<MeanBackward0>) test:tensor(0.2100, grad_fn=<MeanBackward0>)\n",
      "times:190 train:tensor(0.1875, grad_fn=<MeanBackward0>) test:tensor(0.2098, grad_fn=<MeanBackward0>)\n",
      "times:191 train:tensor(0.1872, grad_fn=<MeanBackward0>) test:tensor(0.2095, grad_fn=<MeanBackward0>)\n",
      "times:192 train:tensor(0.1870, grad_fn=<MeanBackward0>) test:tensor(0.2093, grad_fn=<MeanBackward0>)\n",
      "times:193 train:tensor(0.1867, grad_fn=<MeanBackward0>) test:tensor(0.2091, grad_fn=<MeanBackward0>)\n",
      "times:194 train:tensor(0.1865, grad_fn=<MeanBackward0>) test:tensor(0.2089, grad_fn=<MeanBackward0>)\n",
      "times:195 train:tensor(0.1862, grad_fn=<MeanBackward0>) test:tensor(0.2087, grad_fn=<MeanBackward0>)\n",
      "times:196 train:tensor(0.1860, grad_fn=<MeanBackward0>) test:tensor(0.2086, grad_fn=<MeanBackward0>)\n",
      "times:197 train:tensor(0.1858, grad_fn=<MeanBackward0>) test:tensor(0.2085, grad_fn=<MeanBackward0>)\n",
      "times:198 train:tensor(0.1857, grad_fn=<MeanBackward0>) test:tensor(0.2093, grad_fn=<MeanBackward0>)\n",
      "times:199 train:tensor(0.1861, grad_fn=<MeanBackward0>) test:tensor(0.2112, grad_fn=<MeanBackward0>)\n",
      "times:200 train:tensor(0.1879, grad_fn=<MeanBackward0>) test:tensor(0.2081, grad_fn=<MeanBackward0>)\n",
      "times:201 train:tensor(0.1852, grad_fn=<MeanBackward0>) test:tensor(0.2167, grad_fn=<MeanBackward0>)\n",
      "times:202 train:tensor(0.1920, grad_fn=<MeanBackward0>) test:tensor(0.2208, grad_fn=<MeanBackward0>)\n",
      "times:203 train:tensor(0.1960, grad_fn=<MeanBackward0>) test:tensor(0.2141, grad_fn=<MeanBackward0>)\n",
      "times:204 train:tensor(0.1903, grad_fn=<MeanBackward0>) test:tensor(0.2081, grad_fn=<MeanBackward0>)\n",
      "times:205 train:tensor(0.1854, grad_fn=<MeanBackward0>) test:tensor(0.2249, grad_fn=<MeanBackward0>)\n",
      "times:206 train:tensor(0.1995, grad_fn=<MeanBackward0>) test:tensor(0.2120, grad_fn=<MeanBackward0>)\n",
      "times:207 train:tensor(0.1889, grad_fn=<MeanBackward0>) test:tensor(0.2329, grad_fn=<MeanBackward0>)\n",
      "times:208 train:tensor(0.2079, grad_fn=<MeanBackward0>) test:tensor(0.2132, grad_fn=<MeanBackward0>)\n",
      "times:209 train:tensor(0.1902, grad_fn=<MeanBackward0>) test:tensor(0.2118, grad_fn=<MeanBackward0>)\n",
      "times:210 train:tensor(0.1887, grad_fn=<MeanBackward0>) test:tensor(0.2302, grad_fn=<MeanBackward0>)\n",
      "times:211 train:tensor(0.2047, grad_fn=<MeanBackward0>) test:tensor(0.2106, grad_fn=<MeanBackward0>)\n",
      "times:212 train:tensor(0.1880, grad_fn=<MeanBackward0>) test:tensor(0.2153, grad_fn=<MeanBackward0>)\n",
      "times:213 train:tensor(0.1919, grad_fn=<MeanBackward0>) test:tensor(0.2223, grad_fn=<MeanBackward0>)\n",
      "times:214 train:tensor(0.1980, grad_fn=<MeanBackward0>) test:tensor(0.2193, grad_fn=<MeanBackward0>)\n",
      "times:215 train:tensor(0.1951, grad_fn=<MeanBackward0>) test:tensor(0.2141, grad_fn=<MeanBackward0>)\n",
      "times:216 train:tensor(0.1908, grad_fn=<MeanBackward0>) test:tensor(0.2106, grad_fn=<MeanBackward0>)\n",
      "times:217 train:tensor(0.1878, grad_fn=<MeanBackward0>) test:tensor(0.2125, grad_fn=<MeanBackward0>)\n",
      "times:218 train:tensor(0.1889, grad_fn=<MeanBackward0>) test:tensor(0.2177, grad_fn=<MeanBackward0>)\n",
      "times:219 train:tensor(0.1930, grad_fn=<MeanBackward0>) test:tensor(0.2109, grad_fn=<MeanBackward0>)\n",
      "times:220 train:tensor(0.1873, grad_fn=<MeanBackward0>) test:tensor(0.2092, grad_fn=<MeanBackward0>)\n",
      "times:221 train:tensor(0.1861, grad_fn=<MeanBackward0>) test:tensor(0.2109, grad_fn=<MeanBackward0>)\n",
      "times:222 train:tensor(0.1873, grad_fn=<MeanBackward0>) test:tensor(0.2130, grad_fn=<MeanBackward0>)\n",
      "times:223 train:tensor(0.1888, grad_fn=<MeanBackward0>) test:tensor(0.2131, grad_fn=<MeanBackward0>)\n",
      "times:224 train:tensor(0.1888, grad_fn=<MeanBackward0>) test:tensor(0.2107, grad_fn=<MeanBackward0>)\n",
      "times:225 train:tensor(0.1867, grad_fn=<MeanBackward0>) test:tensor(0.2089, grad_fn=<MeanBackward0>)\n",
      "times:226 train:tensor(0.1851, grad_fn=<MeanBackward0>) test:tensor(0.2098, grad_fn=<MeanBackward0>)\n",
      "times:227 train:tensor(0.1855, grad_fn=<MeanBackward0>) test:tensor(0.2125, grad_fn=<MeanBackward0>)\n",
      "times:228 train:tensor(0.1875, grad_fn=<MeanBackward0>) test:tensor(0.2111, grad_fn=<MeanBackward0>)\n",
      "times:229 train:tensor(0.1863, grad_fn=<MeanBackward0>) test:tensor(0.2091, grad_fn=<MeanBackward0>)\n",
      "times:230 train:tensor(0.1849, grad_fn=<MeanBackward0>) test:tensor(0.2095, grad_fn=<MeanBackward0>)\n",
      "times:231 train:tensor(0.1853, grad_fn=<MeanBackward0>) test:tensor(0.2106, grad_fn=<MeanBackward0>)\n",
      "times:232 train:tensor(0.1861, grad_fn=<MeanBackward0>) test:tensor(0.2107, grad_fn=<MeanBackward0>)\n",
      "times:233 train:tensor(0.1862, grad_fn=<MeanBackward0>) test:tensor(0.2097, grad_fn=<MeanBackward0>)\n",
      "times:234 train:tensor(0.1854, grad_fn=<MeanBackward0>) test:tensor(0.2088, grad_fn=<MeanBackward0>)\n",
      "times:235 train:tensor(0.1846, grad_fn=<MeanBackward0>) test:tensor(0.2091, grad_fn=<MeanBackward0>)\n",
      "times:236 train:tensor(0.1847, grad_fn=<MeanBackward0>) test:tensor(0.2100, grad_fn=<MeanBackward0>)\n",
      "times:237 train:tensor(0.1853, grad_fn=<MeanBackward0>) test:tensor(0.2097, grad_fn=<MeanBackward0>)\n",
      "times:238 train:tensor(0.1851, grad_fn=<MeanBackward0>) test:tensor(0.2086, grad_fn=<MeanBackward0>)\n",
      "times:239 train:tensor(0.1844, grad_fn=<MeanBackward0>) test:tensor(0.2084, grad_fn=<MeanBackward0>)\n",
      "times:240 train:tensor(0.1844, grad_fn=<MeanBackward0>) test:tensor(0.2086, grad_fn=<MeanBackward0>)\n",
      "times:241 train:tensor(0.1846, grad_fn=<MeanBackward0>) test:tensor(0.2087, grad_fn=<MeanBackward0>)\n",
      "times:242 train:tensor(0.1847, grad_fn=<MeanBackward0>) test:tensor(0.2086, grad_fn=<MeanBackward0>)\n",
      "times:243 train:tensor(0.1846, grad_fn=<MeanBackward0>) test:tensor(0.2082, grad_fn=<MeanBackward0>)\n",
      "times:244 train:tensor(0.1842, grad_fn=<MeanBackward0>) test:tensor(0.2081, grad_fn=<MeanBackward0>)\n",
      "times:245 train:tensor(0.1840, grad_fn=<MeanBackward0>) test:tensor(0.2084, grad_fn=<MeanBackward0>)\n",
      "times:246 train:tensor(0.1841, grad_fn=<MeanBackward0>) test:tensor(0.2087, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:247 train:tensor(0.1842, grad_fn=<MeanBackward0>) test:tensor(0.2084, grad_fn=<MeanBackward0>)\n",
      "times:248 train:tensor(0.1840, grad_fn=<MeanBackward0>) test:tensor(0.2080, grad_fn=<MeanBackward0>)\n",
      "times:249 train:tensor(0.1837, grad_fn=<MeanBackward0>) test:tensor(0.2079, grad_fn=<MeanBackward0>)\n",
      "times:250 train:tensor(0.1837, grad_fn=<MeanBackward0>) test:tensor(0.2080, grad_fn=<MeanBackward0>)\n",
      "times:251 train:tensor(0.1838, grad_fn=<MeanBackward0>) test:tensor(0.2080, grad_fn=<MeanBackward0>)\n",
      "times:252 train:tensor(0.1837, grad_fn=<MeanBackward0>) test:tensor(0.2079, grad_fn=<MeanBackward0>)\n",
      "times:253 train:tensor(0.1835, grad_fn=<MeanBackward0>) test:tensor(0.2078, grad_fn=<MeanBackward0>)\n",
      "times:254 train:tensor(0.1834, grad_fn=<MeanBackward0>) test:tensor(0.2079, grad_fn=<MeanBackward0>)\n",
      "times:255 train:tensor(0.1834, grad_fn=<MeanBackward0>) test:tensor(0.2080, grad_fn=<MeanBackward0>)\n",
      "times:256 train:tensor(0.1834, grad_fn=<MeanBackward0>) test:tensor(0.2078, grad_fn=<MeanBackward0>)\n",
      "times:257 train:tensor(0.1833, grad_fn=<MeanBackward0>) test:tensor(0.2075, grad_fn=<MeanBackward0>)\n",
      "times:258 train:tensor(0.1831, grad_fn=<MeanBackward0>) test:tensor(0.2075, grad_fn=<MeanBackward0>)\n",
      "times:259 train:tensor(0.1831, grad_fn=<MeanBackward0>) test:tensor(0.2075, grad_fn=<MeanBackward0>)\n",
      "times:260 train:tensor(0.1831, grad_fn=<MeanBackward0>) test:tensor(0.2074, grad_fn=<MeanBackward0>)\n",
      "times:261 train:tensor(0.1830, grad_fn=<MeanBackward0>) test:tensor(0.2072, grad_fn=<MeanBackward0>)\n",
      "times:262 train:tensor(0.1829, grad_fn=<MeanBackward0>) test:tensor(0.2071, grad_fn=<MeanBackward0>)\n",
      "times:263 train:tensor(0.1828, grad_fn=<MeanBackward0>) test:tensor(0.2071, grad_fn=<MeanBackward0>)\n",
      "times:264 train:tensor(0.1828, grad_fn=<MeanBackward0>) test:tensor(0.2071, grad_fn=<MeanBackward0>)\n",
      "times:265 train:tensor(0.1827, grad_fn=<MeanBackward0>) test:tensor(0.2069, grad_fn=<MeanBackward0>)\n",
      "times:266 train:tensor(0.1826, grad_fn=<MeanBackward0>) test:tensor(0.2068, grad_fn=<MeanBackward0>)\n",
      "times:267 train:tensor(0.1826, grad_fn=<MeanBackward0>) test:tensor(0.2068, grad_fn=<MeanBackward0>)\n",
      "times:268 train:tensor(0.1825, grad_fn=<MeanBackward0>) test:tensor(0.2067, grad_fn=<MeanBackward0>)\n",
      "times:269 train:tensor(0.1825, grad_fn=<MeanBackward0>) test:tensor(0.2066, grad_fn=<MeanBackward0>)\n",
      "times:270 train:tensor(0.1824, grad_fn=<MeanBackward0>) test:tensor(0.2066, grad_fn=<MeanBackward0>)\n",
      "times:271 train:tensor(0.1823, grad_fn=<MeanBackward0>) test:tensor(0.2066, grad_fn=<MeanBackward0>)\n",
      "times:272 train:tensor(0.1823, grad_fn=<MeanBackward0>) test:tensor(0.2065, grad_fn=<MeanBackward0>)\n",
      "times:273 train:tensor(0.1822, grad_fn=<MeanBackward0>) test:tensor(0.2065, grad_fn=<MeanBackward0>)\n",
      "times:274 train:tensor(0.1822, grad_fn=<MeanBackward0>) test:tensor(0.2064, grad_fn=<MeanBackward0>)\n",
      "times:275 train:tensor(0.1821, grad_fn=<MeanBackward0>) test:tensor(0.2064, grad_fn=<MeanBackward0>)\n",
      "times:276 train:tensor(0.1821, grad_fn=<MeanBackward0>) test:tensor(0.2064, grad_fn=<MeanBackward0>)\n",
      "times:277 train:tensor(0.1820, grad_fn=<MeanBackward0>) test:tensor(0.2063, grad_fn=<MeanBackward0>)\n",
      "times:278 train:tensor(0.1819, grad_fn=<MeanBackward0>) test:tensor(0.2063, grad_fn=<MeanBackward0>)\n",
      "times:279 train:tensor(0.1819, grad_fn=<MeanBackward0>) test:tensor(0.2062, grad_fn=<MeanBackward0>)\n",
      "times:280 train:tensor(0.1818, grad_fn=<MeanBackward0>) test:tensor(0.2062, grad_fn=<MeanBackward0>)\n",
      "times:281 train:tensor(0.1818, grad_fn=<MeanBackward0>) test:tensor(0.2061, grad_fn=<MeanBackward0>)\n",
      "times:282 train:tensor(0.1817, grad_fn=<MeanBackward0>) test:tensor(0.2061, grad_fn=<MeanBackward0>)\n",
      "times:283 train:tensor(0.1817, grad_fn=<MeanBackward0>) test:tensor(0.2060, grad_fn=<MeanBackward0>)\n",
      "times:284 train:tensor(0.1816, grad_fn=<MeanBackward0>) test:tensor(0.2060, grad_fn=<MeanBackward0>)\n",
      "times:285 train:tensor(0.1816, grad_fn=<MeanBackward0>) test:tensor(0.2059, grad_fn=<MeanBackward0>)\n",
      "times:286 train:tensor(0.1815, grad_fn=<MeanBackward0>) test:tensor(0.2058, grad_fn=<MeanBackward0>)\n",
      "times:287 train:tensor(0.1815, grad_fn=<MeanBackward0>) test:tensor(0.2058, grad_fn=<MeanBackward0>)\n",
      "times:288 train:tensor(0.1814, grad_fn=<MeanBackward0>) test:tensor(0.2057, grad_fn=<MeanBackward0>)\n",
      "times:289 train:tensor(0.1814, grad_fn=<MeanBackward0>) test:tensor(0.2057, grad_fn=<MeanBackward0>)\n",
      "times:290 train:tensor(0.1813, grad_fn=<MeanBackward0>) test:tensor(0.2056, grad_fn=<MeanBackward0>)\n",
      "times:291 train:tensor(0.1813, grad_fn=<MeanBackward0>) test:tensor(0.2056, grad_fn=<MeanBackward0>)\n",
      "times:292 train:tensor(0.1812, grad_fn=<MeanBackward0>) test:tensor(0.2055, grad_fn=<MeanBackward0>)\n",
      "times:293 train:tensor(0.1812, grad_fn=<MeanBackward0>) test:tensor(0.2055, grad_fn=<MeanBackward0>)\n",
      "times:294 train:tensor(0.1811, grad_fn=<MeanBackward0>) test:tensor(0.2054, grad_fn=<MeanBackward0>)\n",
      "times:295 train:tensor(0.1811, grad_fn=<MeanBackward0>) test:tensor(0.2054, grad_fn=<MeanBackward0>)\n",
      "times:296 train:tensor(0.1810, grad_fn=<MeanBackward0>) test:tensor(0.2054, grad_fn=<MeanBackward0>)\n",
      "times:297 train:tensor(0.1810, grad_fn=<MeanBackward0>) test:tensor(0.2053, grad_fn=<MeanBackward0>)\n",
      "times:298 train:tensor(0.1809, grad_fn=<MeanBackward0>) test:tensor(0.2053, grad_fn=<MeanBackward0>)\n",
      "times:299 train:tensor(0.1809, grad_fn=<MeanBackward0>) test:tensor(0.2052, grad_fn=<MeanBackward0>)\n",
      "times:300 train:tensor(0.1808, grad_fn=<MeanBackward0>) test:tensor(0.2052, grad_fn=<MeanBackward0>)\n",
      "times:301 train:tensor(0.1808, grad_fn=<MeanBackward0>) test:tensor(0.2051, grad_fn=<MeanBackward0>)\n",
      "times:302 train:tensor(0.1808, grad_fn=<MeanBackward0>) test:tensor(0.2051, grad_fn=<MeanBackward0>)\n",
      "times:303 train:tensor(0.1807, grad_fn=<MeanBackward0>) test:tensor(0.2050, grad_fn=<MeanBackward0>)\n",
      "times:304 train:tensor(0.1807, grad_fn=<MeanBackward0>) test:tensor(0.2049, grad_fn=<MeanBackward0>)\n",
      "times:305 train:tensor(0.1806, grad_fn=<MeanBackward0>) test:tensor(0.2049, grad_fn=<MeanBackward0>)\n",
      "times:306 train:tensor(0.1806, grad_fn=<MeanBackward0>) test:tensor(0.2048, grad_fn=<MeanBackward0>)\n",
      "times:307 train:tensor(0.1805, grad_fn=<MeanBackward0>) test:tensor(0.2048, grad_fn=<MeanBackward0>)\n",
      "times:308 train:tensor(0.1805, grad_fn=<MeanBackward0>) test:tensor(0.2047, grad_fn=<MeanBackward0>)\n",
      "times:309 train:tensor(0.1804, grad_fn=<MeanBackward0>) test:tensor(0.2047, grad_fn=<MeanBackward0>)\n",
      "times:310 train:tensor(0.1804, grad_fn=<MeanBackward0>) test:tensor(0.2046, grad_fn=<MeanBackward0>)\n",
      "times:311 train:tensor(0.1803, grad_fn=<MeanBackward0>) test:tensor(0.2045, grad_fn=<MeanBackward0>)\n",
      "times:312 train:tensor(0.1803, grad_fn=<MeanBackward0>) test:tensor(0.2045, grad_fn=<MeanBackward0>)\n",
      "times:313 train:tensor(0.1802, grad_fn=<MeanBackward0>) test:tensor(0.2044, grad_fn=<MeanBackward0>)\n",
      "times:314 train:tensor(0.1802, grad_fn=<MeanBackward0>) test:tensor(0.2044, grad_fn=<MeanBackward0>)\n",
      "times:315 train:tensor(0.1801, grad_fn=<MeanBackward0>) test:tensor(0.2043, grad_fn=<MeanBackward0>)\n",
      "times:316 train:tensor(0.1801, grad_fn=<MeanBackward0>) test:tensor(0.2043, grad_fn=<MeanBackward0>)\n",
      "times:317 train:tensor(0.1800, grad_fn=<MeanBackward0>) test:tensor(0.2042, grad_fn=<MeanBackward0>)\n",
      "times:318 train:tensor(0.1800, grad_fn=<MeanBackward0>) test:tensor(0.2042, grad_fn=<MeanBackward0>)\n",
      "times:319 train:tensor(0.1800, grad_fn=<MeanBackward0>) test:tensor(0.2041, grad_fn=<MeanBackward0>)\n",
      "times:320 train:tensor(0.1799, grad_fn=<MeanBackward0>) test:tensor(0.2041, grad_fn=<MeanBackward0>)\n",
      "times:321 train:tensor(0.1799, grad_fn=<MeanBackward0>) test:tensor(0.2040, grad_fn=<MeanBackward0>)\n",
      "times:322 train:tensor(0.1798, grad_fn=<MeanBackward0>) test:tensor(0.2040, grad_fn=<MeanBackward0>)\n",
      "times:323 train:tensor(0.1798, grad_fn=<MeanBackward0>) test:tensor(0.2039, grad_fn=<MeanBackward0>)\n",
      "times:324 train:tensor(0.1797, grad_fn=<MeanBackward0>) test:tensor(0.2038, grad_fn=<MeanBackward0>)\n",
      "times:325 train:tensor(0.1797, grad_fn=<MeanBackward0>) test:tensor(0.2038, grad_fn=<MeanBackward0>)\n",
      "times:326 train:tensor(0.1796, grad_fn=<MeanBackward0>) test:tensor(0.2037, grad_fn=<MeanBackward0>)\n",
      "times:327 train:tensor(0.1796, grad_fn=<MeanBackward0>) test:tensor(0.2037, grad_fn=<MeanBackward0>)\n",
      "times:328 train:tensor(0.1795, grad_fn=<MeanBackward0>) test:tensor(0.2036, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:329 train:tensor(0.1795, grad_fn=<MeanBackward0>) test:tensor(0.2036, grad_fn=<MeanBackward0>)\n",
      "times:330 train:tensor(0.1794, grad_fn=<MeanBackward0>) test:tensor(0.2035, grad_fn=<MeanBackward0>)\n",
      "times:331 train:tensor(0.1794, grad_fn=<MeanBackward0>) test:tensor(0.2034, grad_fn=<MeanBackward0>)\n",
      "times:332 train:tensor(0.1793, grad_fn=<MeanBackward0>) test:tensor(0.2034, grad_fn=<MeanBackward0>)\n",
      "times:333 train:tensor(0.1793, grad_fn=<MeanBackward0>) test:tensor(0.2033, grad_fn=<MeanBackward0>)\n",
      "times:334 train:tensor(0.1792, grad_fn=<MeanBackward0>) test:tensor(0.2033, grad_fn=<MeanBackward0>)\n",
      "times:335 train:tensor(0.1792, grad_fn=<MeanBackward0>) test:tensor(0.2032, grad_fn=<MeanBackward0>)\n",
      "times:336 train:tensor(0.1791, grad_fn=<MeanBackward0>) test:tensor(0.2032, grad_fn=<MeanBackward0>)\n",
      "times:337 train:tensor(0.1791, grad_fn=<MeanBackward0>) test:tensor(0.2031, grad_fn=<MeanBackward0>)\n",
      "times:338 train:tensor(0.1790, grad_fn=<MeanBackward0>) test:tensor(0.2030, grad_fn=<MeanBackward0>)\n",
      "times:339 train:tensor(0.1790, grad_fn=<MeanBackward0>) test:tensor(0.2030, grad_fn=<MeanBackward0>)\n",
      "times:340 train:tensor(0.1789, grad_fn=<MeanBackward0>) test:tensor(0.2029, grad_fn=<MeanBackward0>)\n",
      "times:341 train:tensor(0.1789, grad_fn=<MeanBackward0>) test:tensor(0.2029, grad_fn=<MeanBackward0>)\n",
      "times:342 train:tensor(0.1788, grad_fn=<MeanBackward0>) test:tensor(0.2028, grad_fn=<MeanBackward0>)\n",
      "times:343 train:tensor(0.1788, grad_fn=<MeanBackward0>) test:tensor(0.2027, grad_fn=<MeanBackward0>)\n",
      "times:344 train:tensor(0.1788, grad_fn=<MeanBackward0>) test:tensor(0.2027, grad_fn=<MeanBackward0>)\n",
      "times:345 train:tensor(0.1787, grad_fn=<MeanBackward0>) test:tensor(0.2026, grad_fn=<MeanBackward0>)\n",
      "times:346 train:tensor(0.1787, grad_fn=<MeanBackward0>) test:tensor(0.2026, grad_fn=<MeanBackward0>)\n",
      "times:347 train:tensor(0.1786, grad_fn=<MeanBackward0>) test:tensor(0.2025, grad_fn=<MeanBackward0>)\n",
      "times:348 train:tensor(0.1785, grad_fn=<MeanBackward0>) test:tensor(0.2024, grad_fn=<MeanBackward0>)\n",
      "times:349 train:tensor(0.1785, grad_fn=<MeanBackward0>) test:tensor(0.2024, grad_fn=<MeanBackward0>)\n",
      "times:350 train:tensor(0.1784, grad_fn=<MeanBackward0>) test:tensor(0.2023, grad_fn=<MeanBackward0>)\n",
      "times:351 train:tensor(0.1784, grad_fn=<MeanBackward0>) test:tensor(0.2022, grad_fn=<MeanBackward0>)\n",
      "times:352 train:tensor(0.1783, grad_fn=<MeanBackward0>) test:tensor(0.2022, grad_fn=<MeanBackward0>)\n",
      "times:353 train:tensor(0.1783, grad_fn=<MeanBackward0>) test:tensor(0.2021, grad_fn=<MeanBackward0>)\n",
      "times:354 train:tensor(0.1782, grad_fn=<MeanBackward0>) test:tensor(0.2021, grad_fn=<MeanBackward0>)\n",
      "times:355 train:tensor(0.1782, grad_fn=<MeanBackward0>) test:tensor(0.2020, grad_fn=<MeanBackward0>)\n",
      "times:356 train:tensor(0.1781, grad_fn=<MeanBackward0>) test:tensor(0.2019, grad_fn=<MeanBackward0>)\n",
      "times:357 train:tensor(0.1781, grad_fn=<MeanBackward0>) test:tensor(0.2019, grad_fn=<MeanBackward0>)\n",
      "times:358 train:tensor(0.1780, grad_fn=<MeanBackward0>) test:tensor(0.2018, grad_fn=<MeanBackward0>)\n",
      "times:359 train:tensor(0.1780, grad_fn=<MeanBackward0>) test:tensor(0.2017, grad_fn=<MeanBackward0>)\n",
      "times:360 train:tensor(0.1779, grad_fn=<MeanBackward0>) test:tensor(0.2017, grad_fn=<MeanBackward0>)\n",
      "times:361 train:tensor(0.1779, grad_fn=<MeanBackward0>) test:tensor(0.2016, grad_fn=<MeanBackward0>)\n",
      "times:362 train:tensor(0.1778, grad_fn=<MeanBackward0>) test:tensor(0.2015, grad_fn=<MeanBackward0>)\n",
      "times:363 train:tensor(0.1778, grad_fn=<MeanBackward0>) test:tensor(0.2014, grad_fn=<MeanBackward0>)\n",
      "times:364 train:tensor(0.1777, grad_fn=<MeanBackward0>) test:tensor(0.2014, grad_fn=<MeanBackward0>)\n",
      "times:365 train:tensor(0.1777, grad_fn=<MeanBackward0>) test:tensor(0.2013, grad_fn=<MeanBackward0>)\n",
      "times:366 train:tensor(0.1776, grad_fn=<MeanBackward0>) test:tensor(0.2012, grad_fn=<MeanBackward0>)\n",
      "times:367 train:tensor(0.1775, grad_fn=<MeanBackward0>) test:tensor(0.2012, grad_fn=<MeanBackward0>)\n",
      "times:368 train:tensor(0.1775, grad_fn=<MeanBackward0>) test:tensor(0.2011, grad_fn=<MeanBackward0>)\n",
      "times:369 train:tensor(0.1774, grad_fn=<MeanBackward0>) test:tensor(0.2010, grad_fn=<MeanBackward0>)\n",
      "times:370 train:tensor(0.1774, grad_fn=<MeanBackward0>) test:tensor(0.2009, grad_fn=<MeanBackward0>)\n",
      "times:371 train:tensor(0.1773, grad_fn=<MeanBackward0>) test:tensor(0.2009, grad_fn=<MeanBackward0>)\n",
      "times:372 train:tensor(0.1773, grad_fn=<MeanBackward0>) test:tensor(0.2008, grad_fn=<MeanBackward0>)\n",
      "times:373 train:tensor(0.1772, grad_fn=<MeanBackward0>) test:tensor(0.2007, grad_fn=<MeanBackward0>)\n",
      "times:374 train:tensor(0.1771, grad_fn=<MeanBackward0>) test:tensor(0.2006, grad_fn=<MeanBackward0>)\n",
      "times:375 train:tensor(0.1771, grad_fn=<MeanBackward0>) test:tensor(0.2006, grad_fn=<MeanBackward0>)\n",
      "times:376 train:tensor(0.1770, grad_fn=<MeanBackward0>) test:tensor(0.2005, grad_fn=<MeanBackward0>)\n",
      "times:377 train:tensor(0.1770, grad_fn=<MeanBackward0>) test:tensor(0.2004, grad_fn=<MeanBackward0>)\n",
      "times:378 train:tensor(0.1769, grad_fn=<MeanBackward0>) test:tensor(0.2003, grad_fn=<MeanBackward0>)\n",
      "times:379 train:tensor(0.1768, grad_fn=<MeanBackward0>) test:tensor(0.2002, grad_fn=<MeanBackward0>)\n",
      "times:380 train:tensor(0.1768, grad_fn=<MeanBackward0>) test:tensor(0.2002, grad_fn=<MeanBackward0>)\n",
      "times:381 train:tensor(0.1767, grad_fn=<MeanBackward0>) test:tensor(0.2001, grad_fn=<MeanBackward0>)\n",
      "times:382 train:tensor(0.1766, grad_fn=<MeanBackward0>) test:tensor(0.2000, grad_fn=<MeanBackward0>)\n",
      "times:383 train:tensor(0.1766, grad_fn=<MeanBackward0>) test:tensor(0.1999, grad_fn=<MeanBackward0>)\n",
      "times:384 train:tensor(0.1765, grad_fn=<MeanBackward0>) test:tensor(0.1998, grad_fn=<MeanBackward0>)\n",
      "times:385 train:tensor(0.1765, grad_fn=<MeanBackward0>) test:tensor(0.1997, grad_fn=<MeanBackward0>)\n",
      "times:386 train:tensor(0.1764, grad_fn=<MeanBackward0>) test:tensor(0.1996, grad_fn=<MeanBackward0>)\n",
      "times:387 train:tensor(0.1763, grad_fn=<MeanBackward0>) test:tensor(0.1995, grad_fn=<MeanBackward0>)\n",
      "times:388 train:tensor(0.1763, grad_fn=<MeanBackward0>) test:tensor(0.1995, grad_fn=<MeanBackward0>)\n",
      "times:389 train:tensor(0.1762, grad_fn=<MeanBackward0>) test:tensor(0.1994, grad_fn=<MeanBackward0>)\n",
      "times:390 train:tensor(0.1761, grad_fn=<MeanBackward0>) test:tensor(0.1993, grad_fn=<MeanBackward0>)\n",
      "times:391 train:tensor(0.1761, grad_fn=<MeanBackward0>) test:tensor(0.1992, grad_fn=<MeanBackward0>)\n",
      "times:392 train:tensor(0.1760, grad_fn=<MeanBackward0>) test:tensor(0.1991, grad_fn=<MeanBackward0>)\n",
      "times:393 train:tensor(0.1759, grad_fn=<MeanBackward0>) test:tensor(0.1990, grad_fn=<MeanBackward0>)\n",
      "times:394 train:tensor(0.1759, grad_fn=<MeanBackward0>) test:tensor(0.1989, grad_fn=<MeanBackward0>)\n",
      "times:395 train:tensor(0.1758, grad_fn=<MeanBackward0>) test:tensor(0.1988, grad_fn=<MeanBackward0>)\n",
      "times:396 train:tensor(0.1757, grad_fn=<MeanBackward0>) test:tensor(0.1987, grad_fn=<MeanBackward0>)\n",
      "times:397 train:tensor(0.1757, grad_fn=<MeanBackward0>) test:tensor(0.1986, grad_fn=<MeanBackward0>)\n",
      "times:398 train:tensor(0.1756, grad_fn=<MeanBackward0>) test:tensor(0.1985, grad_fn=<MeanBackward0>)\n",
      "times:399 train:tensor(0.1755, grad_fn=<MeanBackward0>) test:tensor(0.1984, grad_fn=<MeanBackward0>)\n",
      "times:400 train:tensor(0.1755, grad_fn=<MeanBackward0>) test:tensor(0.1983, grad_fn=<MeanBackward0>)\n",
      "times:401 train:tensor(0.1754, grad_fn=<MeanBackward0>) test:tensor(0.1982, grad_fn=<MeanBackward0>)\n",
      "times:402 train:tensor(0.1753, grad_fn=<MeanBackward0>) test:tensor(0.1982, grad_fn=<MeanBackward0>)\n",
      "times:403 train:tensor(0.1753, grad_fn=<MeanBackward0>) test:tensor(0.1981, grad_fn=<MeanBackward0>)\n",
      "times:404 train:tensor(0.1752, grad_fn=<MeanBackward0>) test:tensor(0.1980, grad_fn=<MeanBackward0>)\n",
      "times:405 train:tensor(0.1751, grad_fn=<MeanBackward0>) test:tensor(0.1979, grad_fn=<MeanBackward0>)\n",
      "times:406 train:tensor(0.1750, grad_fn=<MeanBackward0>) test:tensor(0.1978, grad_fn=<MeanBackward0>)\n",
      "times:407 train:tensor(0.1750, grad_fn=<MeanBackward0>) test:tensor(0.1977, grad_fn=<MeanBackward0>)\n",
      "times:408 train:tensor(0.1749, grad_fn=<MeanBackward0>) test:tensor(0.1976, grad_fn=<MeanBackward0>)\n",
      "times:409 train:tensor(0.1748, grad_fn=<MeanBackward0>) test:tensor(0.1975, grad_fn=<MeanBackward0>)\n",
      "times:410 train:tensor(0.1748, grad_fn=<MeanBackward0>) test:tensor(0.1974, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:411 train:tensor(0.1747, grad_fn=<MeanBackward0>) test:tensor(0.1973, grad_fn=<MeanBackward0>)\n",
      "times:412 train:tensor(0.1746, grad_fn=<MeanBackward0>) test:tensor(0.1972, grad_fn=<MeanBackward0>)\n",
      "times:413 train:tensor(0.1746, grad_fn=<MeanBackward0>) test:tensor(0.1971, grad_fn=<MeanBackward0>)\n",
      "times:414 train:tensor(0.1745, grad_fn=<MeanBackward0>) test:tensor(0.1970, grad_fn=<MeanBackward0>)\n",
      "times:415 train:tensor(0.1744, grad_fn=<MeanBackward0>) test:tensor(0.1969, grad_fn=<MeanBackward0>)\n",
      "times:416 train:tensor(0.1744, grad_fn=<MeanBackward0>) test:tensor(0.1969, grad_fn=<MeanBackward0>)\n",
      "times:417 train:tensor(0.1743, grad_fn=<MeanBackward0>) test:tensor(0.1968, grad_fn=<MeanBackward0>)\n",
      "times:418 train:tensor(0.1742, grad_fn=<MeanBackward0>) test:tensor(0.1967, grad_fn=<MeanBackward0>)\n",
      "times:419 train:tensor(0.1742, grad_fn=<MeanBackward0>) test:tensor(0.1966, grad_fn=<MeanBackward0>)\n",
      "times:420 train:tensor(0.1741, grad_fn=<MeanBackward0>) test:tensor(0.1965, grad_fn=<MeanBackward0>)\n",
      "times:421 train:tensor(0.1740, grad_fn=<MeanBackward0>) test:tensor(0.1964, grad_fn=<MeanBackward0>)\n",
      "times:422 train:tensor(0.1740, grad_fn=<MeanBackward0>) test:tensor(0.1964, grad_fn=<MeanBackward0>)\n",
      "times:423 train:tensor(0.1739, grad_fn=<MeanBackward0>) test:tensor(0.1963, grad_fn=<MeanBackward0>)\n",
      "times:424 train:tensor(0.1738, grad_fn=<MeanBackward0>) test:tensor(0.1962, grad_fn=<MeanBackward0>)\n",
      "times:425 train:tensor(0.1738, grad_fn=<MeanBackward0>) test:tensor(0.1961, grad_fn=<MeanBackward0>)\n",
      "times:426 train:tensor(0.1737, grad_fn=<MeanBackward0>) test:tensor(0.1960, grad_fn=<MeanBackward0>)\n",
      "times:427 train:tensor(0.1736, grad_fn=<MeanBackward0>) test:tensor(0.1960, grad_fn=<MeanBackward0>)\n",
      "times:428 train:tensor(0.1736, grad_fn=<MeanBackward0>) test:tensor(0.1959, grad_fn=<MeanBackward0>)\n",
      "times:429 train:tensor(0.1735, grad_fn=<MeanBackward0>) test:tensor(0.1958, grad_fn=<MeanBackward0>)\n",
      "times:430 train:tensor(0.1735, grad_fn=<MeanBackward0>) test:tensor(0.1957, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-18fced147fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtest_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mloss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'times:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' train:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' test:'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-634f113ab197>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# h_n shape (n_layers, batch, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# h_c shape (n_layers, batch, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# None represents zero initial hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# choose r_out at the last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 577\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_x = []\n",
    "loss_y = []\n",
    "for n in range(2000):\n",
    "    loss_x.append(n)\n",
    "    train_out = rnn(x_train)\n",
    "    #print(train_out.size())\n",
    "    \n",
    "    loss = loss_func(train_out, y_train)\n",
    "    #print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_y.append(loss.data.item())\n",
    "    \n",
    "    test_out = rnn(x_test)\n",
    "    loss_test = loss_func(test_out,y_test)\n",
    "    print('times:'+str(n)+' train:'+str(loss)+' test:'+ str(loss_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (864,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3e256da9fcf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Times'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss Value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Analysis_loss.tif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2840\u001b[0m     return gca().plot(\n\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (864,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Times')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.plot(loss_x,loss_y)\n",
    "plt.savefig('Analysis_loss.tif', dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysx/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/ysx/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.879159927368164, 10.958829879760742, 10.94572925567627, 11.037543296813965, 11.035038948059082, 11.054723739624023, 11.066934585571289, 11.165462493896484, 11.177306175231934, 11.261505126953125, 11.278011322021484, 11.413973808288574, 11.39466381072998, 11.45206356048584, 11.454554557800293, 11.501648902893066, 11.48918628692627, 11.464253425598145, 11.510924339294434, 11.516667366027832, 11.662895202636719, 11.679076194763184, 11.819659233093262, 11.817609786987305, 11.886134147644043, 11.87942123413086, 11.956701278686523, 11.970905303955078, 11.707962989807129, 11.705293655395508, 11.719368934631348, 11.710921287536621, 11.738763809204102, 11.809722900390625, 11.813216209411621, 11.807909965515137, 11.812341690063477, 11.807206153869629, 11.811893463134766, 11.811086654663086]\n"
     ]
    }
   ],
   "source": [
    "my_matrix = np.loadtxt(open(\"results.csv\"),delimiter=\",\",skiprows=0)\n",
    "#print(my_matrix)\n",
    " #对于矩阵而言，将矩阵倒数第一列之前的数值给了X（输入数据），将矩阵大最后一列的数值给了y（标签）\n",
    "X, y = my_matrix[:,:-1],my_matrix[:,-1]\n",
    "torch_y = torch.from_numpy(y)\n",
    "y_input = torch.tensor(torch_y , dtype=torch.float32)\n",
    "X_loss = []\n",
    "axis = []\n",
    "for n in range(40):\n",
    "    #X_minloss = 0\n",
    "    #print('start'+ str(X_minloss))\n",
    "    axis.append(n)\n",
    "    for i in range(30):\n",
    "        for j in range(len(X)):\n",
    "            X[j][n*30+i] = 0\n",
    "    \n",
    "            \n",
    "    torch_X = torch.from_numpy(X)\n",
    "    X_input = torch.tensor(torch_X , dtype=torch.float32)\n",
    "    X_input = X_input.reshape(X_input.shape[0], TIME_STEP, INPUT_SIZE)\n",
    "    loss_func = My_loss()\n",
    "    \n",
    "    X_out = rnn(X_input)\n",
    "    X_minloss = loss_func(X_out,y_input)\n",
    "    X_minloss = X_minloss.data.item()\n",
    "    \n",
    "    #print(X_minloss)\n",
    "    \n",
    "    '''\n",
    "    for m in range(len(X)):\n",
    "        minloss = out[m].data.item() - X_out[m].data.item()\n",
    "        minloss = minloss\n",
    "        X_minloss += minloss\n",
    "    print('time: '+str(n)+'loss: '+str(X_minloss))\n",
    "    '''\n",
    "    X_loss.append(X_minloss)\n",
    "    \n",
    "print(X_loss)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = preprocessing.scale(X_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.03645391 -1.78510615 -1.82643682 -1.53677613 -1.54467701 -1.48257419\n",
      " -1.44405065 -1.13320863 -1.09584344 -0.83020731 -0.77813253 -0.34918955\n",
      " -0.41010994 -0.22902162 -0.22116286 -0.07258667 -0.11190451 -0.19056427\n",
      " -0.04332394 -0.02520548  0.43612326  0.48717207  0.93069225  0.92422653\n",
      "  1.14041148  1.11923316  1.36304109  1.40785284  0.57830594  0.56988456\n",
      "  0.61429013  0.58763897  0.6754783   0.89934449  0.9103654   0.89362493\n",
      "  0.90760641  0.8914045   0.90619232  0.90364695]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(results)\n",
    "axis =[]\n",
    "for n in range(40):\n",
    "    axis.append(n)\n",
    "#print(axis)\n",
    "\n",
    "plt.xlabel('Raman Segment')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.bar(axis, results)\n",
    "plt.savefig('Analysis.tif', dpi = 400)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
